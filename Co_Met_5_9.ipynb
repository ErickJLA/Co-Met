{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErickJLA/Co-Met/blob/main/Co_Met_5_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TKxhknlidcLn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title ⚙️ Setup: IMPORT LIBRARIES & LOAD FUNCTIONS\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 1: ENVIRONMENT SETUP\n",
        "# Purpose: Import required libraries and authenticate Google Sheets access\n",
        "# Dependencies: None\n",
        "# Outputs: Authentication status, library versions, system info\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import ipywidgets as widgets\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import display, HTML, clear_output, Javascript\n",
        "from scipy.stats import norm, t, chi2\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import sys\n",
        "import warnings\n",
        "from scipy.special import gamma\n",
        "import scipy.stats as stats\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suppress unnecessary warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# --- Configuration Constants ---\n",
        "REQUIRED_COLUMNS = {\n",
        "    'effect_data': ['xe', 'sde', 'ne', 'xc', 'sdc', 'nc'],\n",
        "    'metadata': ['id']\n",
        "}\n",
        "\n",
        "SUPPORTED_EFFECT_SIZES = {\n",
        "    'lnRR': 'Log Response Ratio',\n",
        "    'hedges_g': \"Hedges' g (corrected SMD)\",\n",
        "    'cohen_d': \"Cohen's d (uncorrected SMD)\",\n",
        "    'log_OR': 'Log Odds Ratio'\n",
        "}\n",
        "\n",
        "# --- Authentication ---\n",
        "print(\"=\" * 70)\n",
        "print(\"CO-META - INITIALIZATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# UTILITY FUNCTIONS - Extracted from original cells for reusability\n",
        "# =============================================================================\n",
        "\n",
        "# --- STATISTICAL FUNCTIONS ---\n",
        "\n",
        "def calculate_tau_squared_DL(df, effect_col, var_col):\n",
        "    \"\"\"\n",
        "    DerSimonian-Laird estimator for tau-squared\n",
        "\n",
        "    Advantages:\n",
        "    - Simple, fast\n",
        "    - Non-iterative\n",
        "    - Always converges\n",
        "\n",
        "    Disadvantages:\n",
        "    - Can underestimate tau² in small samples\n",
        "    - Negative values truncated to 0\n",
        "    - Less efficient than ML methods\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Data with effect sizes and variances\n",
        "    effect_col : str\n",
        "        Name of effect size column\n",
        "    var_col : str\n",
        "        Name of variance column\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : tau-squared estimate\n",
        "    \"\"\"\n",
        "    k = len(df)\n",
        "    if k < 2:\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        # Fixed-effects weights\n",
        "        w = 1 / df[var_col]\n",
        "        sum_w = w.sum()\n",
        "\n",
        "        if sum_w <= 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Fixed-effects pooled estimate\n",
        "        pooled_effect = (w * df[effect_col]).sum() / sum_w\n",
        "\n",
        "        # Q statistic\n",
        "        Q = (w * (df[effect_col] - pooled_effect)**2).sum()\n",
        "        df_Q = k - 1\n",
        "\n",
        "        # C constant\n",
        "        sum_w_sq = (w**2).sum()\n",
        "        C = sum_w - (sum_w_sq / sum_w)\n",
        "\n",
        "        # Tau-squared\n",
        "        if C > 0 and Q > df_Q:\n",
        "            tau_sq = (Q - df_Q) / C\n",
        "        else:\n",
        "            tau_sq = 0.0\n",
        "\n",
        "        return max(0.0, tau_sq)\n",
        "\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error in DL estimator: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "\n",
        "def calculate_tau_squared(df, effect_col, var_col, method='REML', **kwargs):\n",
        "    \"\"\"\n",
        "    Unified function to calculate tau-squared using specified method\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Data with effect sizes and variances\n",
        "    effect_col : str\n",
        "        Name of effect size column\n",
        "    var_col : str\n",
        "        Name of variance column\n",
        "    method : str\n",
        "        Estimation method: 'DL', 'REML', 'ML', 'PM', 'SJ'\n",
        "        Default: 'REML' (recommended)\n",
        "    **kwargs : dict\n",
        "        Additional arguments passed to estimator\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : tau-squared estimate\n",
        "    dict : additional information (method used, convergence, etc.)\n",
        "    \"\"\"\n",
        "    method = method.upper()\n",
        "\n",
        "    estimators = {\n",
        "        'DL': calculate_tau_squared_DL,\n",
        "        'REML': calculate_tau_squared_REML,\n",
        "        'ML': calculate_tau_squared_ML,\n",
        "        'PM': calculate_tau_squared_PM,\n",
        "        'SJ': calculate_tau_squared_SJ\n",
        "    }\n",
        "\n",
        "    if method not in estimators:\n",
        "        warnings.warn(f\"Unknown method '{method}', using REML\")\n",
        "        method = 'REML'\n",
        "\n",
        "    try:\n",
        "        tau_sq = estimators[method](df, effect_col, var_col, **kwargs)\n",
        "\n",
        "        info = {\n",
        "            'method': method,\n",
        "            'tau_squared': tau_sq,\n",
        "            'tau': np.sqrt(tau_sq),\n",
        "            'success': True\n",
        "        }\n",
        "\n",
        "        return tau_sq, info\n",
        "\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error with {method}, falling back to DL: {e}\")\n",
        "        tau_sq = calculate_tau_squared_DL(df, effect_col, var_col)\n",
        "\n",
        "        info = {\n",
        "            'method': 'DL',\n",
        "            'tau_squared': tau_sq,\n",
        "            'tau': np.sqrt(tau_sq),\n",
        "            'success': False,\n",
        "            'fallback': True,\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "        return tau_sq, info\n",
        "\n",
        "\n",
        "\n",
        "def calculate_tau_squared_dl(df, effect_col, var_col):\n",
        "    \"\"\"\n",
        "    Calculate Tau-squared. Uses Global Advanced Estimator (Cell 4.5) if available,\n",
        "    otherwise falls back to DerSimonian-Laird (DL).\n",
        "    \"\"\"\n",
        "    k = len(df)\n",
        "    if k < 2: return 0.0\n",
        "\n",
        "    # Try using the advanced REML estimator from Cell 4.5 first\n",
        "    if 'calculate_tau_squared' in globals():\n",
        "        tau_method = 'REML' # Prefer REML for consistency\n",
        "        try:\n",
        "            tau_sq, info = calculate_tau_squared(df, effect_col, var_col, method=tau_method)\n",
        "            if info.get('success', True):\n",
        "                return tau_sq\n",
        "        except Exception:\n",
        "            pass # Fall back to DL if REML fails (common in small cumulative steps)\n",
        "\n",
        "    # Classic DL Method (Fallback)\n",
        "    try:\n",
        "        w_fixed = 1 / df[var_col]\n",
        "        sum_w = w_fixed.sum()\n",
        "        if sum_w <= 0: return 0.0\n",
        "        pooled_effect = (w_fixed * df[effect_col]).sum() / sum_w\n",
        "        Qt = (w_fixed * (df[effect_col] - pooled_effect)**2).sum()\n",
        "        df_Q = k - 1\n",
        "        sum_w_sq = (w_fixed**2).sum()\n",
        "        C = sum_w - (sum_w_sq / sum_w)\n",
        "        if C > 0 and Qt > df_Q:\n",
        "            tau_squared = (Qt - df_Q) / C\n",
        "        else:\n",
        "            tau_squared = 0.0\n",
        "        return max(0.0, tau_squared)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def calculate_re_pooled(df, tau_squared, effect_col, var_col, alpha=0.05):\n",
        "    \"\"\"Calculate Random-Effects pooled estimate with CI\"\"\"\n",
        "    k = len(df)\n",
        "    if k < 1: return np.nan, np.nan, np.nan, np.nan, np.nan\n",
        "    try:\n",
        "        w_re = 1 / (df[var_col] + tau_squared)\n",
        "        sum_w_re = w_re.sum()\n",
        "        if sum_w_re <= 0: return np.nan, np.nan, np.nan, np.nan, np.nan\n",
        "\n",
        "        pooled_effect = (w_re * df[effect_col]).sum() / sum_w_re\n",
        "        pooled_var = 1 / sum_w_re\n",
        "        pooled_se = np.sqrt(pooled_var)\n",
        "\n",
        "        z_crit = norm.ppf(1 - alpha / 2)\n",
        "        ci_lower = pooled_effect - z_crit * pooled_se\n",
        "        ci_upper = pooled_effect + z_crit * pooled_se\n",
        "\n",
        "        # Calculate I-squared\n",
        "        w_fixed = 1 / df[var_col]\n",
        "        sum_w_fixed = w_fixed.sum()\n",
        "        pooled_effect_fe = (w_fixed * df[effect_col]).sum() / sum_w_fixed\n",
        "        Q = (w_fixed * (df[effect_col] - pooled_effect_fe)**2).sum()\n",
        "        df_Q = k - 1\n",
        "        I_sq = max(0, ((Q - df_Q) / Q) * 100) if Q > 0 else 0\n",
        "\n",
        "        return pooled_effect, pooled_se, ci_lower, ci_upper, I_sq\n",
        "    except Exception:\n",
        "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
        "\n",
        "\n",
        "def calculate_knapp_hartung_ci(yi, vi, tau_sq, pooled_effect, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Calculate Knapp-Hartung adjusted confidence interval\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    yi = np.array(yi)\n",
        "    vi = np.array(vi)\n",
        "\n",
        "    # Random-effects weights\n",
        "    wi_star = 1 / (vi + tau_sq)\n",
        "    sum_wi_star = np.sum(wi_star)\n",
        "\n",
        "    # Degrees of freedom\n",
        "    k = len(yi)\n",
        "    df = k - 1\n",
        "\n",
        "    if df <= 0:\n",
        "        # Can't use K-H with k=1\n",
        "        return None\n",
        "\n",
        "    # Calculate Q statistic (residual heterogeneity)\n",
        "    Q = np.sum(wi_star * (yi - pooled_effect)**2)\n",
        "\n",
        "    # Standard random-effects variance\n",
        "    var_standard = 1 / sum_wi_star\n",
        "\n",
        "    # Knapp-Hartung adjusted variance\n",
        "    # SE_KH² = (Q / (k-1)) × (1 / Σw*)\n",
        "    var_KH = (Q / df) * var_standard\n",
        "    se_KH = np.sqrt(var_KH)\n",
        "\n",
        "    # t-distribution critical value\n",
        "    t_crit = t.ppf(1 - alpha/2, df)\n",
        "\n",
        "    # Confidence interval\n",
        "    ci_lower = pooled_effect - t_crit * se_KH\n",
        "    ci_upper = pooled_effect + t_crit * se_KH\n",
        "\n",
        "    # Test statistic and p-value\n",
        "    t_stat = pooled_effect / se_KH\n",
        "    p_value = 2 * (1 - t.cdf(abs(t_stat), df))\n",
        "\n",
        "    return {\n",
        "        'se_KH': se_KH,\n",
        "        'var_KH': var_KH,\n",
        "        'ci_lower': ci_lower,\n",
        "        'ci_upper': ci_upper,\n",
        "        't_stat': t_stat,\n",
        "        't_crit': t_crit,\n",
        "        'df': df,\n",
        "        'p_value': p_value,\n",
        "        'Q': Q\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def compare_tau_estimators(df, effect_col, var_col):\n",
        "    \"\"\"\n",
        "    Compare all tau-squared estimators on the same dataset\n",
        "\n",
        "    Useful for sensitivity analysis and understanding which method\n",
        "    is most appropriate for your data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Data with effect sizes and variances\n",
        "    effect_col : str\n",
        "        Name of effect size column\n",
        "    var_col : str\n",
        "        Name of variance column\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame : Comparison of all methods\n",
        "    \"\"\"\n",
        "    methods = ['DL', 'REML', 'ML', 'PM', 'SJ']\n",
        "    results = []\n",
        "\n",
        "    for method in methods:\n",
        "        try:\n",
        "            tau_sq, info = calculate_tau_squared(df, effect_col, var_col, method=method)\n",
        "\n",
        "            results.append({\n",
        "                'Method': method,\n",
        "                'τ²': tau_sq,\n",
        "                'τ': np.sqrt(tau_sq),\n",
        "                'Success': info['success']\n",
        "            })\n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                'Method': method,\n",
        "                'τ²': np.nan,\n",
        "                'τ': np.nan,\n",
        "                'Success': False\n",
        "            })\n",
        "\n",
        "    comparison_df = pd.DataFrame(results)\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "\n",
        "\n",
        "def calculate_hedges_g_python(df):\n",
        "    \"\"\"Calculate Hedges' g using EXACT Gamma correction.\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Pooled SD\n",
        "    n_e, n_c = df['ne'], df['nc']\n",
        "    sd_e, sd_c = df['sde'], df['sdc']\n",
        "    mean_e, mean_c = df['xe'], df['xc']\n",
        "\n",
        "    df_d = n_e + n_c - 2\n",
        "    sd_pooled = np.sqrt(((n_e - 1)*sd_e**2 + (n_c - 1)*sd_c**2) / df_d)\n",
        "\n",
        "    # Cohen's d\n",
        "    d = (mean_e - mean_c) / sd_pooled\n",
        "\n",
        "    # Hedges' correction (J) - EXACT FORMULA to match metafor\n",
        "    # J = exp(lgamma(m/2) - log(sqrt(m/2)) - lgamma((m-1)/2))\n",
        "    m = df_d\n",
        "    J = gamma(m / 2) / (np.sqrt(m / 2) * gamma((m - 1) / 2))\n",
        "\n",
        "    g = d * J\n",
        "\n",
        "    # Variance of g (Exact)\n",
        "    vg = ((n_e + n_c) / (n_e * n_c) + (g**2 / (2 * (n_e + n_c)))) * J**2\n",
        "\n",
        "    return g, vg\n",
        "\n",
        "\n",
        "# --- THREE-LEVEL MODEL FUNCTIONS ---\n",
        "\n",
        "def _get_three_level_estimates(params, y_all, vcv_all, N_total, M_studies):\n",
        "    \"\"\"\n",
        "    Core function to calculate 3-level estimates using VCV matrices.\n",
        "    FIXED: Proper REML log-likelihood formula to match metafor.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tau_sq, sigma_sq = params\n",
        "        # Safety check for negatives\n",
        "        if tau_sq < 0: tau_sq = 1e-10\n",
        "        if sigma_sq < 0: sigma_sq = 1e-10\n",
        "\n",
        "        sum_log_det_Vi = 0.0\n",
        "        sum_S = 0.0       # 1' * V_i⁻¹ * 1\n",
        "        sum_Sy = 0.0      # 1' * V_i⁻¹ * y_i\n",
        "        sum_ySy = 0.0     # y_i' * V_i⁻¹ * y_i\n",
        "\n",
        "        for i in range(M_studies):\n",
        "            y_i = np.asarray(y_all[i], dtype=np.float64)\n",
        "            V_i = np.asarray(vcv_all[i], dtype=np.float64)\n",
        "            k = len(y_i)\n",
        "\n",
        "            # Ensure V_i is 2D\n",
        "            if V_i.ndim == 1:\n",
        "                V_i = np.diag(V_i)\n",
        "\n",
        "            # --- CONSTRUCT TOTAL VARIANCE MATRIX (Sigma_i) ---\n",
        "            # Sigma_i = V_i (Sampling Error) + sigma²*I (Level 2) + tau²*J (Level 3)\n",
        "\n",
        "            # Check if V_i is diagonal (no shared controls) - use fast path\n",
        "            is_diagonal = (k == 1) or np.allclose(V_i, np.diag(np.diag(V_i)))\n",
        "\n",
        "            if is_diagonal:\n",
        "                # --- FAST PATH: Sherman-Morrison Formula ---\n",
        "                v_i = np.diag(V_i) if k > 1 else np.array([V_i[0, 0]])\n",
        "\n",
        "                # A = diag(v_ij + σ²)\n",
        "                A_diag = v_i + sigma_sq\n",
        "                inv_A_diag = 1.0 / A_diag\n",
        "\n",
        "                # Sherman-Morrison components\n",
        "                sum_inv_A = np.sum(inv_A_diag)\n",
        "                denom = 1.0 + tau_sq * sum_inv_A\n",
        "\n",
        "                # Log Determinant: |A + tau²*J| = |A| * (1 + tau² * 1'A⁻¹1)\n",
        "                log_det_A = np.sum(np.log(A_diag))\n",
        "                log_det_Vi = log_det_A + np.log(denom)\n",
        "\n",
        "                # Sherman-Morrison inverse: (A + tau²*J)⁻¹ = A⁻¹ - (tau² * A⁻¹ * J * A⁻¹) / (1 + tau² * trace(A⁻¹))\n",
        "                # V⁻¹y\n",
        "                inv_A_y = inv_A_diag * y_i\n",
        "                sum_inv_A_y = np.sum(inv_A_y)\n",
        "                w_y = inv_A_y - (tau_sq * inv_A_diag * sum_inv_A_y) / denom\n",
        "\n",
        "                # V⁻¹1\n",
        "                w_1 = inv_A_diag - (tau_sq * inv_A_diag * sum_inv_A) / denom\n",
        "\n",
        "\n",
        "            else:\n",
        "                # --- FULL PATH: Matrix Inversion for Shared Controls ---\n",
        "                Sigma_i = V_i.copy()\n",
        "\n",
        "                # Add Level 2 (within-study): σ² on diagonal\n",
        "                # Use faster fill_diagonal logic\n",
        "                np.fill_diagonal(Sigma_i, np.diag(Sigma_i) + sigma_sq)\n",
        "\n",
        "                # Add Level 3 (between-study): τ² for all elements\n",
        "                Sigma_i += tau_sq\n",
        "\n",
        "                # --- INVERSION & DETERMINANT ---\n",
        "                try:\n",
        "                    # Try Cholesky first (Fastest & Most Stable)\n",
        "                    L = np.linalg.cholesky(Sigma_i)\n",
        "                    # Solve Ax = I is faster/stable than explicit inv(A)\n",
        "                    inv_Sigma_i = np.linalg.solve(Sigma_i, np.eye(k))\n",
        "                    log_det_Vi = 2.0 * np.sum(np.log(np.diag(L)))\n",
        "                except np.linalg.LinAlgError:\n",
        "                    # Fallback 1: Add Jitter (Ridge) and retry Cholesky\n",
        "                    try:\n",
        "                        Sigma_ridged = Sigma_i + 1e-6 * np.eye(k)\n",
        "                        L = np.linalg.cholesky(Sigma_ridged)\n",
        "                        inv_Sigma_i = np.linalg.solve(Sigma_ridged, np.eye(k))\n",
        "                        log_det_Vi = 2.0 * np.sum(np.log(np.diag(L)))\n",
        "                    except np.linalg.LinAlgError:\n",
        "                        # Fallback 2: Pseudo-Inverse (Slowest, but guarantees result)\n",
        "                        inv_Sigma_i = np.linalg.pinv(Sigma_i)\n",
        "                        sign, log_det_Vi = np.linalg.slogdet(Sigma_i)\n",
        "                        if sign <= 0:\n",
        "                            return {'log_lik_reml': np.inf}\n",
        "\n",
        "                ones = np.ones(k)\n",
        "                w_y = np.dot(inv_Sigma_i, y_i)\n",
        "                w_1 = np.dot(inv_Sigma_i, ones)\n",
        "\n",
        "            # Accumulate across studies\n",
        "            sum_log_det_Vi += log_det_Vi\n",
        "            sum_S += np.sum(w_1)\n",
        "            sum_Sy += np.sum(w_y)\n",
        "            sum_ySy += np.dot(y_i, w_y)\n",
        "\n",
        "        if sum_S <= 1e-10:\n",
        "            return {'log_lik_reml': np.inf}\n",
        "\n",
        "        # --- FINAL ESTIMATES ---\n",
        "        mu_hat = sum_Sy / sum_S\n",
        "        var_mu = 1.0 / sum_S\n",
        "        se_mu = np.sqrt(var_mu)\n",
        "\n",
        "        # Residual Sum of Squares (quadratic form)\n",
        "        residual_ss = sum_ySy - (sum_Sy ** 2) / sum_S  # Equivalent but more stable\n",
        "\n",
        "        # REML Log-Likelihood (matches metafor::rma.mv)\n",
        "        # -2 * logLik = log|V| + log|X'V⁻¹X| + (y-Xμ)'V⁻¹(y-Xμ) + (n-p)*log(2π)\n",
        "        # For intercept-only: X'V⁻¹X = sum_S, so log|X'V⁻¹X| = log(sum_S)\n",
        "        p = 1  # Number of fixed effects (intercept only)\n",
        "\n",
        "        log_lik_reml = -0.5 * (\n",
        "            (N_total - p) * np.log(2.0 * np.pi) +  # Constant term\n",
        "            sum_log_det_Vi +                        # log|V|\n",
        "            np.log(sum_S) +                         # log|X'V⁻¹X|\n",
        "            residual_ss                             # Residual quadratic form\n",
        "        )\n",
        "\n",
        "        # ML Log-Likelihood\n",
        "        log_lik_ml = -0.5 * (\n",
        "            N_total * np.log(2.0 * np.pi) +\n",
        "            sum_log_det_Vi +\n",
        "            residual_ss\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'mu': mu_hat, 'se_mu': se_mu, 'var_mu': var_mu,\n",
        "            'log_lik_reml': log_lik_reml, 'log_lik_ml': log_lik_ml,\n",
        "            'tau_sq': tau_sq, 'sigma_sq': sigma_sq\n",
        "        }\n",
        "\n",
        "    except (FloatingPointError, ValueError, np.linalg.LinAlgError) as e:\n",
        "        return {'log_lik_reml': np.inf}\n",
        "\n",
        "\n",
        "def _negative_log_likelihood_reml(params, y_all, vcv_all, N_total, M_studies):\n",
        "    \"\"\"Wrapper for optimizer.\"\"\"\n",
        "    estimates = _get_three_level_estimates(params, y_all, vcv_all, N_total, M_studies)\n",
        "    return -estimates['log_lik_reml']\n",
        "\n",
        "\n",
        "def _get_three_level_estimates_loo(params, y_all, vcv_all, N_total, M_studies):\n",
        "    \"\"\"\n",
        "    Core calculation for 3-level estimates with VCV matrix support (LOO Version).\n",
        "    Silent version without ML likelihood calculation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tau_sq, sigma_sq = params\n",
        "        if tau_sq < 0: tau_sq = 1e-10\n",
        "        if sigma_sq < 0: sigma_sq = 1e-10\n",
        "\n",
        "        sum_log_det_Vi = 0.0\n",
        "        sum_S = 0.0\n",
        "        sum_Sy = 0.0\n",
        "        sum_ySy = 0.0\n",
        "\n",
        "        for i in range(M_studies):\n",
        "            y_i = y_all[i]\n",
        "            V_i = vcv_all[i]  # Now using VCV matrix\n",
        "            k = len(y_i)\n",
        "\n",
        "            # Check if diagonal (no shared controls) - use fast path\n",
        "            is_diagonal = (k == 1) or np.allclose(V_i, np.diag(np.diag(V_i)))\n",
        "\n",
        "            if is_diagonal:\n",
        "                # Fast path: Sherman-Morrison\n",
        "                v_i = np.diag(V_i) if k > 1 else np.array([V_i[0, 0]])\n",
        "\n",
        "                A_diag = v_i + sigma_sq\n",
        "                inv_A_diag = 1.0 / A_diag\n",
        "                sum_inv_A = np.sum(inv_A_diag)\n",
        "                denom = 1 + tau_sq * sum_inv_A\n",
        "\n",
        "                log_det_A = np.sum(np.log(A_diag))\n",
        "                sum_log_det_Vi += log_det_A + np.log(denom)\n",
        "\n",
        "                inv_A_y = inv_A_diag * y_i\n",
        "                sum_inv_A_y = np.sum(inv_A_y)\n",
        "\n",
        "                w_y = inv_A_y - (tau_sq * inv_A_diag * sum_inv_A_y) / denom\n",
        "                w_1 = inv_A_diag - (tau_sq * inv_A_diag * sum_inv_A) / denom\n",
        "            else:\n",
        "                # Full path: Matrix inversion for shared controls\n",
        "                Sigma_i = V_i.copy()\n",
        "                np.fill_diagonal(Sigma_i, np.diag(Sigma_i) + sigma_sq)\n",
        "                Sigma_i += tau_sq\n",
        "\n",
        "                try:\n",
        "                    L = np.linalg.cholesky(Sigma_i)\n",
        "                    inv_Sigma_i = np.linalg.inv(Sigma_i)\n",
        "                    log_det_Vi = 2 * np.sum(np.log(np.diag(L)))\n",
        "                except np.linalg.LinAlgError:\n",
        "                    inv_Sigma_i = np.linalg.pinv(Sigma_i)\n",
        "                    sign, log_det_Vi = np.linalg.slogdet(Sigma_i)\n",
        "                    if sign <= 0:\n",
        "                        log_det_Vi = 1e10\n",
        "\n",
        "                sum_log_det_Vi += log_det_Vi\n",
        "\n",
        "                ones = np.ones(k)\n",
        "                w_y = np.dot(inv_Sigma_i, y_i)\n",
        "                w_1 = np.dot(inv_Sigma_i, ones)\n",
        "\n",
        "            sum_S += np.sum(w_1)\n",
        "            sum_Sy += np.sum(w_y)\n",
        "            sum_ySy += np.dot(y_i, w_y)\n",
        "\n",
        "        if sum_S <= 1e-10: return {'log_lik_reml': np.inf}\n",
        "\n",
        "        mu_hat = sum_Sy / sum_S\n",
        "        var_mu = 1.0 / sum_S\n",
        "        se_mu = np.sqrt(var_mu)\n",
        "        residual_ss = sum_ySy - 2.0 * mu_hat * sum_Sy + mu_hat**2 * sum_S\n",
        "\n",
        "        log_lik_reml = -0.5 * (sum_log_det_Vi + np.log(sum_S) + residual_ss)\n",
        "        if np.isnan(log_lik_reml): return {'log_lik_reml': np.inf}\n",
        "\n",
        "        return {'mu': mu_hat, 'se_mu': se_mu, 'log_lik_reml': log_lik_reml,\n",
        "                'tau_sq': tau_sq, 'sigma_sq': sigma_sq}\n",
        "\n",
        "    except (FloatingPointError, ValueError, np.linalg.LinAlgError):\n",
        "        return {'log_lik_reml': np.inf}\n",
        "\n",
        "\n",
        "def _neg_log_lik_reml_loo(params, y_all, vcv_all, N_total, M_studies):\n",
        "    \"\"\"Wrapper for LOO optimizer (short name version).\"\"\"\n",
        "    est = _get_three_level_estimates_loo(params, y_all, vcv_all, N_total, M_studies)\n",
        "    return -est['log_lik_reml']\n",
        "\n",
        "\n",
        "def _negative_log_likelihood_reml_loo(params, y_all, vcv_all, N_total, M_studies):\n",
        "    \"\"\"Wrapper for LOO optimizer (full name version).\"\"\"\n",
        "    estimates = _get_three_level_estimates_loo(params, y_all, vcv_all, N_total, M_studies)\n",
        "    return -estimates['log_lik_reml']\n",
        "\n",
        "\n",
        "\n",
        "def _neg_log_lik_reml(params, y, v, groups):\n",
        "    tau2, sigma2 = params\n",
        "    # Bounds are handled by optimizer, but safe-guard here for math domain errors\n",
        "    if tau2 < 0: tau2 = 1e-10\n",
        "    if sigma2 < 0: sigma2 = 1e-10\n",
        "\n",
        "    unique_groups = np.unique(groups)\n",
        "\n",
        "    log_lik = 0\n",
        "    sum_S = 0\n",
        "    sum_Sy = 0\n",
        "    sum_ySy = 0\n",
        "\n",
        "    for grp in unique_groups:\n",
        "        mask = (groups == grp)\n",
        "        y_i = y[mask]\n",
        "        v_i = v[mask]\n",
        "\n",
        "        # V_i = D + sigma2*I + tau2*J\n",
        "        # A = D + sigma2*I (Diagonal matrix)\n",
        "        A_diag = v_i + sigma2\n",
        "        inv_A_diag = 1.0 / A_diag\n",
        "\n",
        "        # Woodbury/Sherman-Morrison components\n",
        "        # (A + uv^T)^-1 = A^-1 - (A^-1 u v^T A^-1) / (1 + v^T A^-1 u)\n",
        "        # Here u = v = tau * 1\n",
        "\n",
        "        sum_inv_A = np.sum(inv_A_diag)\n",
        "        denom = 1 + tau2 * sum_inv_A\n",
        "\n",
        "        # Log Determinant of V_i\n",
        "        # det(A + uv^T) = det(A) * (1 + v^T A^-1 u)\n",
        "        log_det_A = np.sum(np.log(A_diag))\n",
        "        log_det_Vi = log_det_A + np.log(denom)\n",
        "        log_lik += log_det_Vi\n",
        "\n",
        "        # Inversion Operations\n",
        "        inv_A_y = inv_A_diag * y_i\n",
        "        # w_y = V_i^-1 * y_i\n",
        "        w_y = inv_A_y - (tau2 * inv_A_diag * np.sum(inv_A_y)) / denom\n",
        "\n",
        "        # w_1 = V_i^-1 * 1\n",
        "        w_1 = inv_A_diag - (tau2 * inv_A_diag * sum_inv_A) / denom\n",
        "\n",
        "        sum_S += np.sum(w_1)      # 1^T V^-1 1\n",
        "        sum_Sy += np.sum(w_y)     # 1^T V^-1 y\n",
        "        sum_ySy += np.dot(y_i, w_y) # y^T V^-1 y\n",
        "\n",
        "    # REML Profile Likelihood Calculation\n",
        "    mu = sum_Sy / sum_S\n",
        "    resid = sum_ySy - 2*mu*sum_Sy + mu**2 * sum_S\n",
        "\n",
        "    # Full REML Log Likelihood\n",
        "    total_log_lik = -0.5 * (log_lik + np.log(sum_S) + resid)\n",
        "\n",
        "    return -total_log_lik\n",
        "\n",
        "\n",
        "def run_python_3level(yi, vi, study_ids):\n",
        "    # 1. First Pass: L-BFGS-B (Global search)\n",
        "    best_res = None\n",
        "    best_fun = np.inf\n",
        "\n",
        "    # Multiple start points to avoid local minima\n",
        "    start_points = [[0.01, 0.01], [0.5, 0.1], [0.1, 0.5], [0.001, 0.001]]\n",
        "\n",
        "    for start in start_points:\n",
        "        res = minimize(_neg_log_lik_reml, x0=start, args=(yi, vi, study_ids),\n",
        "                       bounds=[(1e-8, None), (1e-8, None)],\n",
        "                       method='L-BFGS-B',\n",
        "                       options={'ftol': 1e-12, 'gtol': 1e-12}) # High precision\n",
        "        if res.success and res.fun < best_fun:\n",
        "            best_fun = res.fun\n",
        "            best_res = res\n",
        "\n",
        "    if not best_res: return None\n",
        "\n",
        "    # 2. Second Pass: Nelder-Mead (Polishing)\n",
        "    # Sometimes gradient methods get stuck slightly off in flat valleys\n",
        "    final_res = minimize(_neg_log_lik_reml, x0=best_res.x, args=(yi, vi, study_ids),\n",
        "                         method='Nelder-Mead',\n",
        "                         bounds=[(1e-8, None), (1e-8, None)],\n",
        "                         options={'xatol': 1e-12, 'fatol': 1e-12})\n",
        "\n",
        "    tau2, sigma2 = final_res.x\n",
        "    return tau2, sigma2\n",
        "\n",
        "\n",
        "# --- REGRESSION FUNCTIONS ---\n",
        "\n",
        "def _estimate_variance_from_intercept_model(y_all, v_all, M_studies):\n",
        "    \"\"\"\n",
        "    Estimate tau² and sigma² from intercept-only 3-level model.\n",
        "    Used to get reliable starting values when moderator is constant within studies.\n",
        "    \"\"\"\n",
        "    JITTER = 1e-8\n",
        "\n",
        "    def neg_ll_intercept(params):\n",
        "        tau_sq, sigma_sq = max(params[0], 1e-10), max(params[1], 1e-10)\n",
        "\n",
        "        sum_log_det = 0.0\n",
        "        sum_w = 0.0\n",
        "        sum_wy = 0.0\n",
        "        sum_ywy = 0.0\n",
        "\n",
        "        for i in range(M_studies):\n",
        "            y_i, v_i = y_all[i], v_all[i]\n",
        "            A_diag = v_i + sigma_sq + JITTER\n",
        "            inv_A = 1.0 / A_diag\n",
        "            sum_inv_A = np.sum(inv_A)\n",
        "            denom = 1.0 + tau_sq * sum_inv_A\n",
        "\n",
        "            w_i = sum_inv_A - (tau_sq * sum_inv_A**2) / denom\n",
        "            inv_A_y = inv_A * y_i\n",
        "            sum_inv_A_y = np.sum(inv_A_y)\n",
        "            wy_i = sum_inv_A_y - (tau_sq * sum_inv_A * sum_inv_A_y) / denom\n",
        "            ywy_i = np.dot(y_i, inv_A_y) - (tau_sq * sum_inv_A_y**2) / denom\n",
        "\n",
        "            sum_w += w_i\n",
        "            sum_wy += wy_i\n",
        "            sum_ywy += ywy_i\n",
        "            sum_log_det += np.sum(np.log(A_diag)) + np.log(denom)\n",
        "\n",
        "        rss = sum_ywy - (sum_wy**2) / sum_w\n",
        "        log_lik = -0.5 * (sum_log_det + np.log(sum_w) + rss)\n",
        "        return -log_lik if np.isfinite(log_lik) else 1e10\n",
        "\n",
        "    # Multi-start optimization\n",
        "    starts = [[0.1, 0.1], [1.0, 0.1], [5.0, 0.5], [0.5, 1.0], [10.0, 0.1]]\n",
        "    best_res, best_fun = None, np.inf\n",
        "\n",
        "    for start in starts:\n",
        "        try:\n",
        "            res = minimize(neg_ll_intercept, x0=start, method='L-BFGS-B',\n",
        "                          bounds=[(1e-8, None), (1e-8, None)], options={'ftol': 1e-12})\n",
        "            if res.success and res.fun < best_fun:\n",
        "                best_fun, best_res = res.fun, res\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if best_res is None:\n",
        "        return 1.0, 0.1\n",
        "    return max(best_res.x[0], 1e-8), max(best_res.x[1], 1e-8)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CORE: _get_three_level_regression_estimates_v2 (BACKWARD COMPATIBLE)\n",
        "# =============================================================================\n",
        "\n",
        "def _get_three_level_regression_estimates_v2(params, y_all, v_all, X_all,\n",
        "                                              N_total, M_studies, p_params):\n",
        "    \"\"\"\n",
        "    Calculates betas, standard errors, p-values, and likelihood.\n",
        "\n",
        "    BACKWARD COMPATIBLE: Returns all original keys plus new ones.\n",
        "    \"\"\"\n",
        "    JITTER = 1e-8\n",
        "\n",
        "    try:\n",
        "        tau_sq = max(params[0], 1e-10)\n",
        "        sigma_sq = max(params[1], 1e-10)\n",
        "\n",
        "        sum_log_det_Vi = 0.0\n",
        "        sum_XWX = np.zeros((p_params, p_params))\n",
        "        sum_XWy = np.zeros(p_params)\n",
        "        sum_yWy = 0.0\n",
        "\n",
        "        for i in range(M_studies):\n",
        "            y_i = y_all[i]\n",
        "            v_i = v_all[i]\n",
        "            X_i = X_all[i]\n",
        "\n",
        "            # V_i = diag(v_i) + sigma²*I + tau²*J, inverted via Sherman-Morrison\n",
        "            A_diag = v_i + sigma_sq + JITTER\n",
        "            inv_A_diag = 1.0 / A_diag\n",
        "            sum_inv_A = np.sum(inv_A_diag)\n",
        "            denom = max(1.0 + tau_sq * sum_inv_A, JITTER)\n",
        "\n",
        "            # Log determinant\n",
        "            log_det_A = np.sum(np.log(A_diag))\n",
        "            sum_log_det_Vi += log_det_A + np.log(denom)\n",
        "\n",
        "            # Efficient X'V⁻¹X and X'V⁻¹y computation\n",
        "            inv_A_X = inv_A_diag[:, None] * X_i\n",
        "            inv_A_y = inv_A_diag * y_i\n",
        "            sum_inv_A_X = np.sum(inv_A_X, axis=0)\n",
        "            sum_inv_A_y = np.sum(inv_A_y)\n",
        "\n",
        "            xt_invA_x = X_i.T @ inv_A_X\n",
        "            correction_term = (tau_sq / denom) * np.outer(sum_inv_A_X, sum_inv_A_X)\n",
        "            sum_XWX += xt_invA_x - correction_term\n",
        "\n",
        "            xt_invA_y = X_i.T @ inv_A_y\n",
        "            correction_y = (tau_sq / denom) * sum_inv_A_X * sum_inv_A_y\n",
        "            sum_XWy += xt_invA_y - correction_y\n",
        "\n",
        "            yt_invA_y = np.dot(y_i, inv_A_y)\n",
        "            correction_yy = (tau_sq / denom) * (sum_inv_A_y ** 2)\n",
        "            sum_yWy += yt_invA_y - correction_yy\n",
        "\n",
        "        # Add jitter for numerical stability\n",
        "        sum_XWX += JITTER * np.eye(p_params)\n",
        "\n",
        "        try:\n",
        "            betas = np.linalg.solve(sum_XWX, sum_XWy)\n",
        "            var_betas = np.linalg.inv(sum_XWX)\n",
        "        except np.linalg.LinAlgError:\n",
        "            return {'log_lik_reml': np.inf}\n",
        "\n",
        "        se_betas = np.sqrt(np.diag(var_betas))\n",
        "\n",
        "        # === CORRECT DF CALCULATION ===\n",
        "        # For 3-level models, metafor uses different df depending on the moderator:\n",
        "        # - If moderator varies within studies: df = k_studies - p (conservative)\n",
        "        # - If moderator is constant within studies (study-level): df = n_obs - p\n",
        "        #\n",
        "        # We detect this by checking if sigma_sq ≈ 0 (within-study variance negligible)\n",
        "        # When sigma_sq → 0, the model effectively treats each observation as independent\n",
        "\n",
        "        if sigma_sq < 1e-4:\n",
        "            # Study-level moderator: use observation-based df (matches metafor)\n",
        "            df = max(N_total - p_params, 1)\n",
        "        else:\n",
        "            # Effect-level moderator: use study-based df (conservative)\n",
        "            df = max(M_studies - p_params, 1)\n",
        "\n",
        "        t_values = betas / se_betas\n",
        "        p_values = 2.0 * t_dist.sf(np.abs(t_values), df=df)\n",
        "\n",
        "        t_crit = t_dist.ppf(0.975, df=df)\n",
        "        ci_lower = betas - t_crit * se_betas\n",
        "        ci_upper = betas + t_crit * se_betas\n",
        "\n",
        "        # REML likelihood\n",
        "        residual_ss = sum_yWy - np.dot(betas, sum_XWy)\n",
        "        sign, log_det_XWX = np.linalg.slogdet(sum_XWX)\n",
        "        if sign <= 0:\n",
        "            return {'log_lik_reml': np.inf}\n",
        "        log_lik_reml = -0.5 * (sum_log_det_Vi + log_det_XWX + residual_ss)\n",
        "\n",
        "        # === RETURN: All original keys + new ones ===\n",
        "        return {\n",
        "            # Original keys (backward compatible)\n",
        "            'betas': betas,\n",
        "            'se_betas': se_betas,\n",
        "            'var_betas_robust': var_betas,  # Original key name preserved\n",
        "            'log_lik_reml': log_lik_reml,\n",
        "            'tau_sq': tau_sq,\n",
        "            'sigma_sq': sigma_sq,\n",
        "            # New keys (additions)\n",
        "            'var_betas': var_betas,         # Alias\n",
        "            't_values': t_values,\n",
        "            'p_values': p_values,\n",
        "            'df': df,                        # df depends on moderator type\n",
        "            'ci_lower': ci_lower,\n",
        "            'ci_upper': ci_upper,\n",
        "            'n_obs': N_total,               # For reference\n",
        "            'k_studies': M_studies,         # For reference\n",
        "        }\n",
        "\n",
        "    except (FloatingPointError, ValueError, np.linalg.LinAlgError):\n",
        "        return {'log_lik_reml': np.inf}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# NEGATIVE LOG-LIKELIHOOD (unchanged interface)\n",
        "# =============================================================================\n",
        "\n",
        "def _neg_log_lik_reml_reg(params, y_all, v_all, X_all, N_total, M_studies, p_params):\n",
        "    \"\"\"Negative REML log-likelihood for optimization.\"\"\"\n",
        "    tau_sq = max(params[0], 1e-10)\n",
        "    sigma_sq = max(params[1], 1e-10)\n",
        "\n",
        "    est = _get_three_level_regression_estimates_v2(\n",
        "        [tau_sq, sigma_sq], y_all, v_all, X_all, N_total, M_studies, p_params\n",
        "    )\n",
        "\n",
        "    ll = est.get('log_lik_reml', np.inf)\n",
        "    return -ll if np.isfinite(ll) else 1e10\n",
        "\n",
        "\n",
        "def _neg_log_lik_reml_reg_constrained(params, y_all, v_all, X_all,\n",
        "                                       N_total, M_studies, p_params,\n",
        "                                       tau_sq_prior, penalty_weight):\n",
        "    \"\"\"REML likelihood with soft constraint on tau² toward prior.\"\"\"\n",
        "    tau_sq = max(params[0], 1e-10)\n",
        "    sigma_sq = max(params[1], 1e-10)\n",
        "\n",
        "    est = _get_three_level_regression_estimates_v2(\n",
        "        [tau_sq, sigma_sq], y_all, v_all, X_all, N_total, M_studies, p_params\n",
        "    )\n",
        "\n",
        "    ll = est.get('log_lik_reml', np.inf)\n",
        "    if not np.isfinite(ll):\n",
        "        return 1e10\n",
        "\n",
        "    # Penalty pulls tau² toward intercept-only estimate\n",
        "    penalty = penalty_weight * (np.log(tau_sq) - np.log(tau_sq_prior))**2\n",
        "    return -ll + penalty\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN: _run_three_level_reml_regression_v2 (BACKWARD COMPATIBLE)\n",
        "# =============================================================================\n",
        "\n",
        "def _run_three_level_reml_regression_v2(analysis_data, moderator_col, effect_col, var_col):\n",
        "    \"\"\"\n",
        "    3-level REML meta-regression with automatic handling of constant moderators.\n",
        "\n",
        "    BACKWARD COMPATIBLE: Same signature, same return structure.\n",
        "    Internally uses constrained optimization when moderator is constant within studies.\n",
        "    \"\"\"\n",
        "    grouped = analysis_data.groupby('id')\n",
        "    y_all, v_all, X_all = [], [], []\n",
        "\n",
        "    for study_id, group in grouped:\n",
        "        y_all.append(group[effect_col].values.astype(float))\n",
        "        v_all.append(group[var_col].values.astype(float))\n",
        "\n",
        "        # Build design matrix with intercept\n",
        "        mod_values = group[moderator_col].values.astype(float)\n",
        "        n_i = len(mod_values)\n",
        "\n",
        "        # Manually construct X = [1, moderator] to ensure intercept is always present\n",
        "        # sm.add_constant can fail when moderator is constant within a group\n",
        "        X_i = np.column_stack([np.ones(n_i), mod_values])\n",
        "        X_all.append(X_i)\n",
        "\n",
        "    N_total = len(analysis_data)\n",
        "    M_studies = len(y_all)\n",
        "    p_params = X_all[0].shape[1]\n",
        "\n",
        "    # === AUTO-DETECT: Is moderator constant within studies? ===\n",
        "    is_constant_within = analysis_data.groupby('id')[moderator_col].nunique().max() == 1\n",
        "\n",
        "    # === STAGE 1: Get tau² from intercept-only model ===\n",
        "    tau_sq_prior, sigma_sq_prior = _estimate_variance_from_intercept_model(\n",
        "        y_all, v_all, M_studies\n",
        "    )\n",
        "\n",
        "    # === STAGE 2: Choose optimization strategy ===\n",
        "    if is_constant_within:\n",
        "        # Constrained optimization to prevent tau² collapse\n",
        "        starts = [\n",
        "            [tau_sq_prior, sigma_sq_prior],\n",
        "            [tau_sq_prior * 0.5, sigma_sq_prior],\n",
        "            [tau_sq_prior * 2.0, sigma_sq_prior],\n",
        "        ]\n",
        "        penalty_weight = 5.0\n",
        "\n",
        "        best_res, best_fun = None, np.inf\n",
        "        for start in starts:\n",
        "            try:\n",
        "                res = minimize(\n",
        "                    _neg_log_lik_reml_reg_constrained,\n",
        "                    x0=start,\n",
        "                    args=(y_all, v_all, X_all, N_total, M_studies, p_params,\n",
        "                          tau_sq_prior, penalty_weight),\n",
        "                    method='L-BFGS-B',\n",
        "                    bounds=[(1e-8, None), (1e-8, None)],\n",
        "                    options={'ftol': 1e-12, 'maxiter': 1000}\n",
        "                )\n",
        "                if res.fun < best_fun:\n",
        "                    best_fun, best_res = res.fun, res\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        if best_res is None:\n",
        "            # Fallback: use intercept-only estimates directly\n",
        "            best_res = type('obj', (object,), {\n",
        "                'x': [tau_sq_prior, sigma_sq_prior],\n",
        "                'success': True,\n",
        "                'fun': np.inf\n",
        "            })()\n",
        "    else:\n",
        "        # Standard joint optimization (original behavior)\n",
        "        start_points = [\n",
        "            [tau_sq_prior, sigma_sq_prior],\n",
        "            [0.1, 0.1],\n",
        "            [1.0, 0.1],\n",
        "            [5.0, 0.1],\n",
        "            [10.0, 0.5],\n",
        "            [0.01, 1.0],\n",
        "        ]\n",
        "\n",
        "        best_res, best_fun = None, np.inf\n",
        "        for start in start_points:\n",
        "            try:\n",
        "                res = minimize(\n",
        "                    _neg_log_lik_reml_reg, x0=start,\n",
        "                    args=(y_all, v_all, X_all, N_total, M_studies, p_params),\n",
        "                    method='L-BFGS-B', bounds=[(1e-8, None), (1e-8, None)],\n",
        "                    options={'ftol': 1e-10}\n",
        "                )\n",
        "                if res.success and res.fun < best_fun:\n",
        "                    best_fun, best_res = res.fun, res\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        if best_res is None:\n",
        "            # Fallback: Nelder-Mead\n",
        "            try:\n",
        "                best_res = minimize(\n",
        "                    _neg_log_lik_reml_reg, x0=[1.0, 1.0],\n",
        "                    args=(y_all, v_all, X_all, N_total, M_studies, p_params),\n",
        "                    method='Nelder-Mead'\n",
        "                )\n",
        "            except Exception:\n",
        "                return None, None, None\n",
        "\n",
        "    if best_res is None:\n",
        "        return None, None, None\n",
        "\n",
        "    # === STAGE 3: Polish with Nelder-Mead ===\n",
        "    try:\n",
        "        final_res = minimize(\n",
        "            _neg_log_lik_reml_reg, x0=best_res.x,\n",
        "            args=(y_all, v_all, X_all, N_total, M_studies, p_params),\n",
        "            method='Nelder-Mead',\n",
        "            options={'xatol': 1e-10, 'fatol': 1e-10}\n",
        "        )\n",
        "    except Exception:\n",
        "        final_res = best_res\n",
        "\n",
        "    # === FINAL ESTIMATES ===\n",
        "    final_est = _get_three_level_regression_estimates_v2(\n",
        "        final_res.x, y_all, v_all, X_all, N_total, M_studies, p_params\n",
        "    )\n",
        "\n",
        "    return final_est, (N_total, M_studies, p_params), final_res\n",
        "\n",
        "\n",
        "\n",
        "# --- PLOTTING FUNCTIONS ---\n",
        "\n",
        "def plot_trim_fill(data, effect_col, se_col, results, es_label):\n",
        "    \"\"\"Simple Forest Plot for Trim/Fill (Preview)\"\"\"\n",
        "    k0 = results['k0']\n",
        "    orig_est = results['pooled_original']\n",
        "    fill_est = results['pooled_filled']\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Plot Original Studies\n",
        "    ax.scatter(data[effect_col], data[se_col], c='black', alpha=0.6, label='Observed Studies')\n",
        "\n",
        "    # Plot Filled Studies\n",
        "    if k0 > 0:\n",
        "        se_filled = np.sqrt(results['vi_filled'])\n",
        "        ax.scatter(results['yi_filled'], se_filled, c='white', edgecolors='red', marker='o', label='Imputed Studies')\n",
        "\n",
        "    # Plot Center Lines\n",
        "    ax.axvline(orig_est, color='black', linestyle='--', label=f'Original: {orig_est:.3f}')\n",
        "    ax.axvline(fill_est, color='red', linestyle='-', label=f'Adjusted: {fill_est:.3f}')\n",
        "\n",
        "    y_max = data[se_col].max() * 1.1\n",
        "    ax.set_ylim(y_max, 0)\n",
        "    ax.set_xlabel(es_label)\n",
        "    ax.set_ylabel(\"Standard Error\")\n",
        "    ax.set_title(f\"Trim-and-Fill Funnel Plot (Missing: {results['side']})\")\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "# functions\n",
        "\n",
        "#@title 🔧 ADVANCED HETEROGENEITY ESTIMATORS\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 4.5: ADVANCED TAU-SQUARED ESTIMATORS\n",
        "# Purpose: Provides multiple methods for estimating between-study variance\n",
        "# Dependencies: None (standalone functions)\n",
        "# Used by: Cell 6 (Overall Analysis), Cell 8 (Subgroup Analysis)\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import minimize_scalar, minimize\n",
        "from scipy.stats import chi2\n",
        "import warnings\n",
        "\n",
        "#print(\"=\"*70)\n",
        "#print(\"HETEROGENEITY ESTIMATORS MODULE\")\n",
        "#print(\"=\"*70)\n",
        "\n",
        "# --- 1. DERSIMONIAN-LAIRD (Your current method) ---\n",
        "\n",
        "# --- 2. RESTRICTED MAXIMUM LIKELIHOOD (REML) ---\n",
        "\n",
        "def calculate_tau_squared_REML(df, effect_col, var_col, max_iter=100, tol=1e-8):\n",
        "    \"\"\"\n",
        "    REML estimator for tau-squared (RECOMMENDED - Gold Standard)\n",
        "\n",
        "    Advantages:\n",
        "    - Unbiased for tau²\n",
        "    - Accounts for uncertainty in estimating mu\n",
        "    - Better performance in small samples\n",
        "    - Generally preferred in literature\n",
        "\n",
        "    Disadvantages:\n",
        "    - Iterative (slightly slower)\n",
        "    - Can fail to converge in extreme cases\n",
        "\n",
        "    Reference:\n",
        "    Viechtbauer, W. (2005). Bias and efficiency of meta-analytic variance\n",
        "    estimators in the random-effects model. Journal of Educational and\n",
        "    Behavioral Statistics, 30(3), 261-293.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Data with effect sizes and variances\n",
        "    effect_col : str\n",
        "        Name of effect size column\n",
        "    var_col : str\n",
        "        Name of variance column\n",
        "    max_iter : int\n",
        "        Maximum iterations for optimization\n",
        "    tol : float\n",
        "        Convergence tolerance\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : tau-squared estimate\n",
        "    \"\"\"\n",
        "    k = len(df)\n",
        "    if k < 2:\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        # Extract data\n",
        "        yi = df[effect_col].values\n",
        "        vi = df[var_col].values\n",
        "\n",
        "        # Remove any infinite or negative variances\n",
        "        valid_mask = np.isfinite(vi) & (vi > 0)\n",
        "        if not valid_mask.all():\n",
        "            warnings.warn(f\"Removed {(~valid_mask).sum()} observations with invalid variances\")\n",
        "            yi = yi[valid_mask]\n",
        "            vi = vi[valid_mask]\n",
        "            k = len(yi)\n",
        "\n",
        "        if k < 2:\n",
        "            return 0.0\n",
        "\n",
        "        # REML objective function (negative log-likelihood)\n",
        "        def reml_objective(tau2):\n",
        "            # Ensure tau2 is non-negative\n",
        "            tau2 = max(0, tau2)\n",
        "\n",
        "            # Weights\n",
        "            wi = 1 / (vi + tau2)\n",
        "            sum_wi = wi.sum()\n",
        "\n",
        "            if sum_wi <= 0:\n",
        "                return 1e10\n",
        "\n",
        "            # Pooled estimate\n",
        "            mu = (wi * yi).sum() / sum_wi\n",
        "\n",
        "            # Q statistic\n",
        "            Q = (wi * (yi - mu)**2).sum()\n",
        "\n",
        "            # REML log-likelihood (negative for minimization)\n",
        "            # L = -0.5 * [sum(log(vi + tau2)) + log(sum(wi)) + Q]\n",
        "            log_lik = -0.5 * (\n",
        "                np.sum(np.log(vi + tau2)) +\n",
        "                np.log(sum_wi) +\n",
        "                Q\n",
        "            )\n",
        "\n",
        "            return -log_lik  # Return negative for minimization\n",
        "\n",
        "        # Get reasonable bounds for tau2\n",
        "        # Lower bound: 0\n",
        "        # Upper bound: Use variance of effect sizes as upper limit\n",
        "        var_yi = np.var(yi, ddof=1) if k > 2 else 1.0\n",
        "        upper_bound = max(10 * var_yi, 100)\n",
        "\n",
        "        # Optimize\n",
        "        result = minimize_scalar(\n",
        "            reml_objective,\n",
        "            bounds=(0, upper_bound),\n",
        "            method='bounded',\n",
        "            options={'maxiter': max_iter, 'xatol': tol}\n",
        "        )\n",
        "\n",
        "        if result.success:\n",
        "            tau_sq = result.x\n",
        "        else:\n",
        "            warnings.warn(\"REML optimization did not converge, using DL fallback\")\n",
        "            tau_sq = calculate_tau_squared_DL(df, effect_col, var_col)\n",
        "\n",
        "        return max(0.0, tau_sq)\n",
        "\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error in REML estimator: {e}, using DL fallback\")\n",
        "        return calculate_tau_squared_DL(df, effect_col, var_col)\n",
        "\n",
        "\n",
        "# --- 3. MAXIMUM LIKELIHOOD (ML) ---\n",
        "\n",
        "def calculate_tau_squared_ML(df, effect_col, var_col, max_iter=100, tol=1e-8):\n",
        "    \"\"\"\n",
        "    Maximum Likelihood estimator for tau-squared\n",
        "\n",
        "    Advantages:\n",
        "    - Efficient asymptotically\n",
        "    - Produces valid estimates\n",
        "\n",
        "    Disadvantages:\n",
        "    - Biased downward (underestimates tau²)\n",
        "    - Less preferred than REML\n",
        "    - REML is generally recommended instead\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Data with effect sizes and variances\n",
        "    effect_col : str\n",
        "        Name of effect size column\n",
        "    var_col : str\n",
        "        Name of variance column\n",
        "    max_iter : int\n",
        "        Maximum iterations\n",
        "    tol : float\n",
        "        Convergence tolerance\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : tau-squared estimate\n",
        "    \"\"\"\n",
        "    k = len(df)\n",
        "    if k < 2:\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        yi = df[effect_col].values\n",
        "        vi = df[var_col].values\n",
        "\n",
        "        valid_mask = np.isfinite(vi) & (vi > 0)\n",
        "        if not valid_mask.all():\n",
        "            yi = yi[valid_mask]\n",
        "            vi = vi[valid_mask]\n",
        "            k = len(yi)\n",
        "\n",
        "        if k < 2:\n",
        "            return 0.0\n",
        "\n",
        "        # ML objective function\n",
        "        def ml_objective(tau2):\n",
        "            tau2 = max(0, tau2)\n",
        "            wi = 1 / (vi + tau2)\n",
        "            sum_wi = wi.sum()\n",
        "\n",
        "            if sum_wi <= 0:\n",
        "                return 1e10\n",
        "\n",
        "            mu = (wi * yi).sum() / sum_wi\n",
        "            Q = (wi * (yi - mu)**2).sum()\n",
        "\n",
        "            # ML log-likelihood (without the constant term)\n",
        "            log_lik = -0.5 * (np.sum(np.log(vi + tau2)) + Q)\n",
        "\n",
        "            return -log_lik\n",
        "\n",
        "        var_yi = np.var(yi, ddof=1) if k > 2 else 1.0\n",
        "        upper_bound = max(10 * var_yi, 100)\n",
        "\n",
        "        result = minimize_scalar(\n",
        "            ml_objective,\n",
        "            bounds=(0, upper_bound),\n",
        "            method='bounded',\n",
        "            options={'maxiter': max_iter, 'xatol': tol}\n",
        "        )\n",
        "\n",
        "        if result.success:\n",
        "            tau_sq = result.x\n",
        "        else:\n",
        "            warnings.warn(\"ML optimization did not converge, using DL fallback\")\n",
        "            tau_sq = calculate_tau_squared_DL(df, effect_col, var_col)\n",
        "\n",
        "        return max(0.0, tau_sq)\n",
        "\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error in ML estimator: {e}, using DL fallback\")\n",
        "        return calculate_tau_squared_DL(df, effect_col, var_col)\n",
        "\n",
        "\n",
        "# --- 4. PAULE-MANDEL (PM) ---\n",
        "\n",
        "def calculate_tau_squared_PM(df, effect_col, var_col, max_iter=100, tol=1e-8):\n",
        "    \"\"\"\n",
        "    Paule-Mandel estimator for tau-squared\n",
        "\n",
        "    Advantages:\n",
        "    - Exact solution to Q = k-1 equation\n",
        "    - Non-iterative in principle\n",
        "    - Good performance\n",
        "\n",
        "    Disadvantages:\n",
        "    - Can be unstable with few studies\n",
        "    - Requires iterative solution in practice\n",
        "\n",
        "    Reference:\n",
        "    Paule, R. C., & Mandel, J. (1982). Consensus values and weighting factors.\n",
        "    Journal of Research of the National Bureau of Standards, 87(5), 377-385.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Data with effect sizes and variances\n",
        "    effect_col : str\n",
        "        Name of effect size column\n",
        "    var_col : str\n",
        "        Name of variance column\n",
        "    max_iter : int\n",
        "        Maximum iterations\n",
        "    tol : float\n",
        "        Convergence tolerance\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : tau-squared estimate\n",
        "    \"\"\"\n",
        "    k = len(df)\n",
        "    if k < 2:\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        yi = df[effect_col].values\n",
        "        vi = df[var_col].values\n",
        "\n",
        "        valid_mask = np.isfinite(vi) & (vi > 0)\n",
        "        if not valid_mask.all():\n",
        "            yi = yi[valid_mask]\n",
        "            vi = vi[valid_mask]\n",
        "            k = len(yi)\n",
        "\n",
        "        if k < 2:\n",
        "            return 0.0\n",
        "\n",
        "        df_Q = k - 1\n",
        "\n",
        "        # PM objective: Find tau2 such that Q(tau2) = k - 1\n",
        "        def pm_objective(tau2):\n",
        "            tau2 = max(0, tau2)\n",
        "            wi = 1 / (vi + tau2)\n",
        "            sum_wi = wi.sum()\n",
        "\n",
        "            if sum_wi <= 0:\n",
        "                return 1e10\n",
        "\n",
        "            mu = (wi * yi).sum() / sum_wi\n",
        "            Q = (wi * (yi - mu)**2).sum()\n",
        "\n",
        "            # We want Q = k - 1\n",
        "            return (Q - df_Q)**2\n",
        "\n",
        "        var_yi = np.var(yi, ddof=1) if k > 2 else 1.0\n",
        "        upper_bound = max(10 * var_yi, 100)\n",
        "\n",
        "        result = minimize_scalar(\n",
        "            pm_objective,\n",
        "            bounds=(0, upper_bound),\n",
        "            method='bounded',\n",
        "            options={'maxiter': max_iter, 'xatol': tol}\n",
        "        )\n",
        "\n",
        "        if result.success and result.fun < 1:  # Good convergence\n",
        "            tau_sq = result.x\n",
        "        else:\n",
        "            # If PM fails, use DL\n",
        "            tau_sq = calculate_tau_squared_DL(df, effect_col, var_col)\n",
        "\n",
        "        return max(0.0, tau_sq)\n",
        "\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error in PM estimator: {e}, using DL fallback\")\n",
        "        return calculate_tau_squared_DL(df, effect_col, var_col)\n",
        "\n",
        "\n",
        "# --- 5. SIDIK-JONKMAN (SJ) ---\n",
        "\n",
        "def calculate_tau_squared_SJ(df, effect_col, var_col):\n",
        "    \"\"\"\n",
        "    Sidik-Jonkman estimator for tau-squared\n",
        "\n",
        "    Advantages:\n",
        "    - Simple, non-iterative\n",
        "    - Good performance with few studies\n",
        "    - Conservative (tends to produce larger estimates)\n",
        "\n",
        "    Disadvantages:\n",
        "    - Can be overly conservative\n",
        "    - Less commonly used\n",
        "\n",
        "    Reference:\n",
        "    Sidik, K., & Jonkman, J. N. (2005). Simple heterogeneity variance\n",
        "    estimation for meta-analysis. Journal of the Royal Statistical Society,\n",
        "    Series C, 54(2), 367-384.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Data with effect sizes and variances\n",
        "    effect_col : str\n",
        "        Name of effect size column\n",
        "    var_col : str\n",
        "        Name of variance column\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float : tau-squared estimate\n",
        "    \"\"\"\n",
        "    k = len(df)\n",
        "    if k < 3:  # Need at least 3 studies for SJ\n",
        "        return calculate_tau_squared_DL(df, effect_col, var_col)\n",
        "\n",
        "    try:\n",
        "        yi = df[effect_col].values\n",
        "        vi = df[var_col].values\n",
        "\n",
        "        valid_mask = np.isfinite(vi) & (vi > 0)\n",
        "        if not valid_mask.all():\n",
        "            yi = yi[valid_mask]\n",
        "            vi = vi[valid_mask]\n",
        "            k = len(yi)\n",
        "\n",
        "        if k < 3:\n",
        "            return calculate_tau_squared_DL(df, effect_col, var_col)\n",
        "\n",
        "        # Weights for typical average\n",
        "        wi = 1 / vi\n",
        "        sum_wi = wi.sum()\n",
        "\n",
        "        # Typical average (weighted mean)\n",
        "        y_bar = (wi * yi).sum() / sum_wi\n",
        "\n",
        "        # SJ estimator\n",
        "        numerator = ((yi - y_bar)**2 / vi).sum()\n",
        "        denominator = k - 1\n",
        "\n",
        "        tau_sq = (numerator / denominator) - (k / sum_wi)\n",
        "\n",
        "        return max(0.0, tau_sq)\n",
        "\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error in SJ estimator: {e}, using DL fallback\")\n",
        "        return calculate_tau_squared_DL(df, effect_col, var_col)\n",
        "\n",
        "\n",
        "# --- 6. EXPORT EXCEL ---\n",
        "\n",
        "!pip install xlsxwriter\n",
        "import xlsxwriter\n",
        "\n",
        "# --- 1. HELPER: EXCLUSION LOG ---\n",
        "def _get_exclusion_log():\n",
        "    if 'ANALYSIS_CONFIG' in globals() and 'removed_records' in ANALYSIS_CONFIG:\n",
        "        return ANALYSIS_CONFIG['removed_records']\n",
        "    return pd.DataFrame([{'ID': 'All Data Kept', 'Reason': 'No records were removed.'}])\n",
        "\n",
        "# --- 2. HELPER: EXCEL FORMATTER ---\n",
        "def _apply_excel_formatting(writer, sheet_name, df):\n",
        "    workbook = writer.book\n",
        "    worksheet = writer.sheets[sheet_name]\n",
        "    header_fmt = workbook.add_format({'bold': True, 'valign': 'top', 'fg_color': '#1F4E78', 'font_color': '#FFFFFF', 'border': 1})\n",
        "    for col_num, value in enumerate(df.columns.values):\n",
        "        worksheet.write(0, col_num, value, header_fmt)\n",
        "    for i, col in enumerate(df.columns):\n",
        "        col_len = len(str(col))\n",
        "        sample_vals = df[col].head(20).astype(str)\n",
        "        max_val_len = sample_vals.map(len).max() if not sample_vals.empty else 0\n",
        "        worksheet.set_column(i, i, min(max(col_len, max_val_len) + 2, 50))\n",
        "\n",
        "# --- 3. HELPER: PROTOCOL SHEET FORMATTER ---\n",
        "def _apply_protocol_sheet_formatting(writer, sheet_name, df):\n",
        "    \"\"\"\n",
        "    Custom formatting for the Protocol & Settings sheet.\n",
        "    - Bold headers\n",
        "    - Bold Category column\n",
        "    - Auto-adjust column widths for readability\n",
        "    \"\"\"\n",
        "    workbook = writer.book\n",
        "    worksheet = writer.sheets[sheet_name]\n",
        "\n",
        "    # Define formats\n",
        "    header_fmt = workbook.add_format({\n",
        "        'bold': True,\n",
        "        'valign': 'top',\n",
        "        'fg_color': '#1F4E78',\n",
        "        'font_color': '#FFFFFF',\n",
        "        'border': 1\n",
        "    })\n",
        "\n",
        "    category_fmt = workbook.add_format({\n",
        "        'bold': True,\n",
        "        'valign': 'top'\n",
        "    })\n",
        "\n",
        "    # Apply header formatting\n",
        "    for col_num, value in enumerate(df.columns.values):\n",
        "        worksheet.write(0, col_num, value, header_fmt)\n",
        "\n",
        "    # Apply bold formatting to Category column (column 0)\n",
        "    for row_num in range(1, len(df) + 1):\n",
        "        cell_value = df.iloc[row_num - 1, 0]  # Category column\n",
        "        worksheet.write(row_num, 0, cell_value, category_fmt)\n",
        "\n",
        "    # Auto-adjust column widths\n",
        "    for i, col in enumerate(df.columns):\n",
        "        col_len = len(str(col))\n",
        "        sample_vals = df[col].head(50).astype(str)\n",
        "        max_val_len = sample_vals.map(len).max() if not sample_vals.empty else 0\n",
        "        # Make columns wider for better readability\n",
        "        col_width = min(max(col_len, max_val_len) + 3, 60)\n",
        "        worksheet.set_column(i, i, col_width)\n",
        "\n",
        "\n",
        "# --- 4. HELPER: SMART METADATA COLLECTOR (UPDATED) ---\n",
        "def _get_protocol_metadata(report_type):\n",
        "    \"\"\"\n",
        "    Captures comprehensive protocol settings and configuration.\n",
        "    Returns a DataFrame with columns: [Category, Parameter, Value]\n",
        "    \"\"\"\n",
        "    meta = []\n",
        "\n",
        "    # System Information\n",
        "    meta.append({'Category': 'System', 'Parameter': 'Timestamp', 'Value': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
        "\n",
        "    if 'ANALYSIS_CONFIG' not in globals():\n",
        "        return pd.DataFrame(meta)\n",
        "\n",
        "    ac = ANALYSIS_CONFIG\n",
        "\n",
        "    try:\n",
        "        # === ANALYSIS SETTINGS ===\n",
        "        global_settings = ac.get('global_settings', {})\n",
        "\n",
        "        # Confidence Level (convert alpha to percentage)\n",
        "        alpha = global_settings.get('alpha', 0.05)\n",
        "        confidence_pct = (1 - alpha) * 100\n",
        "        meta.append({'Category': 'Settings', 'Parameter': 'Confidence Level', 'Value': f\"{confidence_pct:.0f}%\"})\n",
        "\n",
        "        # Inference Distribution\n",
        "        dist_type = global_settings.get('dist_type', 'norm')\n",
        "        dist_label = 't-distribution' if dist_type == 't' else 'Normal distribution'\n",
        "        meta.append({'Category': 'Settings', 'Parameter': 'Inference Distribution', 'Value': dist_label})\n",
        "\n",
        "        # Model Selection\n",
        "        overall_results = ac.get('overall_results', {})\n",
        "        model_choice = overall_results.get('model_choice', 'Unknown')\n",
        "        meta.append({'Category': 'Settings', 'Parameter': 'Model Selection', 'Value': model_choice})\n",
        "\n",
        "        # Heterogeneity Estimator (tau method)\n",
        "        tau_method = overall_results.get('tau_method', 'REML')\n",
        "        meta.append({'Category': 'Settings', 'Parameter': 'Heterogeneity Estimator', 'Value': tau_method})\n",
        "\n",
        "        # Knapp-Hartung Adjustment\n",
        "        kh_info = overall_results.get('knapp_hartung', {})\n",
        "        use_kh = kh_info.get('used', False)\n",
        "        kh_label = 'Yes' if use_kh else 'No'\n",
        "        meta.append({'Category': 'Settings', 'Parameter': 'Knapp-Hartung Adjustment', 'Value': kh_label})\n",
        "\n",
        "        # === DATA PROVENANCE ===\n",
        "\n",
        "        # Input Mode\n",
        "        data_type = ac.get('data_type', 'Unknown')\n",
        "        input_mode = 'Pre-Calculated' if data_type == 'pre_calculated' else 'Raw Data'\n",
        "        meta.append({'Category': 'Data', 'Parameter': 'Input Mode', 'Value': input_mode})\n",
        "\n",
        "        # Effect Size Information\n",
        "        es_config = ac.get('es_config', {})\n",
        "        effect_label = es_config.get('effect_label', 'Unknown')\n",
        "        effect_label_short = es_config.get('effect_label_short', 'Unknown')\n",
        "        meta.append({'Category': 'Data', 'Parameter': 'Effect Size Type', 'Value': f\"{effect_label} ({effect_label_short})\"})\n",
        "\n",
        "        # Effect Size Column Names\n",
        "        effect_col = ac.get('effect_col', es_config.get('effect_col', 'Unknown'))\n",
        "        var_col = ac.get('var_col', es_config.get('var_col', 'Unknown'))\n",
        "        meta.append({'Category': 'Data', 'Parameter': 'Effect Size Column', 'Value': effect_col})\n",
        "        meta.append({'Category': 'Data', 'Parameter': 'Variance Column', 'Value': var_col})\n",
        "\n",
        "        # Sample Size Information\n",
        "        analysis_data = ac.get('analysis_data')\n",
        "        if analysis_data is not None:\n",
        "            k_total = len(analysis_data)\n",
        "            meta.append({'Category': 'Data', 'Parameter': 'Total Observations (k)', 'Value': k_total})\n",
        "\n",
        "            # Count unique studies\n",
        "            if 'id' in analysis_data.columns:\n",
        "                n_studies = analysis_data['id'].nunique()\n",
        "                meta.append({'Category': 'Data', 'Parameter': 'Unique Studies', 'Value': n_studies})\n",
        "\n",
        "        # === REPORT-SPECIFIC METADATA ===\n",
        "\n",
        "        if report_type == 'cumulative' and 'cumulative_results' in ac:\n",
        "            cum_df = ac['cumulative_results']\n",
        "            if not cum_df.empty:\n",
        "                meta.append({'Category': 'Cumulative', 'Parameter': 'Total Steps', 'Value': len(cum_df)})\n",
        "                meta.append({'Category': 'Cumulative', 'Parameter': 'Year Range', 'Value': f\"{int(cum_df['year'].min())} - {int(cum_df['year'].max())}\"})\n",
        "                meta.append({'Category': 'Cumulative', 'Parameter': 'Initial Effect (Start)', 'Value': f\"{cum_df.iloc[0]['pooled_effect']:.4f}\"})\n",
        "                meta.append({'Category': 'Cumulative', 'Parameter': 'Final Effect (End)', 'Value': f\"{cum_df.iloc[-1]['pooled_effect']:.4f}\"})\n",
        "                meta.append({'Category': 'Cumulative', 'Parameter': 'Method', 'Value': 'Iterative REML'})\n",
        "\n",
        "        elif report_type == 'loo' and 'loo_3level_results' in ac:\n",
        "            loo = ac['loo_3level_results']\n",
        "            meta.append({'Category': 'Sensitivity', 'Parameter': 'Method', 'Value': 'Leave-One-Out (3-Level)'})\n",
        "            meta.append({'Category': 'Sensitivity', 'Parameter': 'Original Effect', 'Value': f\"{loo.get('original_effect', 0):.4f}\"})\n",
        "            meta.append({'Category': 'Sensitivity', 'Parameter': 'Significance Changers', 'Value': loo.get('n_sig_changers', 0)})\n",
        "\n",
        "        elif report_type == 'subgroup' and 'subgroup_config' in ac:\n",
        "            sub_config = ac['subgroup_config']\n",
        "            analysis_type = sub_config.get('analysis_type', 'Unknown')\n",
        "            moderator1 = sub_config.get('moderator1', 'Unknown')\n",
        "            meta.append({'Category': 'Subgroup', 'Parameter': 'Analysis Type', 'Value': analysis_type})\n",
        "            meta.append({'Category': 'Subgroup', 'Parameter': 'Moderator', 'Value': moderator1})\n",
        "            if 'moderator2' in sub_config and sub_config['moderator2']:\n",
        "                moderator2 = sub_config['moderator2']\n",
        "                meta.append({'Category': 'Subgroup', 'Parameter': 'Second Moderator', 'Value': moderator2})\n",
        "\n",
        "        elif report_type == 'regression' and 'meta_regression_RVE_results' in ac:\n",
        "            reg = ac['meta_regression_RVE_results']\n",
        "            moderator_col = reg.get('moderator_col_name', 'Unknown')\n",
        "            meta.append({'Category': 'Regression', 'Parameter': 'Moderator Variable', 'Value': moderator_col})\n",
        "            meta.append({'Category': 'Regression', 'Parameter': 'Model Type', 'Value': '3-Level REML'})\n",
        "\n",
        "        elif report_type == 'spline' and 'spline_model_results' in ac:\n",
        "            spline = ac['spline_model_results']\n",
        "            moderator = spline.get('moderator_col', 'Unknown')\n",
        "            n_knots = spline.get('n_knots', 'Unknown')\n",
        "            meta.append({'Category': 'Spline', 'Parameter': 'Moderator Variable', 'Value': moderator})\n",
        "            meta.append({'Category': 'Spline', 'Parameter': 'Number of Knots', 'Value': n_knots})\n",
        "\n",
        "        elif report_type == 'publication_bias':\n",
        "            meta.append({'Category': 'Publication Bias', 'Parameter': 'Tests Performed', 'Value': 'Egger Test, Trim & Fill'})\n",
        "\n",
        "    except Exception as e:\n",
        "        meta.append({'Category': 'Error', 'Parameter': 'Metadata Error', 'Value': str(e)})\n",
        "\n",
        "    return pd.DataFrame(meta)\n",
        "\n",
        "# --- 4. MAIN ORCHESTRATOR (UPDATED) ---\n",
        "def export_analysis_report(report_type='overall', filename_prefix=\"MetaAnalysis\"):\n",
        "    if 'ANALYSIS_CONFIG' not in globals(): return\n",
        "\n",
        "    buffer = io.BytesIO()\n",
        "    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:\n",
        "        try:\n",
        "            text_key = 'latest_text'\n",
        "\n",
        "            # ... (Existing blocks for overall, subgroup, regression, spline, pub_bias) ...\n",
        "\n",
        "            if report_type == 'overall':\n",
        "                text_key = 'overall_text'\n",
        "                res = ANALYSIS_CONFIG['overall_results']\n",
        "                simple_res = {k:v for k,v in res.items() if not isinstance(v, dict)}\n",
        "                pd.DataFrame([simple_res]).T.reset_index().to_excel(writer, sheet_name='Overall Results', index=False)\n",
        "\n",
        "            elif report_type == 'subgroup':\n",
        "                text_key = 'subgroup_text'\n",
        "                if 'subgroup_results' in ANALYSIS_CONFIG:\n",
        "                    ANALYSIS_CONFIG['subgroup_results']['results_df'].to_excel(writer, sheet_name='Subgroup Results', index=False)\n",
        "\n",
        "            elif report_type == 'regression':\n",
        "                text_key = 'regression_text'\n",
        "                if 'meta_regression_RVE_results' in ANALYSIS_CONFIG:\n",
        "                    reg = ANALYSIS_CONFIG['meta_regression_RVE_results']\n",
        "                    pd.DataFrame({'Term': ['Intercept', reg['moderator_col_name']], 'Beta': reg['betas'], 'SE': reg['std_errors_robust'], 'P-Value': [reg['p_intercept'], reg['p_slope']]}).to_excel(writer, sheet_name='Regression Results', index=False)\n",
        "\n",
        "            elif report_type == 'spline':\n",
        "                text_key = 'spline_text'\n",
        "                if 'spline_model_results' in ANALYSIS_CONFIG:\n",
        "                    res = ANALYSIS_CONFIG['spline_model_results']\n",
        "                    pd.DataFrame([{'Metric': 'Chi2', 'Value': res['omnibus_chi2']}, {'Metric': 'P-Value', 'Value': res['omnibus_p']}]).to_excel(writer, sheet_name='Spline Results', index=False)\n",
        "\n",
        "            elif report_type == 'publication_bias':\n",
        "                text_key = 'bias_text'\n",
        "                if 'funnel_results' in ANALYSIS_CONFIG:\n",
        "                    egg = ANALYSIS_CONFIG['funnel_results']\n",
        "                    pd.DataFrame([{'Test': 'Egger', 'Intercept': egg['intercept'], 'P-Value': egg['p_value']}]).to_excel(writer, sheet_name='Eggers Test', index=False)\n",
        "                if 'trimfill_results' in ANALYSIS_CONFIG:\n",
        "                    tf = ANALYSIS_CONFIG['trimfill_results']\n",
        "                    pd.DataFrame([{'Missing': tf['k0'], 'Original': tf['pooled_original'], 'Adjusted': tf['pooled_filled']}]).to_excel(writer, sheet_name='Trim Fill', index=False)\n",
        "\n",
        "            # === UPDATED: CUMULATIVE RESULTS ===\n",
        "            elif report_type == 'cumulative':\n",
        "                text_key = 'cumulative_text'\n",
        "                if 'cumulative_results' in ANALYSIS_CONFIG:\n",
        "                    # 1. Full Step-by-Step Table\n",
        "                    ANALYSIS_CONFIG['cumulative_results'].to_excel(writer, sheet_name='Cumulative Results', index=False)\n",
        "\n",
        "                    # 2. Summary Table (Start vs End)\n",
        "                    df = ANALYSIS_CONFIG['cumulative_results']\n",
        "                    if not df.empty:\n",
        "                        summary = df.iloc[[0, -1]].copy()\n",
        "                        summary.insert(0, 'Stage', ['Start', 'End'])\n",
        "                        summary.to_excel(writer, sheet_name='Cumulative Summary', index=False)\n",
        "\n",
        "            # === UPDATED: LOO RESULTS ===\n",
        "            elif report_type == 'loo':\n",
        "                text_key = 'loo_text'\n",
        "                if 'loo_3level_results' in ANALYSIS_CONFIG:\n",
        "                    # 1. Full Sensitivity Table\n",
        "                    res_df = ANALYSIS_CONFIG['loo_3level_results']['results_df']\n",
        "                    res_df.to_excel(writer, sheet_name='Sensitivity Results', index=False)\n",
        "\n",
        "                    # 2. Influential Studies (if any)\n",
        "                    influencers = res_df[res_df['changes_sig'] == True]\n",
        "                    if not influencers.empty:\n",
        "                        influencers.to_excel(writer, sheet_name='Influential Studies', index=False)\n",
        "                    else:\n",
        "                        pd.DataFrame([{'Result': 'Robust', 'Note': 'No single study changed significance.'}]).to_excel(writer, sheet_name='Influential Studies', index=False)\n",
        "\n",
        "            # --- COMMON SHEETS ---\n",
        "            if 'analysis_data' in ANALYSIS_CONFIG:\n",
        "                ANALYSIS_CONFIG['analysis_data'].to_excel(writer, sheet_name='Processed Data', index=False)\n",
        "                _apply_excel_formatting(writer, 'Processed Data', ANALYSIS_CONFIG['analysis_data'])\n",
        "\n",
        "            df_excl = _get_exclusion_log()\n",
        "            df_excl.to_excel(writer, sheet_name='Data Exclusions', index=False)\n",
        "            _apply_excel_formatting(writer, 'Data Exclusions', df_excl)\n",
        "\n",
        "            # --- GENERATED TEXT ---\n",
        "            final_text = ANALYSIS_CONFIG.get(text_key, \"\")\n",
        "            if final_text:\n",
        "                clean_text = _clean_html_tags(final_text)\n",
        "                df_text = pd.DataFrame({'Generated Interpretation': [clean_text]})\n",
        "                df_text.to_excel(writer, sheet_name='Report Text', index=False)\n",
        "                writer.sheets['Report Text'].set_column(0, 0, 100)\n",
        "                writer.sheets['Report Text'].set_row(1, 300, writer.book.add_format({'text_wrap': True, 'valign': 'top'}))\n",
        "\n",
        "            # --- PROTOCOL & SETTINGS (LAST SHEET) ---\n",
        "            # This comprehensive sheet captures all configuration settings used for this analysis\n",
        "            df_proto = _get_protocol_metadata(report_type)\n",
        "            df_proto.to_excel(writer, sheet_name='Protocol & Settings', index=False)\n",
        "            _apply_protocol_sheet_formatting(writer, 'Protocol & Settings', df_proto)\n",
        "\n",
        "        except Exception as e:\n",
        "            pd.DataFrame([{'Error': str(e)}]).to_excel(writer, sheet_name='Error_Log')\n",
        "\n",
        "    buffer.seek(0)\n",
        "    b64 = base64.b64encode(buffer.read()).decode()\n",
        "    filename = f\"{filename_prefix}_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.xlsx\"\n",
        "    payload = f\"var link = document.createElement('a'); link.href = 'data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}'; link.download = '{filename}'; document.body.appendChild(link); link.click(); document.body.removeChild(link);\"\n",
        "    display(Javascript(payload))\n",
        "\n",
        "\n",
        "# --- 7. PRESET FOR PLOTS ---\n",
        "\n",
        "PRESETS = {\n",
        "    'Custom': {},\n",
        "\n",
        "    # 1 Column: 8.5 cm -> 3.35 inches\n",
        "    'Cell Press 1-Col (85mm)': {\n",
        "        'width': 3.35, 'height': 3.35,\n",
        "        'title': 9, 'label': 8, 'tick': 7  # Fonts must be small to fit\n",
        "    },\n",
        "    # 1.5 Column: 11.4 cm -> 4.49 inches\n",
        "    'Cell Press 1.5-Col (114mm)': {\n",
        "        'width': 4.49, 'height': 4.00,\n",
        "        'title': 10, 'label': 9, 'tick': 8\n",
        "    },\n",
        "    # Full Width: 17.4 cm -> 6.85 inches\n",
        "    'Cell Press Full (174mm)': {\n",
        "        'width': 6.85, 'height': 5.50,\n",
        "        'title': 11, 'label': 10, 'tick': 9\n",
        "    },\n",
        "\n",
        "    # 1 Column: 13.4 cm -> 5.28 inches\n",
        "    'STAR Protocols 1-Col (134mm)': {\n",
        "        'width': 5.28, 'height': 5.00,\n",
        "        'title': 11, 'label': 10, 'tick': 9\n",
        "    },\n",
        "    # Full Width: 17.2 cm -> 6.77 inches\n",
        "    'STAR Protocols Full (172mm)': {\n",
        "        'width': 6.77, 'height': 6.00,\n",
        "        'title': 12, 'label': 11, 'tick': 10\n",
        "    },\n",
        "\n",
        "    # 1 Column: 5.5 cm -> 2.17 inches (Very narrow!)\n",
        "    'Cell 3-Col Layout (Narrow/55mm)': {\n",
        "        'width': 2.17, 'height': 2.50,\n",
        "        'title': 8, 'label': 7, 'tick': 6\n",
        "    },\n",
        "\n",
        "    # --- General / Thesis ---\n",
        "    'Thesis (A4 Portrait)':  {'width': 6.30, 'height': 8.00, 'title': 12, 'label': 11, 'tick': 10},\n",
        "    'Presentation (16:9)':   {'width': 13.3, 'height': 7.50, 'title': 18, 'label': 14, 'tick': 12}\n",
        "}\n",
        "\n",
        "preset_widget = widgets.Dropdown(\n",
        "    options=list(PRESETS.keys()),\n",
        "    value='Custom',\n",
        "    description='📏 Preset:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "def on_preset_change(change):\n",
        "    \"\"\"Updates size and font sliders based on selection\"\"\"\n",
        "    settings = PRESETS.get(change['new'], {})\n",
        "    if not settings: return # Do nothing for 'Custom'\n",
        "\n",
        "    # Update Widgets (checking if they exist to allow re-use across cells)\n",
        "    if 'width_widget' in globals(): width_widget.value = settings['width']\n",
        "    if 'height_widget' in globals(): height_widget.value = settings['height']\n",
        "    if 'title_font_widget' in globals(): title_font_widget.value = settings['title']\n",
        "    if 'label_font_widget' in globals(): label_font_widget.value = settings['label']\n",
        "    if 'tick_font_widget' in globals(): tick_font_widget.value = settings['tick']\n",
        "\n",
        "    # Specific handling for Cumulative Plot which uses slightly different names\n",
        "    if 'title_fontsize_widget' in globals(): title_fontsize_widget.value = settings['title']\n",
        "    if 'label_fontsize_widget' in globals(): label_fontsize_widget.value = settings['label']\n",
        "    if 'tick_fontsize_widget' in globals(): tick_fontsize_widget.value = settings['tick']\n",
        "\n",
        "preset_widget.observe(on_preset_change, names='value')\n",
        "\n",
        "\n",
        "# --- 8. DISPLAY MODULE INFO ---\n",
        "print(\"\\n✅ Setup complete. Proceed to next cell to load data.\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ⚙️ Setup: LOAD & STANDARDIZE DATA\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 2: DATA INGESTION & COLUMN MAPPING (DUAL-MODE REFACTOR)\n",
        "# Purpose: Load data (Sheets/CSV/Excel) and map columns to standard names.\n",
        "# Modes: 1) Raw Data (Means/SDs) or 2) Pre-calculated Effect Sizes\n",
        "# Output: Global 'raw_data_standardized' DataFrame\n",
        "# =============================================================================\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import pandas as pd\n",
        "import io\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "\n",
        "# --- Configuration: Required Columns & Synonyms ---\n",
        "# RAW DATA MODE: Experimental vs Control statistics\n",
        "RAW_COLUMN_SPECS = {\n",
        "    'id':  ['id', 'study', 'study_id', 'paper', 'author'],\n",
        "    'xe':  ['xe', 'mean_e', 'mean_exp', 'x_e', 'treatment_mean'],\n",
        "    'sde': ['sde', 'sd_e', 'sd_exp', 'sigma_e'],\n",
        "    'ne':  ['ne', 'n_e', 'n_exp', 'sample_e'],\n",
        "    'xc':  ['xc', 'mean_c', 'mean_ctrl', 'x_c', 'control_mean'],\n",
        "    'sdc': ['sdc', 'sd_c', 'sd_ctrl', 'sigma_c'],\n",
        "    'nc':  ['nc', 'n_c', 'n_ctrl', 'sample_c']\n",
        "}\n",
        "\n",
        "# PRE-CALCULATED MODE: Effect sizes already computed\n",
        "PRECALC_COLUMN_SPECS = {\n",
        "    'id':       ['id', 'study', 'study_id', 'paper', 'author'],\n",
        "    'yi':       ['yi', 'effect_size', 'es', 'hedges_g', 'lnrr', 'smd', 'effect', 'g', 'd'],\n",
        "    'variance': ['variance', 'vi', 'var', 'v'],\n",
        "    'se':       ['se', 'standard_error', 'stderr', 'se_es'],\n",
        "    'n_total':  ['n_total', 'n', 'sample_size', 'total_n', 'sample_n']\n",
        "}\n",
        "\n",
        "# Global placeholders\n",
        "temp_raw_df = None\n",
        "data_type_widget = None  # Will be created below\n",
        "\n",
        "# =============================================================================\n",
        "# HELPER: UNIVERSAL FILE EXTRACTOR\n",
        "# =============================================================================\n",
        "def get_uploaded_file_data(uploader_widget):\n",
        "    \"\"\"\n",
        "    Robustly extracts filename and binary content from ipywidgets.FileUpload\n",
        "    Handles differences between Widget versions 7.x (Colab default) and 8.x\n",
        "    \"\"\"\n",
        "    val = uploader_widget.value\n",
        "    if not val:\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        # CASE A: Widgets 8.x (List or Tuple of dicts)\n",
        "        if isinstance(val, (tuple, list)):\n",
        "            file_obj = val[0]\n",
        "            fname = file_obj['name']\n",
        "            content = file_obj['content']\n",
        "\n",
        "        # CASE B: Widgets 7.x (Dictionary where Key=Filename)\n",
        "        elif isinstance(val, dict):\n",
        "            fname = list(val.keys())[0] # Get the first key\n",
        "            content = val[fname]['content']\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown widget format: {type(val)}\")\n",
        "\n",
        "        # Ensure content is bytes (sometimes it's a memoryview)\n",
        "        if hasattr(content, 'tobytes'):\n",
        "            content = content.tobytes()\n",
        "\n",
        "        return fname, content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Debug Info - Raw Type: {type(val)}\")\n",
        "        raise e\n",
        "\n",
        "# =============================================================================\n",
        "# UI PART 1: DATA LOADING (Tabs)\n",
        "# =============================================================================\n",
        "\n",
        "# --- Tab 1: Google Sheets Widgets ---\n",
        "btn_auth = widgets.Button(description=\"1. Connect Google Account\", button_style='warning', icon='google')\n",
        "txt_sheet_name = widgets.Text(value='tesis', description='Sheet Name:', layout=widgets.Layout(width='300px'))\n",
        "btn_fetch_ws = widgets.Button(description=\"2. Find Worksheets\", button_style='primary', disabled=True)\n",
        "dd_worksheets = widgets.Dropdown(description='Worksheet:', layout=widgets.Layout(width='300px'), disabled=True)\n",
        "btn_load_sheet = widgets.Button(description=\"3. Load Data\", button_style='success', disabled=True)\n",
        "\n",
        "gs_vbox = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Step A: Load from Google Sheets</b>\"),\n",
        "    btn_auth,\n",
        "    widgets.HBox([txt_sheet_name, btn_fetch_ws]),\n",
        "    widgets.HBox([dd_worksheets, btn_load_sheet])\n",
        "])\n",
        "\n",
        "# --- Tab 2: Local File Widgets ---\n",
        "uploader = widgets.FileUpload(accept='.csv,.xlsx,.xls', multiple=False, description='Upload File')\n",
        "\n",
        "# New: Dropdown for Excel Sheets (Hidden by default)\n",
        "dd_local_sheets = widgets.Dropdown(\n",
        "    description='Select Sheet:',\n",
        "    layout=widgets.Layout(width='300px', display='none'),\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "btn_process_file = widgets.Button(description=\"Process File\", button_style='success', disabled=True)\n",
        "\n",
        "local_vbox = widgets.VBox([\n",
        "    widgets.HTML(\"<b>Step B: Upload Local File</b><br><i style='font-size:10px; color:#666'>(Supports .csv and .xlsx)</i>\"),\n",
        "    uploader,\n",
        "    dd_local_sheets,\n",
        "    btn_process_file\n",
        "])\n",
        "\n",
        "# --- Output Areas ---\n",
        "log_output = widgets.Output()\n",
        "mapping_output = widgets.Output()\n",
        "\n",
        "# =============================================================================\n",
        "# LOGIC PART 1: LOADERS\n",
        "# =============================================================================\n",
        "\n",
        "def on_auth_clicked(b):\n",
        "    with log_output:\n",
        "        print(\"⏳ Authenticating...\")\n",
        "        try:\n",
        "            auth.authenticate_user()\n",
        "            creds, _ = default()\n",
        "            global gc\n",
        "            gc = gspread.authorize(creds)\n",
        "            btn_auth.button_style = 'success'\n",
        "            btn_auth.description = \"Connected ✓\"\n",
        "            btn_auth.disabled = True\n",
        "            btn_fetch_ws.disabled = False\n",
        "            print(\"✓ Authentication successful.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Auth failed: {e}\")\n",
        "\n",
        "def on_fetch_ws_clicked(b):\n",
        "    with log_output:\n",
        "        clear_output()\n",
        "        if 'gc' not in globals(): return print(\"❌ Please connect Google Account first.\")\n",
        "\n",
        "        name = txt_sheet_name.value\n",
        "        print(f\"🔎 Looking for '{name}'...\")\n",
        "        try:\n",
        "            global spreadsheet\n",
        "            spreadsheet = gc.open(name)\n",
        "            titles = [ws.title for ws in spreadsheet.worksheets()]\n",
        "            dd_worksheets.options = titles\n",
        "            dd_worksheets.disabled = False\n",
        "            btn_load_sheet.disabled = False\n",
        "            print(f\"✓ Found {len(titles)} worksheets.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error finding sheet: {e}\")\n",
        "\n",
        "def on_load_sheet_clicked(b):\n",
        "    with log_output:\n",
        "        print(f\"📥 Downloading '{dd_worksheets.value}'...\")\n",
        "        try:\n",
        "            ws = spreadsheet.worksheet(dd_worksheets.value)\n",
        "            rows = ws.get_all_values()\n",
        "            if len(rows) < 2: raise ValueError(\"Sheet empty or no headers.\")\n",
        "\n",
        "            global temp_raw_df\n",
        "            temp_raw_df = pd.DataFrame.from_records(rows[1:], columns=rows[0])\n",
        "            initiate_mapping_interface(temp_raw_df)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading data: {e}\")\n",
        "\n",
        "# --- LOCAL FILE LOGIC (Corrected) ---\n",
        "\n",
        "def on_file_upload(change):\n",
        "    \"\"\"Analyze uploaded file immediately\"\"\"\n",
        "    with log_output:\n",
        "        clear_output()\n",
        "        try:\n",
        "            # 1. Reset UI\n",
        "            dd_local_sheets.layout.display = 'none'\n",
        "            btn_process_file.disabled = True\n",
        "\n",
        "            # 2. Get Safe File Data\n",
        "            fname, content_bytes = get_uploaded_file_data(uploader)\n",
        "            if not fname: return\n",
        "\n",
        "            print(f\"🔎 Analyzing '{fname}'...\")\n",
        "\n",
        "            # 3. If Excel, find sheets\n",
        "            if fname.endswith(('.xls', '.xlsx')):\n",
        "                # Use bytes wrapper\n",
        "                excel_file = pd.ExcelFile(io.BytesIO(content_bytes))\n",
        "                sheets = excel_file.sheet_names\n",
        "\n",
        "                if len(sheets) > 1:\n",
        "                    print(f\"✓ Found {len(sheets)} sheets. Please select one below.\")\n",
        "                    dd_local_sheets.options = sheets\n",
        "                    dd_local_sheets.layout.display = 'block'\n",
        "                    dd_local_sheets.value = sheets[0]\n",
        "                else:\n",
        "                    print(\"✓ Excel file ready (1 sheet found).\")\n",
        "                    dd_local_sheets.options = sheets\n",
        "                    dd_local_sheets.value = sheets[0]\n",
        "                    dd_local_sheets.layout.display = 'block'\n",
        "\n",
        "            else:\n",
        "                print(\"✓ CSV file ready.\")\n",
        "\n",
        "            btn_process_file.disabled = False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error reading file structure: {e}\")\n",
        "\n",
        "def on_process_file_clicked(b):\n",
        "    \"\"\"Read the actual data based on selection\"\"\"\n",
        "    with log_output:\n",
        "        clear_output()\n",
        "        try:\n",
        "            fname, content_bytes = get_uploaded_file_data(uploader)\n",
        "            content = io.BytesIO(content_bytes)\n",
        "\n",
        "            global temp_raw_df\n",
        "\n",
        "            if fname.endswith('.csv'):\n",
        "                print(\"📥 Reading CSV...\")\n",
        "                temp_raw_df = pd.read_csv(content)\n",
        "            else:\n",
        "                sheet_target = dd_local_sheets.value\n",
        "                print(f\"📥 Reading Excel Sheet: '{sheet_target}'...\")\n",
        "                temp_raw_df = pd.read_excel(content, sheet_name=sheet_target)\n",
        "\n",
        "            initiate_mapping_interface(temp_raw_df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ File processing error: {e}\")\n",
        "\n",
        "# =============================================================================\n",
        "# LOGIC PART 2: COLUMN MAPPING (The \"Bridge\") - REFACTORED FOR DUAL MODE\n",
        "# =============================================================================\n",
        "\n",
        "def initiate_mapping_interface(df):\n",
        "    \"\"\"Analyzes columns, guesses matches, and shows detailed mapping widgets.\"\"\"\n",
        "    global temp_raw_df\n",
        "    temp_raw_df = df  # Store for later use when data type changes\n",
        "\n",
        "    with log_output:\n",
        "        print(f\"✓ Data loaded! ({len(df)} rows, {len(df.columns)} columns)\")\n",
        "        print(\"⬇ Please select data type and verify column names below.\")\n",
        "\n",
        "    with mapping_output:\n",
        "        clear_output()\n",
        "\n",
        "        # --- 1. DATA TYPE SELECTION (NEW) ---\n",
        "        global data_type_widget\n",
        "        if data_type_widget is None:\n",
        "            data_type_widget = widgets.RadioButtons(\n",
        "                options=[\n",
        "                    ('Raw Data (Means/SDs)', 'raw'),\n",
        "                    ('Pre-calculated (Effect/SE)', 'pre_calculated')\n",
        "                ],\n",
        "                value='raw',\n",
        "                description='Data Type:',\n",
        "                style={'description_width': 'initial'},\n",
        "                layout=widgets.Layout(width='600px')\n",
        "            )\n",
        "            # Observer to re-render mapping when data type changes\n",
        "            data_type_widget.observe(lambda change: render_column_mapping(df, change['new']), names='value')\n",
        "\n",
        "        # Header\n",
        "        header_html = \"\"\"\n",
        "        <h3 style='color:#2E86AB; margin-bottom:10px'>Step 2: Select Data Type & Map Columns</h3>\n",
        "        <div style='background-color:#e7f2fa; padding:10px; border-radius:5px; color:#333; margin-bottom:15px'>\n",
        "            <b>Choose your data type:</b><br>\n",
        "            • <b>Raw Data:</b> You have means, SDs, and sample sizes for Treatment and Control groups<br>\n",
        "            • <b>Pre-calculated:</b> You already have computed effect sizes (e.g., from published papers)\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(header_html))\n",
        "        display(data_type_widget)\n",
        "\n",
        "        # Container for dynamic column mapping\n",
        "        global mapping_container\n",
        "        mapping_container = widgets.Output()\n",
        "        display(mapping_container)\n",
        "\n",
        "        # Initial render\n",
        "        render_column_mapping(df, data_type_widget.value)\n",
        "\n",
        "def render_column_mapping(df, data_type):\n",
        "    \"\"\"Renders the appropriate column mapping UI based on data type\"\"\"\n",
        "    with mapping_container:\n",
        "        clear_output()\n",
        "\n",
        "        if data_type == 'raw':\n",
        "            render_raw_mapping(df)\n",
        "        else:\n",
        "            render_precalc_mapping(df)\n",
        "\n",
        "def render_raw_mapping(df):\n",
        "    \"\"\"Original column mapping for raw data\"\"\"\n",
        "    COLUMN_SPECS = RAW_COLUMN_SPECS\n",
        "\n",
        "    FIELD_INFO = {\n",
        "        'id':  {'label': 'Study ID / Label:', 'desc': 'Unique name for the study or paper (e.g., \"Smith 2020\").'},\n",
        "        'xe':  {'label': 'Experimental Mean (xe):', 'desc': 'Mean outcome for the Treatment group.'},\n",
        "        'sde': {'label': 'Experimental SD (sde):', 'desc': 'Standard Deviation for the Treatment group.'},\n",
        "        'ne':  {'label': 'Experimental N (ne):', 'desc': 'Sample size for the Treatment group.'},\n",
        "        'xc':  {'label': 'Control Mean (xc):', 'desc': 'Mean outcome for the Control group.'},\n",
        "        'sdc': {'label': 'Control SD (sdc):', 'desc': 'Standard Deviation for the Control group.'},\n",
        "        'nc':  {'label': 'Control N (nc):', 'desc': 'Sample size for the Control group.'}\n",
        "    }\n",
        "\n",
        "    cols_lower = [str(c).lower().strip() for c in df.columns]\n",
        "    mapping_widgets = {}\n",
        "    ui_rows = []\n",
        "\n",
        "    # Header\n",
        "    ui_rows.append(widgets.HTML(\"<hr><h4 style='color:#2E86AB; margin-top:15px;'>Map Your Columns (Raw Data Mode)</h4>\"))\n",
        "\n",
        "    # Build Widgets\n",
        "    for std_name, synonyms in COLUMN_SPECS.items():\n",
        "        # Auto-Guess Logic\n",
        "        guessed_val = None\n",
        "        for syn in synonyms:\n",
        "            if syn in cols_lower:\n",
        "                guessed_val = df.columns[cols_lower.index(syn)]\n",
        "                break\n",
        "\n",
        "        # Create the Dropdown\n",
        "        w = widgets.Dropdown(\n",
        "            options=list(df.columns),\n",
        "            value=guessed_val,\n",
        "            description=FIELD_INFO[std_name]['label'],\n",
        "            style={'description_width': '180px'},\n",
        "            layout=widgets.Layout(width='600px')\n",
        "        )\n",
        "        mapping_widgets[std_name] = w\n",
        "\n",
        "        row_box = widgets.VBox([\n",
        "            w,\n",
        "            widgets.HTML(f\"<div style='margin-left:185px; font-size:11px; color:#666; margin-bottom:8px'><i>{FIELD_INFO[std_name]['desc']}</i></div>\")\n",
        "        ])\n",
        "        ui_rows.append(row_box)\n",
        "\n",
        "    # Save Button\n",
        "    btn_finalize = widgets.Button(\n",
        "        description=\"✓ Confirm Mapping & Finalize Data\",\n",
        "        button_style='success',\n",
        "        layout=widgets.Layout(width='600px', height='40px', margin='20px 0 0 0'),\n",
        "        icon='check-circle'\n",
        "    )\n",
        "\n",
        "    def on_finalize_clicked(b):\n",
        "        try:\n",
        "            col_map = {k: w.value for k, w in mapping_widgets.items()}\n",
        "\n",
        "            # Check for None\n",
        "            if None in col_map.values():\n",
        "                missing = [k for k, v in col_map.items() if v is None]\n",
        "                raise ValueError(f\"Please select a column for: {', '.join(missing)}\")\n",
        "\n",
        "            # Check for duplicates\n",
        "            if len(set(col_map.values())) != len(col_map.values()):\n",
        "                raise ValueError(\"Duplicate mapping detected. You cannot map one column to two fields.\")\n",
        "\n",
        "            global raw_data_standardized\n",
        "            mapped_cols = list(col_map.values())\n",
        "            extra_cols = [c for c in df.columns if c not in mapped_cols]\n",
        "\n",
        "            raw_data_standardized = df[mapped_cols + extra_cols].copy()\n",
        "            raw_data_standardized.rename(columns={v: k for k, v in col_map.items()}, inplace=True)\n",
        "\n",
        "            # Store data type in a temporary global (will be saved to ANALYSIS_CONFIG in Cell 3)\n",
        "            global DATA_TYPE_SELECTED\n",
        "            DATA_TYPE_SELECTED = 'raw'\n",
        "\n",
        "            clear_output()\n",
        "            display(HTML(f\"\"\"\n",
        "            <div style='background-color:#d4edda; color:#155724; padding:15px; border-radius:5px; border:1px solid #c3e6cb'>\n",
        "                <b>✅ SUCCESS! Data Ready (Raw Mode).</b><br>\n",
        "                • {len(raw_data_standardized)} Rows loaded.<br>\n",
        "                • Moderators detected: {len(extra_cols)} ({', '.join(extra_cols[:5])}...)<br>\n",
        "                <br>Please proceed to the next cell to configure analysis filters.\n",
        "            </div>\n",
        "            \"\"\"))\n",
        "            display(raw_data_standardized.head(3))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n",
        "\n",
        "    btn_finalize.on_click(on_finalize_clicked)\n",
        "    ui_rows.append(btn_finalize)\n",
        "\n",
        "    display(widgets.VBox(ui_rows, layout=widgets.Layout(padding='10px')))\n",
        "\n",
        "def render_precalc_mapping(df):\n",
        "    \"\"\"New column mapping for pre-calculated effect sizes\"\"\"\n",
        "    COLUMN_SPECS = PRECALC_COLUMN_SPECS\n",
        "\n",
        "    FIELD_INFO = {\n",
        "        'id':       {'label': 'Study ID / Label:', 'desc': 'Unique identifier for each study.', 'required': True},\n",
        "        'yi':       {'label': 'Effect Size (yi):', 'desc': 'The calculated effect size (e.g., Hedges\\' g, lnRR, etc.).', 'required': True},\n",
        "        'variance': {'label': 'Variance (vi):', 'desc': 'The variance of the effect size.', 'required': False},\n",
        "        'se':       {'label': 'Standard Error (SE):', 'desc': 'The standard error of the effect size (will convert to variance if needed).', 'required': False},\n",
        "        'n_total':  {'label': 'Sample Size (n_total):', 'desc': 'Total sample size (OPTIONAL - useful for diagnostics).', 'required': False}\n",
        "    }\n",
        "\n",
        "    cols_lower = [str(c).lower().strip() for c in df.columns]\n",
        "    mapping_widgets = {}\n",
        "    ui_rows = []\n",
        "\n",
        "    # Header\n",
        "    ui_rows.append(widgets.HTML(\"\"\"\n",
        "    <hr>\n",
        "    <h4 style='color:#2E86AB; margin-top:15px;'>Map Your Columns (Pre-calculated Mode)</h4>\n",
        "    <div style='background-color:#e3f2fd; padding:10px; border-radius:5px; margin-bottom:10px;'>\n",
        "        <b>💡 About Pre-calculated Effect Sizes:</b><br>\n",
        "        Use this mode if you already have calculated effect sizes (e.g., from published papers).<br>\n",
        "        You'll need:<br>\n",
        "        • <b>Effect Size (yi):</b> The standardized effect (g, lnRR, etc.)<br>\n",
        "        • <b>Variance OR Standard Error:</b> The uncertainty measure (map at least one)<br>\n",
        "        • <b>Sample Size (optional):</b> Helps with some diagnostics\n",
        "    </div>\n",
        "    \"\"\"))\n",
        "\n",
        "    # Build Widgets (in specific order)\n",
        "    field_order = ['id', 'yi', 'variance', 'se', 'n_total']\n",
        "\n",
        "    for std_name in field_order:\n",
        "        synonyms = COLUMN_SPECS[std_name]\n",
        "        info = FIELD_INFO[std_name]\n",
        "\n",
        "        # Auto-Guess Logic\n",
        "        guessed_val = None\n",
        "        for syn in synonyms:\n",
        "            if syn in cols_lower:\n",
        "                guessed_val = df.columns[cols_lower.index(syn)]\n",
        "                break\n",
        "\n",
        "        # Add \"None\" option for optional fields\n",
        "        options = ['None'] + list(df.columns) if not info['required'] else list(df.columns)\n",
        "\n",
        "        # Create the Dropdown\n",
        "        w = widgets.Dropdown(\n",
        "            options=options,\n",
        "            value=guessed_val if guessed_val is not None else ('None' if not info['required'] else None),\n",
        "            description=info['label'],\n",
        "            style={'description_width': '180px'},\n",
        "            layout=widgets.Layout(width='600px')\n",
        "        )\n",
        "        mapping_widgets[std_name] = w\n",
        "\n",
        "        # Required indicator\n",
        "        req_text = \" <b style='color:#c0392b;'>(Required)</b>\" if info['required'] else \" <i>(Optional)</i>\"\n",
        "\n",
        "        row_box = widgets.VBox([\n",
        "            w,\n",
        "            widgets.HTML(f\"<div style='margin-left:185px; font-size:11px; color:#666; margin-bottom:8px'><i>{info['desc']}</i>{req_text}</div>\")\n",
        "        ])\n",
        "        ui_rows.append(row_box)\n",
        "\n",
        "    # Save Button\n",
        "    btn_finalize = widgets.Button(\n",
        "        description=\"✓ Confirm Mapping & Finalize Data\",\n",
        "        button_style='success',\n",
        "        layout=widgets.Layout(width='600px', height='40px', margin='20px 0 0 0'),\n",
        "        icon='check-circle'\n",
        "    )\n",
        "\n",
        "    def on_finalize_clicked(b):\n",
        "        try:\n",
        "            col_map = {k: w.value for k, w in mapping_widgets.items()}\n",
        "\n",
        "            # Remove 'None' mappings\n",
        "            col_map = {k: v for k, v in col_map.items() if v != 'None'}\n",
        "\n",
        "            # Validation: Required fields\n",
        "            if 'id' not in col_map or 'yi' not in col_map:\n",
        "                raise ValueError(\"Please map required fields: Study ID and Effect Size (yi)\")\n",
        "\n",
        "            # Validation: Must have variance OR se\n",
        "            if 'variance' not in col_map and 'se' not in col_map:\n",
        "                raise ValueError(\"Please map either Variance (vi) OR Standard Error (SE)\")\n",
        "\n",
        "            # Check for duplicates (excluding None)\n",
        "            mapped_values = list(col_map.values())\n",
        "            if len(set(mapped_values)) != len(mapped_values):\n",
        "                raise ValueError(\"Duplicate mapping detected. You cannot map one column to two fields.\")\n",
        "\n",
        "            global raw_data_standardized\n",
        "            mapped_cols = list(col_map.values())\n",
        "            extra_cols = [c for c in df.columns if c not in mapped_cols]\n",
        "\n",
        "            raw_data_standardized = df[mapped_cols + extra_cols].copy()\n",
        "            raw_data_standardized.rename(columns={v: k for k, v in col_map.items()}, inplace=True)\n",
        "\n",
        "            # Store metadata for Cell 6 to use\n",
        "            global DATA_TYPE_SELECTED, VARIANCE_TYPE_SELECTED\n",
        "            DATA_TYPE_SELECTED = 'pre_calculated'\n",
        "\n",
        "            # Determine which variance type was mapped\n",
        "            if 'variance' in col_map and 'se' in col_map:\n",
        "                VARIANCE_TYPE_SELECTED = 'both'\n",
        "            elif 'variance' in col_map:\n",
        "                VARIANCE_TYPE_SELECTED = 'variance'\n",
        "            else:\n",
        "                VARIANCE_TYPE_SELECTED = 'se'\n",
        "\n",
        "            clear_output()\n",
        "            display(HTML(f\"\"\"\n",
        "            <div style='background-color:#d4edda; color:#155724; padding:15px; border-radius:5px; border:1px solid #c3e6cb'>\n",
        "                <b>✅ SUCCESS! Data Ready (Pre-calculated Mode).</b><br>\n",
        "                • {len(raw_data_standardized)} Rows loaded.<br>\n",
        "                • Effect Size Column: {col_map['yi']}<br>\n",
        "                • Variance Type: {VARIANCE_TYPE_SELECTED.upper()}<br>\n",
        "                • Additional Columns: {len(extra_cols)} ({', '.join(extra_cols[:5])}...)<br>\n",
        "                <br>Please proceed to the next cell to configure analysis filters.\n",
        "            </div>\n",
        "            \"\"\"))\n",
        "            display(raw_data_standardized.head(3))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n",
        "\n",
        "    btn_finalize.on_click(on_finalize_clicked)\n",
        "    ui_rows.append(btn_finalize)\n",
        "\n",
        "    display(widgets.VBox(ui_rows, layout=widgets.Layout(padding='10px')))\n",
        "\n",
        "# =============================================================================\n",
        "# INITIALIZE UI\n",
        "# =============================================================================\n",
        "\n",
        "# Bind inputs\n",
        "btn_auth.on_click(on_auth_clicked)\n",
        "btn_fetch_ws.on_click(on_fetch_ws_clicked)\n",
        "btn_load_sheet.on_click(on_load_sheet_clicked)\n",
        "uploader.observe(on_file_upload, names='value')\n",
        "btn_process_file.on_click(on_process_file_clicked)\n",
        "\n",
        "# Display\n",
        "tabs = widgets.Tab(children=[gs_vbox, local_vbox])\n",
        "tabs.set_title(0, \"Google Sheets\")\n",
        "tabs.set_title(1, \"Upload Excel/CSV\")\n",
        "\n",
        "display(HTML(\"<h3 style='color:#2E86AB'>Step 1: Import Data Source</h3>\"))\n",
        "display(tabs)\n",
        "display(log_output)\n",
        "display(widgets.HTML(\"<hr style='border-top: 1px dashed #ccc;'>\"))\n",
        "display(mapping_output)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eBLUqJAdH8WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ⚙️ Setup: CONFIGURE ANALYSIS\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 3: DATA CLEANING & ANALYSIS CONFIGURATION\n",
        "# Purpose: Convert data types and apply GLOBAL filters (Pre-filters).\n",
        "# Dependencies: Cell 2 (global 'raw_data_standardized')\n",
        "# Output: Global 'ANALYSIS_CONFIG' dictionary\n",
        "# Note: SKIP this cell if using Pre-calculated mode\n",
        "# =============================================================================\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- CHECK IF PRE-CALCULATED MODE ---\n",
        "data_type_mode = globals().get('DATA_TYPE_SELECTED', 'raw')\n",
        "\n",
        "if data_type_mode == 'pre_calculated':\n",
        "    display(HTML(\"\"\"\n",
        "    <div style='background-color:#fff3cd; border-left: 5px solid #ffc107; padding: 20px; border-radius:5px; margin-bottom:20px;'>\n",
        "        <h3 style='color:#856404; margin-top:0;'>⏭️ SKIP THIS CELL (Pre-calculated Mode)</h3>\n",
        "        <p style='color:#856404; margin-bottom:10px;'>\n",
        "            This cell is for <b>Raw Data Mode</b> only (configuring filters based on means and SDs).\n",
        "        </p>\n",
        "        <p style='color:#856404; margin-bottom:0;'>\n",
        "            <b>✓ Action:</b> Skip directly to <b>Cell 5: DETECT & SELECT EFFECT SIZE TYPE</b>\n",
        "        </p>\n",
        "    </div>\n",
        "    \"\"\"))\n",
        "\n",
        "    # Initialize minimal ANALYSIS_CONFIG for pre-calculated mode\n",
        "    if 'ANALYSIS_CONFIG' not in globals():\n",
        "        ANALYSIS_CONFIG = {\n",
        "            'prefilter_col': 'None',\n",
        "            'prefilter_values': [],\n",
        "            'factor1': 'None',\n",
        "            'factor2': 'None',\n",
        "            'min_papers': 1,\n",
        "            'min_obs': 1,\n",
        "            'clean_dataframe': raw_data_standardized.copy() if 'raw_data_standardized' in globals() else None\n",
        "        }\n",
        "\n",
        "else:\n",
        "    # RAW DATA MODE: Full configuration\n",
        "    # --- 1. INITIALIZATION & SAFETY CHECK ---\n",
        "    if 'raw_data_standardized' not in globals():\n",
        "        display(HTML(\"<div style='background-color:#f8d7da; color:#721c24; padding:10px; border-radius:5px;'>❌ <b>Error:</b> Data not found. Please run Cell 2 first.</div>\"))\n",
        "    else:\n",
        "        # --- 2. DATA TYPE CONVERSION (The \"Pre-Clean\") ---\n",
        "        # We work on a copy to preserve the original load\n",
        "        df_config = raw_data_standardized.copy()\n",
        "\n",
        "        # Force numeric conversion for statistics columns\n",
        "        numeric_cols = ['xe', 'sde', 'ne', 'xc', 'sdc', 'nc']\n",
        "        for col in numeric_cols:\n",
        "            # Coerce errors to NaN (e.g., if someone wrote \"n/a\" in a number field)\n",
        "            df_config[col] = pd.to_numeric(df_config[col], errors='coerce')\n",
        "\n",
        "        # Identify Moderators (Any column that isn't a required stat or ID)\n",
        "        reserved_cols = numeric_cols + ['id']\n",
        "        available_moderators = [c for c in df_config.columns if c not in reserved_cols]\n",
        "\n",
        "        # Handle case where no moderators exist\n",
        "        if not available_moderators:\n",
        "            available_moderators = ['None']\n",
        "\n",
        "        # --- 3. WIDGET DEFINITIONS ---\n",
        "\n",
        "        # A. PRE-FILTER SECTION (Global Inclusion/Exclusion)\n",
        "        # --------------------------------------------------\n",
        "        dd_prefilter_mod = widgets.Dropdown(\n",
        "            options=['None'] + available_moderators,\n",
        "            value='None',\n",
        "            description='Filter Variable:',\n",
        "            style={'description_width': '120px'},\n",
        "            layout=widgets.Layout(width='400px')\n",
        "        )\n",
        "\n",
        "        # Container for the checkboxes (populated dynamically)\n",
        "        vbox_prefilter_values = widgets.VBox([])\n",
        "\n",
        "        def on_prefilter_change(change):\n",
        "            \"\"\"Updates checkboxes when a moderator is selected\"\"\"\n",
        "            col = change['new']\n",
        "            if col == 'None':\n",
        "                vbox_prefilter_values.children = []\n",
        "                return\n",
        "\n",
        "            # Get unique values from the column\n",
        "            unique_vals = df_config[col].dropna().unique()\n",
        "\n",
        "            # Create a checkbox for each value\n",
        "            checks = []\n",
        "            checks.append(widgets.HTML(f\"<i>Select which values of <b>{col}</b> to KEEP in the analysis:</i>\"))\n",
        "            for val in unique_vals:\n",
        "                count = len(df_config[df_config[col] == val])\n",
        "                checks.append(widgets.Checkbox(value=True, description=f\"{val} (n={count})\"))\n",
        "\n",
        "            vbox_prefilter_values.children = checks\n",
        "\n",
        "        dd_prefilter_mod.observe(on_prefilter_change, names='value')\n",
        "\n",
        "        # B. SAVE BUTTON\n",
        "        # --------------\n",
        "        btn_save_config = widgets.Button(\n",
        "            description=\"▶ Save Configuration\",\n",
        "            button_style='success',\n",
        "            layout=widgets.Layout(width='400px', height='50px'),\n",
        "            icon='check'\n",
        "        )\n",
        "\n",
        "        output_config = widgets.Output()\n",
        "\n",
        "        def on_save_config_clicked(b):\n",
        "            with output_config:\n",
        "                clear_output()\n",
        "                try:\n",
        "                    # 1. Capture Pre-filter values\n",
        "                    kept_values = []\n",
        "                    if dd_prefilter_mod.value != 'None':\n",
        "                        # The first child is HTML text, so we skip it [1:]\n",
        "                        for cb in vbox_prefilter_values.children[1:]:\n",
        "                            if cb.value:\n",
        "                                # Extract value name from description \"Name (n=5)\"\n",
        "                                val_name = cb.description.rsplit(' (n=', 1)[0]\n",
        "                                kept_values.append(val_name)\n",
        "\n",
        "                    # 2. Build Config Dictionary\n",
        "                    global ANALYSIS_CONFIG\n",
        "                    ANALYSIS_CONFIG = {\n",
        "                        'prefilter_col': dd_prefilter_mod.value,\n",
        "                        'prefilter_values': kept_values,\n",
        "                        # We set defaults for legacy compatibility with Cell 4\n",
        "                        'factor1': 'None',\n",
        "                        'factor2': 'None',\n",
        "                        'min_papers': 1,\n",
        "                        'min_obs': 1,\n",
        "                        'clean_dataframe': df_config\n",
        "                    }\n",
        "\n",
        "                    # 3. Success Message\n",
        "                    print(\"=\"*60)\n",
        "                    print(\"✅ CONFIGURATION SAVED\")\n",
        "                    print(\"=\"*60)\n",
        "                    if dd_prefilter_mod.value != 'None':\n",
        "                        print(f\"• Global Filter: {ANALYSIS_CONFIG['prefilter_col']} ({len(kept_values)} values kept)\")\n",
        "                    else:\n",
        "                        print(\"• Global Filter: None (All data included)\")\n",
        "                    print(\"\\n⬇ You may now proceed to Cell 4 to clean and prepare data.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error saving config: {e}\")\n",
        "\n",
        "        btn_save_config.on_click(on_save_config_clicked)\n",
        "\n",
        "        # --- 4. LAYOUT & DISPLAY ---\n",
        "\n",
        "        # Create container box\n",
        "        box_pre = widgets.VBox([\n",
        "            widgets.HTML(\"<h4 style='color:#444'>Global Data Filtering (Optional)</h4><p style='font-size:11px; margin-top:0'>Exclude specific regions, species, or types from the <b>entire</b> analysis.</p>\"),\n",
        "            dd_prefilter_mod,\n",
        "            vbox_prefilter_values\n",
        "        ], layout=widgets.Layout(border='1px solid #ddd', padding='10px', margin='0 0 10px 0', width='95%'))\n",
        "\n",
        "        # Final Display\n",
        "        display(widgets.HTML(f\"<h3 style='color:#2E86AB'>Step 3: Configure Analysis</h3>\"))\n",
        "        display(widgets.HTML(f\"<i>Dataset loaded with <b>{len(df_config)}</b> rows.</i>\"))\n",
        "        display(box_pre)\n",
        "        display(widgets.HTML(\"<hr>\"))\n",
        "        display(btn_save_config, output_config)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "d3R6H3L3JD9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ⚙️ Setup: APPLY CONFIGURATION & PREPARE DATA\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 4: APPLY CONFIGURATION & PREPARE DATA\n",
        "# Purpose: Apply filters and prepare data for effect size calculation\n",
        "# Note: SKIP this cell if using Pre-calculated mode\n",
        "# =============================================================================\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# DECLARE GLOBALS AT THE TOP (before any assignments)\n",
        "global raw_data, data_filtered\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SHARED CONTROL GROUP DETECTION FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def detect_shared_controls(df):\n",
        "    \"\"\"\n",
        "    Detects rows within the same study (id) that share control group data.\n",
        "\n",
        "    Logic:\n",
        "    - Group by 'id'\n",
        "    - Within each study, identify rows with identical nc, xc, and sdc\n",
        "    - Assign a unique shared_group_id to rows sharing controls\n",
        "    - Rows that don't share controls get None\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Dataframe with columns: id, nc, xc, sdc\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        Original dataframe with added column 'shared_group_id'\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    df = df.copy()\n",
        "\n",
        "    # Initialize the shared_group_id column\n",
        "    df['shared_group_id'] = None\n",
        "\n",
        "    # Check if required columns exist\n",
        "    required_cols = ['id', 'nc', 'xc', 'sdc']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        return df\n",
        "\n",
        "    shared_count = 0\n",
        "    group_counter = 0\n",
        "\n",
        "    # Group by study ID\n",
        "    for study_id, group in df.groupby('id'):\n",
        "        # Skip if only one row in the study (no potential for sharing)\n",
        "        if len(group) == 1:\n",
        "            continue\n",
        "\n",
        "        # Create a composite key for control characteristics\n",
        "        # Round to handle floating point comparison issues\n",
        "        group = group.copy()\n",
        "        group['_control_key'] = (\n",
        "            group['nc'].fillna(-999).astype(str) + '_' +\n",
        "            group['xc'].fillna(-999).round(6).astype(str) + '_' +\n",
        "            group['sdc'].fillna(-999).round(6).astype(str)\n",
        "        )\n",
        "\n",
        "        # Find groups of rows with identical control data\n",
        "        control_groups = group.groupby('_control_key')\n",
        "\n",
        "        for control_key, control_group in control_groups:\n",
        "            # Only assign shared_group_id if 2+ rows share the same control\n",
        "            if len(control_group) >= 2:\n",
        "                # Create a unique group identifier\n",
        "                group_counter += 1\n",
        "                shared_id = f\"{study_id}_shared_grp_{group_counter}\"\n",
        "\n",
        "                # Assign this ID to all rows in the dataframe that match\n",
        "                df.loc[control_group.index, 'shared_group_id'] = shared_id\n",
        "                shared_count += len(control_group)\n",
        "\n",
        "    return df, shared_count\n",
        "\n",
        "# --- CHECK IF PRE-CALCULATED MODE ---\n",
        "data_type_mode = globals().get('DATA_TYPE_SELECTED', 'raw')\n",
        "\n",
        "if data_type_mode == 'pre_calculated':\n",
        "    display(HTML(\"\"\"\n",
        "    <div style='background-color:#fff3cd; border-left: 5px solid #ffc107; padding: 20px; border-radius:5px; margin-bottom:20px;'>\n",
        "        <h3 style='color:#856404; margin-top:0;'>⏭️ SKIP THIS CELL (Pre-calculated Mode)</h3>\n",
        "        <p style='color:#856404; margin-bottom:10px;'>\n",
        "            This cell applies filtering rules for <b>Raw Data Mode</b>.\n",
        "        </p>\n",
        "        <p style='color:#856404; margin-bottom:0;'>\n",
        "            <b>✓ Action:</b> Skip directly to <b>Cell 5: DETECT & SELECT EFFECT SIZE TYPE</b>\n",
        "        </p>\n",
        "    </div>\n",
        "    \"\"\"))\n",
        "\n",
        "    # For pre-calculated mode, just pass through the data\n",
        "    # No filtering needed - user's effect sizes are already calculated\n",
        "    if 'raw_data_standardized' in globals():\n",
        "        raw_data = raw_data_standardized.copy()\n",
        "        data_filtered = raw_data_standardized.copy()\n",
        "\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style='background-color:#d4edda; color:#155724; padding:15px; border-radius:5px; margin-top:10px;'>\n",
        "            <b>✓ Pre-calculated Mode: Data Passed Through</b><br>\n",
        "            • {len(data_filtered)} observations ready for analysis<br>\n",
        "            • No filtering applied (data already processed)\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "    else:\n",
        "        display(HTML(\"\"\"\n",
        "        <div style='background-color:#f8d7da; color:#721c24; padding:15px; border-radius:5px;'>\n",
        "            <b>❌ Error:</b> raw_data_standardized not found. Please run Cell 2 first.\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "else:\n",
        "    # RAW DATA MODE: Apply full filtering logic\n",
        "    # --- UI SETUP ---\n",
        "    tab_summary = widgets.Output()\n",
        "    tab_removed = widgets.Output()\n",
        "\n",
        "    tabs = widgets.Tab(children=[tab_summary, tab_removed])\n",
        "    tabs.set_title(0, '📊 Data Summary')\n",
        "    tabs.set_title(1, '🗑️ Removed Data')\n",
        "\n",
        "    display(tabs)\n",
        "\n",
        "    # Use the summary tab for the main progress updates initially\n",
        "    with tab_summary:\n",
        "        clear_output()\n",
        "        print(\"⏳ Applying filters and cleaning data...\")\n",
        "\n",
        "        try:\n",
        "            # --- 1. Safety Check & Legacy Translation ---\n",
        "            if 'ANALYSIS_CONFIG' not in globals():\n",
        "                raise NameError(\"Configuration not set. Please run Cell 3 and click 'Save Configuration'.\")\n",
        "\n",
        "            # Bridge: Ensure legacy keys exist for the downstream pipeline\n",
        "            if 'filterCol1' not in ANALYSIS_CONFIG:\n",
        "                ANALYSIS_CONFIG['filterCol1'] = ANALYSIS_CONFIG.get('factor1', 'None')\n",
        "            if 'filterCol2' not in ANALYSIS_CONFIG:\n",
        "                ANALYSIS_CONFIG['filterCol2'] = ANALYSIS_CONFIG.get('factor2', 'None')\n",
        "            if 'minPapers' not in ANALYSIS_CONFIG:\n",
        "                ANALYSIS_CONFIG['minPapers'] = ANALYSIS_CONFIG.get('min_papers', 1)\n",
        "            if 'minObservations' not in ANALYSIS_CONFIG:\n",
        "                ANALYSIS_CONFIG['minObservations'] = ANALYSIS_CONFIG.get('min_obs', 1)\n",
        "\n",
        "            # --- 2. Load & Clean Data ---\n",
        "            if 'clean_dataframe' not in ANALYSIS_CONFIG:\n",
        "                raise ValueError(\"Dataframe missing from config. Please re-run Cell 3.\")\n",
        "\n",
        "            raw_data = ANALYSIS_CONFIG['clean_dataframe'].copy()\n",
        "            original_rows = len(raw_data)\n",
        "            cleaning_log = []\n",
        "            removed_records = [] # List to store removed dataframes\n",
        "\n",
        "            # Ensure ID is string\n",
        "            if 'id' in raw_data.columns:\n",
        "                raw_data['id'] = raw_data['id'].astype(str).str.strip()\n",
        "\n",
        "            # Check Essential Cols\n",
        "            essential_cols = ['xe', 'ne', 'xc', 'nc']\n",
        "            missing_cols = [c for c in essential_cols if c not in raw_data.columns]\n",
        "            if missing_cols:\n",
        "                raise ValueError(f\"Critical columns missing from data: {missing_cols}\")\n",
        "\n",
        "            # A. Drop missing values\n",
        "            mask_missing = raw_data[essential_cols].isna().any(axis=1)\n",
        "            if mask_missing.sum() > 0:\n",
        "                dropped = raw_data[mask_missing].copy()\n",
        "                dropped['Reason'] = 'Missing Essential Data (Mean or N)'\n",
        "                removed_records.append(dropped)\n",
        "\n",
        "                raw_data = raw_data[~mask_missing].copy()\n",
        "                cleaning_log.append(f\"Removed {mask_missing.sum()} rows with missing means or sample sizes\")\n",
        "\n",
        "            # B. Ensure N >= 1\n",
        "            # First convert to numeric safely\n",
        "            for col in ['ne', 'nc']:\n",
        "                raw_data[col] = pd.to_numeric(raw_data[col], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "            mask_invalid_n = (raw_data['ne'] < 1) | (raw_data['nc'] < 1)\n",
        "            if mask_invalid_n.sum() > 0:\n",
        "                dropped = raw_data[mask_invalid_n].copy()\n",
        "                dropped['Reason'] = 'Invalid Sample Size (N < 1)'\n",
        "                removed_records.append(dropped)\n",
        "\n",
        "                raw_data = raw_data[~mask_invalid_n].copy()\n",
        "                cleaning_log.append(f\"Removed {mask_invalid_n.sum()} rows where N < 1\")\n",
        "\n",
        "            final_rows = len(raw_data)\n",
        "\n",
        "            # --- 3. Identify Moderators ---\n",
        "            excluded_cols = ['id', 'xe', 'sde', 'ne', 'xc', 'sdc', 'nc']\n",
        "            global available_moderators\n",
        "            available_moderators = [col for col in raw_data.columns if col not in excluded_cols]\n",
        "\n",
        "            # --- 4. Apply Pre-filter ---\n",
        "            data_filtered = raw_data.copy()\n",
        "\n",
        "            prefilter_col = ANALYSIS_CONFIG.get('prefilter_col', 'None')\n",
        "            selected_values = ANALYSIS_CONFIG.get('prefilter_values', [])\n",
        "            rows_dropped_filter = 0\n",
        "\n",
        "            if prefilter_col != 'None' and selected_values:\n",
        "                # Identify rows that DO NOT match the selection\n",
        "                mask_filtered_out = ~data_filtered[prefilter_col].isin(selected_values)\n",
        "\n",
        "                if mask_filtered_out.sum() > 0:\n",
        "                    dropped = data_filtered[mask_filtered_out].copy()\n",
        "                    dropped['Reason'] = f\"Filtered by User ('{prefilter_col}')\"\n",
        "                    removed_records.append(dropped)\n",
        "\n",
        "                    data_filtered = data_filtered[~mask_filtered_out].copy()\n",
        "                    rows_dropped_filter = mask_filtered_out.sum()\n",
        "                    cleaning_log.append(f\"Filtered out {rows_dropped_filter} rows based on selection in '{prefilter_col}'\")\n",
        "\n",
        "            # --- 5. Detect Shared Controls ---\n",
        "            data_filtered, n_shared = detect_shared_controls(data_filtered)\n",
        "            if n_shared > 0:\n",
        "                n_shared_groups = data_filtered['shared_group_id'].notna().sum()\n",
        "                cleaning_log.append(f\"Detected {n_shared} observations in shared control groups\")\n",
        "\n",
        "            # --- 6. Save Metadata ---\n",
        "            global LOAD_METADATA\n",
        "            LOAD_METADATA = {\n",
        "                'timestamp': datetime.datetime.now(),\n",
        "                'original_rows': original_rows,\n",
        "                'final_rows_cleaned': final_rows,\n",
        "                'final_rows_filtered': len(data_filtered),\n",
        "                'cleaning_log': cleaning_log,\n",
        "                'available_moderators': available_moderators\n",
        "            }\n",
        "\n",
        "            if removed_records:\n",
        "                ANALYSIS_CONFIG['removed_records'] = pd.concat(removed_records)\n",
        "            else:\n",
        "                ANALYSIS_CONFIG['removed_records'] = pd.DataFrame()\n",
        "            # Update Config\n",
        "            ANALYSIS_CONFIG['n_observations_pre_filter'] = final_rows\n",
        "            ANALYSIS_CONFIG['n_observations_post_filter'] = len(data_filtered)\n",
        "            ANALYSIS_CONFIG['n_papers_post_filter'] = data_filtered['id'].nunique()\n",
        "\n",
        "            # --- 7. RENDER TABS ---\n",
        "\n",
        "            # --- TAB 1: SUMMARY ---\n",
        "            clear_output()\n",
        "\n",
        "            # Build Filter Info\n",
        "            filter_status = \"None\"\n",
        "            if prefilter_col != 'None':\n",
        "                filter_status = f\"Filtered by <b>{prefilter_col}</b> (Dropped {rows_dropped_filter} rows)\"\n",
        "\n",
        "            # Calculate total removed\n",
        "            total_removed = original_rows - len(data_filtered)\n",
        "\n",
        "            html_card = f\"\"\"\n",
        "            <div style=\"font-family: sans-serif; border: 1px solid #c3e6cb; border-radius: 8px; overflow: hidden; max-width: 800px;\">\n",
        "                <div style=\"background-color: #d4edda; padding: 15px; color: #155724;\">\n",
        "                    <h3 style=\"margin: 0;\">✅ Data Successfully Prepared</h3>\n",
        "                </div>\n",
        "\n",
        "                <div style=\"padding: 20px;\">\n",
        "                    <div style=\"display: flex; gap: 20px; margin-bottom: 20px;\">\n",
        "                        <div style=\"flex: 1; background: #f8f9fa; padding: 10px; border-radius: 5px; text-align: center;\">\n",
        "                            <div style=\"font-size: 24px; font-weight: bold; color: #2E86AB;\">{len(data_filtered)}</div>\n",
        "                            <div style=\"font-size: 12px; color: #666;\">Rows for Analysis</div>\n",
        "                        </div>\n",
        "                        <div style=\"flex: 1; background: #f8f9fa; padding: 10px; border-radius: 5px; text-align: center;\">\n",
        "                            <div style=\"font-size: 24px; font-weight: bold; color: #2E86AB;\">{data_filtered['id'].nunique()}</div>\n",
        "                            <div style=\"font-size: 12px; color: #666;\">Unique Studies</div>\n",
        "                        </div>\n",
        "                        <div style=\"flex: 1; background: #fff3cd; padding: 10px; border-radius: 5px; text-align: center;\">\n",
        "                            <div style=\"font-size: 24px; font-weight: bold; color: #856404;\">{total_removed}</div>\n",
        "                            <div style=\"font-size: 12px; color: #666;\">Total Removed</div>\n",
        "                        </div>\n",
        "                    </div>\n",
        "\n",
        "                    <table style=\"width: 100%; font-size: 13px; color: #333; border-collapse: collapse;\">\n",
        "                        <tr style=\"border-bottom: 1px solid #eee;\">\n",
        "                            <td style=\"padding: 8px; font-weight: bold;\">Subgroup Analysis:</td>\n",
        "                            <td style=\"padding: 8px;\">{ANALYSIS_CONFIG['filterCol1']} & {ANALYSIS_CONFIG['filterCol2']}</td>\n",
        "                        </tr>\n",
        "                        <tr style=\"border-bottom: 1px solid #eee;\">\n",
        "                            <td style=\"padding: 8px; font-weight: bold;\">Pre-Filter Status:</td>\n",
        "                            <td style=\"padding: 8px;\">{filter_status}</td>\n",
        "                        </tr>\n",
        "                        <tr>\n",
        "                            <td style=\"padding: 8px; font-weight: bold;\">Status:</td>\n",
        "                            <td style=\"padding: 8px;\">Ready for Meta-Analysis Models ▶</td>\n",
        "                        </tr>\n",
        "                    </table>\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            display(HTML(html_card))\n",
        "\n",
        "            # --- TAB 2: REMOVED DATA ---\n",
        "            with tab_removed:\n",
        "                clear_output()\n",
        "                if removed_records:\n",
        "                    all_removed_df = pd.concat(removed_records)\n",
        "\n",
        "                    display(HTML(f\"<h4 style='color:#c0392b; margin-top:0;'>🗑️ Removed Observations ({len(all_removed_df)})</h4>\"))\n",
        "\n",
        "                    # Select useful columns to display\n",
        "                    base_cols = ['id', 'Reason']\n",
        "                    data_cols = ['xe', 'ne', 'xc', 'nc']\n",
        "                    # Add moderator info if available\n",
        "                    extra_cols = [c for c in raw_data.columns if c not in base_cols + data_cols and c in ['Year', 'Species', 'Region', prefilter_col]]\n",
        "                    # Remove None/duplicates\n",
        "                    extra_cols = [c for c in extra_cols if c != 'None']\n",
        "\n",
        "                    cols_to_show = base_cols + data_cols + extra_cols\n",
        "                    # Ensure columns exist\n",
        "                    cols_to_show = [c for c in cols_to_show if c in all_removed_df.columns]\n",
        "\n",
        "                    display(all_removed_df[cols_to_show])\n",
        "                else:\n",
        "                    display(HTML(\"\"\"\n",
        "                    <div style='padding:15px; background:#d4edda; color:#155724; border-radius:5px;'>\n",
        "                        <b>✅ No data removed.</b><br>\n",
        "                        All loaded rows passed the quality checks and filter criteria.\n",
        "                    </div>\n",
        "                    \"\"\"))\n",
        "\n",
        "        except Exception as e:\n",
        "            clear_output()\n",
        "            display(HTML(f\"\"\"\n",
        "            <div style=\"border: 1px solid #f5c6cb; border-radius: 8px; padding: 20px; background-color: #f8d7da; color: #721c24;\">\n",
        "                <h3 style=\"margin-top: 0;\">❌ Error Preparing Data</h3>\n",
        "                <pre style=\"background: rgba(255,255,255,0.5); padding: 10px; border-radius: 5px;\">{e}</pre>\n",
        "            </div>\n",
        "            \"\"\"))\n",
        "            traceback.print_exc(file=sys.stdout)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4VzL6TS_I81d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔬 DETECT & SELECT EFFECT SIZE TYPE\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 5: EFFECT SIZE TYPE DETECTION AND SELECTION (DUAL-MODE REFACTOR)\n",
        "# Purpose: Analyze data characteristics and recommend appropriate effect size\n",
        "# Modes: 1) Raw Data (auto-detection) or 2) Pre-calculated (manual selection)\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# --- CHECK DATA TYPE MODE ---\n",
        "data_type_mode = globals().get('DATA_TYPE_SELECTED', 'raw')\n",
        "\n",
        "# =============================================================================\n",
        "# MODE 1: RAW DATA - INTELLIGENT RECOMMENDATION\n",
        "# =============================================================================\n",
        "if data_type_mode == 'raw':\n",
        "    # --- 1. STABILITY FIX: USE RAW DATA IF AVAILABLE ---\n",
        "    target_df = raw_data if 'raw_data' in globals() else data_filtered\n",
        "\n",
        "    # --- 2. ANALYZE DATA ---\n",
        "    xe_stats = target_df['xe'].describe()\n",
        "    xc_stats = target_df['xc'].describe()\n",
        "\n",
        "    # Standard Deviations\n",
        "    has_sde = 'sde' in target_df.columns and target_df['sde'].notna().any()\n",
        "    has_sdc = 'sdc' in target_df.columns and target_df['sdc'].notna().any()\n",
        "    sd_availability = target_df[['sde', 'sdc']].notna().all(axis=1).sum() if has_sde and has_sdc else 0\n",
        "    sd_pct = (sd_availability / len(target_df)) * 100 if len(target_df) > 0 else 0\n",
        "\n",
        "    # 1. Normalization Check\n",
        "    control_near_one = ((target_df['xc'] >= 0.95) & (target_df['xc'] <= 1.05)).sum()\n",
        "    control_exactly_one = (target_df['xc'] == 1.0).sum()\n",
        "    pct_control_near_one = (control_near_one / len(target_df)) * 100\n",
        "    pct_control_exactly_one = (control_exactly_one / len(target_df)) * 100\n",
        "\n",
        "    # 2. Negative Values\n",
        "    n_negative_xe = (target_df['xe'] < 0).sum()\n",
        "    n_negative_xc = (target_df['xc'] < 0).sum()\n",
        "    has_negative = n_negative_xe > 0 or n_negative_xc > 0\n",
        "\n",
        "    # 3. Zero Values\n",
        "    n_zero_xe = (target_df['xe'] == 0).sum()\n",
        "    n_zero_xc = (target_df['xc'] == 0).sum()\n",
        "    has_zero = n_zero_xe > 0 or n_zero_xc > 0\n",
        "\n",
        "    # 4. Scale Heterogeneity\n",
        "    xe_range = xe_stats['max'] - xe_stats['min']\n",
        "    xc_range = xc_stats['max'] - xc_stats['min']\n",
        "    scale_ratio = max(xe_range, xc_range) / (min(xe_range, xc_range) + 0.0001)\n",
        "\n",
        "    # --- 3. RECOMMENDATION LOGIC ---\n",
        "    score_lnRR = 0\n",
        "    score_hedges_g = 0\n",
        "    reasons = []\n",
        "\n",
        "    # Rule 1: Negatives (The \"Hard\" Constraint)\n",
        "    if has_negative:\n",
        "        score_hedges_g += 10\n",
        "        reasons.append(('Negative Values', '+++', 'Hedges g', 'Ratio metrics (lnRR) mathematically fail with negative numbers.'))\n",
        "    else:\n",
        "        score_lnRR += 2\n",
        "        reasons.append(('All Positive', '+', 'lnRR', 'Data is compatible with ratio-based metrics.'))\n",
        "\n",
        "    # Rule 2: Normalization\n",
        "    if pct_control_exactly_one > 50:\n",
        "        score_lnRR += 5\n",
        "        reasons.append(('Fold-Change Data', '+++', 'lnRR', 'Controls set to 1.0 implies data is already a ratio.'))\n",
        "    elif pct_control_near_one > 30:\n",
        "        score_lnRR += 3\n",
        "        reasons.append(('Normalized Data', '++', 'lnRR', 'Controls clustered around 1.0 suggests ratio data.'))\n",
        "    elif 0.8 < xc_stats['mean'] < 1.2:\n",
        "        score_lnRR += 1\n",
        "        reasons.append(('Unity Baseline', '+', 'lnRR', 'Control group mean is close to 1.0.'))\n",
        "\n",
        "    # Rule 3: Heterogeneity\n",
        "    if scale_ratio > 100:\n",
        "        score_lnRR += 3\n",
        "        reasons.append(('High Scale Variance', '+++', 'lnRR', 'Studies measure vastly different scales. Ratios handle this best.'))\n",
        "    elif scale_ratio > 10:\n",
        "        score_lnRR += 2\n",
        "        reasons.append(('Moderate Scale Variance', '++', 'lnRR', 'Ratios normalize scale differences effectively.'))\n",
        "    else:\n",
        "        score_hedges_g += 1\n",
        "        reasons.append(('Consistent Scales', '+', 'Hedges g', 'Scales are similar across studies; standardized differences work well.'))\n",
        "\n",
        "    # Rule 4: Zeros\n",
        "    if has_zero:\n",
        "        score_hedges_g += 2\n",
        "        reasons.append(('Zero Values', '++', 'Hedges g', 'log(0) is undefined. lnRR requires adding arbitrary constants.'))\n",
        "\n",
        "    # Rule 5: SD Availability\n",
        "    if sd_pct > 80:\n",
        "        score_hedges_g += 1\n",
        "        reasons.append(('Good SD Data', '+', 'Hedges g', 'Hedges g requires SDs. Your data has good coverage.'))\n",
        "    elif sd_pct < 20:\n",
        "        reasons.append(('Missing SDs', '⚠', 'Neither', 'Hedges g requires imputation. Response Ratios might be safer if SDs are rare.'))\n",
        "\n",
        "    # Winner\n",
        "    score_diff = abs(score_lnRR - score_hedges_g)\n",
        "    if score_lnRR > score_hedges_g:\n",
        "        recommended_type = 'lnRR'\n",
        "        confidence = \"High\" if score_diff >= 5 else \"Moderate\" if score_diff >= 3 else \"Low\"\n",
        "    elif score_hedges_g > score_lnRR:\n",
        "        recommended_type = 'hedges_g'\n",
        "        confidence = \"High\" if score_diff >= 5 else \"Moderate\" if score_diff >= 3 else \"Low\"\n",
        "    else:\n",
        "        recommended_type = 'hedges_g'\n",
        "        confidence = \"Low\"\n",
        "\n",
        "    # --- 4. SETUP UI ---\n",
        "    tab_main = widgets.Output()\n",
        "    tab_patterns = widgets.Output()\n",
        "    tab_logic = widgets.Output()\n",
        "\n",
        "    tabs = widgets.Tab(children=[tab_main, tab_patterns, tab_logic])\n",
        "    tabs.set_title(0, '💡 Recommendation')\n",
        "    tabs.set_title(1, '📊 Data Patterns')\n",
        "    tabs.set_title(2, '🧠 Decision Logic')\n",
        "\n",
        "    # --- TAB 2: DATA PATTERNS (Educational) ---\n",
        "    with tab_patterns:\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style='padding:10px; font-size:14px; line-height:1.6;'>\n",
        "            <h4 style='margin-top:0; color:#2E86AB;'>🔍 Diagnostic Checks</h4>\n",
        "            <p>We analyzed <b>{len(target_df)} observations</b> to determine the statistical properties of your dataset.\n",
        "            Here is what we found:</p>\n",
        "\n",
        "            <hr>\n",
        "\n",
        "            <b>1️⃣ Control Group Normalization</b><br>\n",
        "            Values near 1.0 often indicate \"Fold-Change\" data (e.g., gene expression normalized to a control).<br>\n",
        "            • <b>Result:</b> {pct_control_exactly_one:.1f}% of controls are exactly 1.0.<br>\n",
        "            • <b>Implication:</b> {'Strong evidence for Ratio data.' if pct_control_exactly_one > 50 else 'No strong evidence of pre-normalization.'}\n",
        "            <br><br>\n",
        "\n",
        "            <b>2️⃣ Negative Values</b><br>\n",
        "            Log-based metrics (like lnRR) <i>cannot</i> mathematically handle negative numbers.<br>\n",
        "            • <b>Result:</b> Found {n_negative_xe + n_negative_xc} negative values.<br>\n",
        "            • <b>Implication:</b> {'❌ MUST use Standardized Difference (Hedges g).' if has_negative else '✓ Compatible with Ratio metrics.'}\n",
        "            <br><br>\n",
        "\n",
        "            <b>3️⃣ Zero Values</b><br>\n",
        "            Log of zero is undefined. Zeros require adding a \"small constant\" to work with lnRR.<br>\n",
        "            • <b>Result:</b> Found {n_zero_xe + n_zero_xc} zero values.<br>\n",
        "            • <b>Implication:</b> {'⚠️ lnRR will require adjustment.' if has_zero else '✓ Clean data.'}\n",
        "            <br><br>\n",
        "\n",
        "            <b>4️⃣ Scale Heterogeneity</b><br>\n",
        "            Do studies measure things on the same scale (e.g., all in grams) or different scales (grams vs. tons)?<br>\n",
        "            • <b>Result:</b> Largest value is {scale_ratio:.1f}× larger than the smallest range.<br>\n",
        "            • <b>Implication:</b> {'High variation favors Ratios (lnRR).' if scale_ratio > 10 else 'Low variation allows Standardized Differences.'}\n",
        "            <br><br>\n",
        "\n",
        "            <b>5️⃣ Data Completeness</b><br>\n",
        "            Standardized differences (Hedges' g) require Standard Deviations (SD) to calculate.<br>\n",
        "            • <b>Result:</b> {sd_pct:.1f}% of rows have valid SDs.<br>\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "    # --- TAB 3: LOGIC (Educational) ---\n",
        "    with tab_logic:\n",
        "        # Create HTML table rows\n",
        "        rows_html = \"\"\n",
        "        for r in reasons:\n",
        "            rows_html += f\"<tr><td><b>{r[0]}</b></td><td>{r[1]}</td><td>{r[2]}</td><td>{r[3]}</td></tr>\"\n",
        "\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style='padding:10px; font-size:14px;'>\n",
        "            <h4 style='margin-top:0; color:#2E86AB;'>🧠 How the Algorithm Decides</h4>\n",
        "            <p>We use a weighted scoring system to recommend the most statistically appropriate effect size.\n",
        "            Some factors (like negative values) are \"hard constraints,\" while others are preferences.</p>\n",
        "\n",
        "            <table style='width:100%; border-collapse:collapse; margin-top:10px;'>\n",
        "                <tr style='background-color:#f0f0f0; text-align:left; border-bottom:2px solid #ddd;'>\n",
        "                    <th style='padding:8px;'>Diagnostic Factor</th>\n",
        "                    <th style='padding:8px;'>Weight</th>\n",
        "                    <th style='padding:8px;'>Favors</th>\n",
        "                    <th style='padding:8px;'>Educational Note</th>\n",
        "                </tr>\n",
        "                {rows_html}\n",
        "            </table>\n",
        "\n",
        "            <div style='margin-top:20px; padding:10px; background-color:#eef; border-radius:5px;'>\n",
        "                <b>Final Score:</b><br>\n",
        "                📊 <b>log Response Ratio (lnRR):</b> {score_lnRR} points<br>\n",
        "                📊 <b>Hedges' g (SMD):</b> {score_hedges_g} points\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "    # --- TAB 1: MAIN (Selection) ---\n",
        "    with tab_main:\n",
        "        # 1. Recommendation Box\n",
        "        if recommended_type == 'lnRR':\n",
        "            html_rec = f\"\"\"\n",
        "            <div style='background-color: #d4edda; border-left: 5px solid #28a745; padding: 15px; margin-bottom: 20px;'>\n",
        "                <h3 style='color: #155724; margin-top: 0;'>💡 Recommendation: log Response Ratio (lnRR)</h3>\n",
        "                <p style='color: #155724; margin-bottom: 0;'><b>Why?</b> Your data appears to be <b>ratio-based</b> (e.g., fold-changes, growth rates).\n",
        "                lnRR is the natural metric for this data type because it handles scale differences and has a direct biological interpretation (% change).</p>\n",
        "            </div>\"\"\"\n",
        "        else:\n",
        "            html_rec = f\"\"\"\n",
        "            <div style='background-color: #d1ecf1; border-left: 5px solid #17a2b8; padding: 15px; margin-bottom: 20px;'>\n",
        "                <h3 style='color: #0c5460; margin-top: 0;'>💡 Recommendation: Hedges' g (SMD)</h3>\n",
        "                <p style='color: #0c5460; margin-bottom: 0;'><b>Why?</b> Your data appears to be <b>absolute measurements</b> on potentially different scales.\n",
        "                Hedges' g is ideal here because it standardizes effects into \"SD units,\" making them comparable even if units differ.</p>\n",
        "            </div>\"\"\"\n",
        "\n",
        "        display(HTML(html_rec))\n",
        "\n",
        "        # 2. Selection Widget\n",
        "        effect_size_widget = widgets.RadioButtons(\n",
        "            options=[\n",
        "                ('log Response Ratio (lnRR) - for ratio/fold-change data', 'lnRR'),\n",
        "                (\"Hedges' g - for standardized mean differences (corrected)\", 'hedges_g'),\n",
        "                (\"Cohen's d - for standardized mean differences (uncorrected)\", 'cohen_d'),\n",
        "                ('log Odds Ratio (logOR) - for binary outcomes', 'log_or')\n",
        "            ],\n",
        "            value=recommended_type,\n",
        "            description='Select Type:',\n",
        "            style={'description_width': 'initial'},\n",
        "            layout=widgets.Layout(width='600px')\n",
        "        )\n",
        "\n",
        "        # 3. Info Panel\n",
        "        info_output = widgets.Output()\n",
        "        info_panels = {\n",
        "            'lnRR': \"<div style='padding:10px; background:#fff; border:1px solid #ddd; color:#555; font-size:13px;'><b>🎓 About lnRR:</b> Calculates the log-ratio of means (ln(Xe/Xc)). Essential for data where 'doubling' is the same magnitude of effect as 'halving'. Commonly used in ecology for biomass, abundance, and size.</div>\",\n",
        "            'hedges_g': \"<div style='padding:10px; background:#fff; border:1px solid #ddd; color:#555; font-size:13px;'><b>🎓 About Hedges' g:</b> A variation of Cohen's d that includes a correction factor (J) for small sample sizes. It prevents overestimation of effects in small studies, making it the gold standard for SMD in meta-analysis.</div>\",\n",
        "            'cohen_d': \"<div style='padding:10px; background:#fff; border:1px solid #ddd; color:#555; font-size:13px;'><b>🎓 About Cohen's d:</b> The classic standardized mean difference. It is slightly biased (too high) when sample sizes are small (N < 20). Hedges' g is usually preferred.</div>\",\n",
        "            'log_or': \"<div style='padding:10px; background:#fff; border:1px solid #ddd; color:#555; font-size:13px;'><b>🎓 About logOR:</b> The log-odds ratio. Strictly for binary 'Yes/No' or 'Event/Non-event' data. Do not use for continuous measurements like weight or length.</div>\"\n",
        "        }\n",
        "\n",
        "        def update_info(change):\n",
        "            with info_output:\n",
        "                clear_output()\n",
        "                display(HTML(info_panels[change['new']]))\n",
        "\n",
        "        effect_size_widget.observe(update_info, names='value')\n",
        "        with info_output: display(HTML(info_panels[recommended_type]))\n",
        "\n",
        "        # 4. Confirm Button\n",
        "        proceed_button = widgets.Button(\n",
        "            description='✓ Confirm Selection',\n",
        "            button_style='success',\n",
        "            layout=widgets.Layout(width='300px', height='40px'),\n",
        "            style={'font_weight': 'bold'}\n",
        "        )\n",
        "        proceed_output = widgets.Output()\n",
        "\n",
        "        def on_proceed(b):\n",
        "            with proceed_output:\n",
        "                clear_output()\n",
        "                sel = effect_size_widget.value\n",
        "                print(f\"✓ Confirmed Selection: {sel}\")\n",
        "\n",
        "                # --- CONFIGURATION ---\n",
        "                es_configs = {\n",
        "                    'lnRR': {\n",
        "                        'effect_col': 'lnRR', 'var_col': 'var_lnRR', 'se_col': 'SE_lnRR',\n",
        "                        'ci_lower_col': 'CI_lower_lnRR', 'ci_upper_col': 'CI_upper_lnRR',\n",
        "                        'effect_label': 'log Response Ratio', 'effect_label_short': 'lnRR',\n",
        "                        'has_fold_change': True, 'null_value': 0, 'scale': 'log', 'allows_negative': False\n",
        "                    },\n",
        "                    'hedges_g': {\n",
        "                        'effect_col': 'hedges_g', 'var_col': 'Vg', 'se_col': 'SE_g',\n",
        "                        'ci_lower_col': 'CI_lower_g', 'ci_upper_col': 'CI_upper_g',\n",
        "                        'effect_label': \"Hedges' g\", 'effect_label_short': 'g',\n",
        "                        'has_fold_change': False, 'null_value': 0, 'scale': 'standardized', 'allows_negative': True\n",
        "                    },\n",
        "                    'cohen_d': {\n",
        "                        'effect_col': 'cohen_d', 'var_col': 'Vd', 'se_col': 'SE_d',\n",
        "                        'ci_lower_col': 'CI_lower_d', 'ci_upper_col': 'CI_upper_d',\n",
        "                        'effect_label': \"Cohen's d\", 'effect_label_short': 'd',\n",
        "                        'has_fold_change': False, 'null_value': 0, 'scale': 'standardized', 'allows_negative': True\n",
        "                    },\n",
        "                    'log_or': {\n",
        "                        'effect_col': 'log_OR', 'var_col': 'var_log_OR', 'se_col': 'SE_log_OR',\n",
        "                        'ci_lower_col': 'CI_lower_log_OR', 'ci_upper_col': 'CI_upper_log_OR',\n",
        "                        'effect_label': 'log Odds Ratio', 'effect_label_short': 'logOR',\n",
        "                        'has_fold_change': True, 'null_value': 0, 'scale': 'log', 'allows_negative': False\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                ANALYSIS_CONFIG['effect_size_type'] = sel\n",
        "                ANALYSIS_CONFIG['es_config'] = es_configs[sel]\n",
        "                ANALYSIS_CONFIG['data_type'] = 'raw'  # Store mode\n",
        "\n",
        "                # Pre-set global vars\n",
        "                ANALYSIS_CONFIG['effect_col'] = es_configs[sel]['effect_col']\n",
        "                ANALYSIS_CONFIG['var_col'] = es_configs[sel]['var_col']\n",
        "                ANALYSIS_CONFIG['se_col'] = es_configs[sel]['se_col']\n",
        "\n",
        "                print(f\"✓ Configuration saved. Please run the next cell to calculate values.\")\n",
        "\n",
        "        proceed_button.on_click(on_proceed)\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            effect_size_widget,\n",
        "            info_output,\n",
        "            widgets.HTML(\"<br>\"),\n",
        "            proceed_button,\n",
        "            proceed_output\n",
        "        ]))\n",
        "\n",
        "    # --- DISPLAY ---\n",
        "    display(tabs)\n",
        "\n",
        "# =============================================================================\n",
        "# MODE 2: PRE-CALCULATED - MANUAL SELECTION\n",
        "# =============================================================================\n",
        "else:\n",
        "    display(HTML(\"\"\"\n",
        "    <div style='background-color:#fff3cd; border-left: 5px solid #ffc107; padding: 15px; margin-bottom: 20px;'>\n",
        "        <h3 style='color:#856404; margin-top: 0;'>📊 Pre-calculated Effect Size Mode</h3>\n",
        "        <p style='color:#856404; margin-bottom: 0;'>Since you uploaded pre-calculated effect sizes,\n",
        "        we cannot analyze the raw data characteristics. Please manually select the type of effect size\n",
        "        you provided below.</p>\n",
        "    </div>\n",
        "    \"\"\"))\n",
        "\n",
        "    # Effect Size Type Descriptions\n",
        "    ES_TYPE_INFO = {\n",
        "        'hedges_g': {\n",
        "            'label': \"Hedges' g (Standardized Mean Difference)\",\n",
        "            'desc': \"Standardized mean difference corrected for small sample bias. Measures the difference between groups in standard deviation units. Use this for continuous outcomes measured on different scales.\"\n",
        "        },\n",
        "        'lnRR': {\n",
        "            'label': 'log Response Ratio (lnRR)',\n",
        "            'desc': 'The natural log of the ratio of means (ln(Treatment/Control)). Ideal for ratio-based data like fold-changes, growth rates, or proportional changes. Commonly used in ecology and biology.'\n",
        "        },\n",
        "        'log_or': {\n",
        "            'label': 'log Odds Ratio (logOR)',\n",
        "            'desc': 'The natural log of the odds ratio. Used exclusively for binary outcomes (Yes/No, Event/Non-event). NOT suitable for continuous measurements.'\n",
        "        },\n",
        "        'fisher_z': {\n",
        "            'label': \"Fisher's z (Correlation)\",\n",
        "            'desc': \"Fisher's z-transformation of correlation coefficients. Use this if your effect sizes are correlations (Pearson's r, Spearman's rho, etc.) that have been transformed.\"\n",
        "        },\n",
        "        'cohen_d': {\n",
        "            'label': \"Cohen's d (Uncorrected SMD)\",\n",
        "            'desc': \"Standardized mean difference without small sample correction. Similar to Hedges' g but may overestimate effects in small studies. Hedges' g is generally preferred.\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Dropdown for effect size type\n",
        "    es_type_dropdown = widgets.Dropdown(\n",
        "        options=[(v['label'], k) for k, v in ES_TYPE_INFO.items()],\n",
        "        value='hedges_g',\n",
        "        description='Effect Size Type:',\n",
        "        style={'description_width': '150px'},\n",
        "        layout=widgets.Layout(width='700px')\n",
        "    )\n",
        "\n",
        "    # Dynamic info panel\n",
        "    info_panel = widgets.Output()\n",
        "\n",
        "    def update_es_info(change):\n",
        "        with info_panel:\n",
        "            clear_output()\n",
        "            es_type = change['new']\n",
        "            info = ES_TYPE_INFO[es_type]\n",
        "            display(HTML(f\"\"\"\n",
        "            <div style='padding:15px; background:#f8f9fa; border:1px solid #dee2e6; border-radius:5px; margin-top:10px;'>\n",
        "                <b style='color:#2E86AB;'>📖 About {info['label']}:</b><br>\n",
        "                <p style='margin-top:5px; margin-bottom:0; color:#555; font-size:13px;'>{info['desc']}</p>\n",
        "            </div>\n",
        "            \"\"\"))\n",
        "\n",
        "    es_type_dropdown.observe(update_es_info, names='value')\n",
        "\n",
        "    # Initial display\n",
        "    with info_panel:\n",
        "        info = ES_TYPE_INFO['hedges_g']\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style='padding:15px; background:#f8f9fa; border:1px solid #dee2e6; border-radius:5px; margin-top:10px;'>\n",
        "            <b style='color:#2E86AB;'>📖 About {info['label']}:</b><br>\n",
        "            <p style='margin-top:5px; margin-bottom:0; color:#555; font-size:13px;'>{info['desc']}</p>\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "    # Confirm button\n",
        "    confirm_button = widgets.Button(\n",
        "        description='✓ Confirm Effect Size Type',\n",
        "        button_style='success',\n",
        "        layout=widgets.Layout(width='300px', height='40px', margin='20px 0 0 0'),\n",
        "        icon='check-circle'\n",
        "    )\n",
        "\n",
        "    confirm_output = widgets.Output()\n",
        "\n",
        "    def on_confirm_es_type(b):\n",
        "        with confirm_output:\n",
        "            clear_output()\n",
        "            sel = es_type_dropdown.value\n",
        "            print(f\"✓ Effect Size Type Confirmed: {ES_TYPE_INFO[sel]['label']}\")\n",
        "\n",
        "            # Configuration mapping\n",
        "            es_configs = {\n",
        "                'hedges_g': {\n",
        "                    'effect_col': 'hedges_g', 'var_col': 'Vg', 'se_col': 'SE_g',\n",
        "                    'ci_lower_col': 'CI_lower_g', 'ci_upper_col': 'CI_upper_g',\n",
        "                    'effect_label': \"Hedges' g\", 'effect_label_short': 'g',\n",
        "                    'has_fold_change': False, 'null_value': 0, 'scale': 'standardized', 'allows_negative': True\n",
        "                },\n",
        "                'lnRR': {\n",
        "                    'effect_col': 'lnRR', 'var_col': 'var_lnRR', 'se_col': 'SE_lnRR',\n",
        "                    'ci_lower_col': 'CI_lower_lnRR', 'ci_upper_col': 'CI_upper_lnRR',\n",
        "                    'effect_label': 'log Response Ratio', 'effect_label_short': 'lnRR',\n",
        "                    'has_fold_change': True, 'null_value': 0, 'scale': 'log', 'allows_negative': False\n",
        "                },\n",
        "                'log_or': {\n",
        "                    'effect_col': 'log_OR', 'var_col': 'var_log_OR', 'se_col': 'SE_log_OR',\n",
        "                    'ci_lower_col': 'CI_lower_log_OR', 'ci_upper_col': 'CI_upper_log_OR',\n",
        "                    'effect_label': 'log Odds Ratio', 'effect_label_short': 'logOR',\n",
        "                    'has_fold_change': True, 'null_value': 0, 'scale': 'log', 'allows_negative': False\n",
        "                },\n",
        "                'fisher_z': {\n",
        "                    'effect_col': 'fisher_z', 'var_col': 'var_fisher_z', 'se_col': 'SE_fisher_z',\n",
        "                    'ci_lower_col': 'CI_lower_z', 'ci_upper_col': 'CI_upper_z',\n",
        "                    'effect_label': \"Fisher's z\", 'effect_label_short': 'z',\n",
        "                    'has_fold_change': False, 'null_value': 0, 'scale': 'transformed', 'allows_negative': True\n",
        "                },\n",
        "                'cohen_d': {\n",
        "                    'effect_col': 'cohen_d', 'var_col': 'Vd', 'se_col': 'SE_d',\n",
        "                    'ci_lower_col': 'CI_lower_d', 'ci_upper_col': 'CI_upper_d',\n",
        "                    'effect_label': \"Cohen's d\", 'effect_label_short': 'd',\n",
        "                    'has_fold_change': False, 'null_value': 0, 'scale': 'standardized', 'allows_negative': True\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Initialize ANALYSIS_CONFIG if it doesn't exist (pre-calculated mode may skip Cell 3)\n",
        "            if 'ANALYSIS_CONFIG' not in globals():\n",
        "                global ANALYSIS_CONFIG\n",
        "                ANALYSIS_CONFIG = {\n",
        "                    'prefilter_col': 'None',\n",
        "                    'prefilter_values': [],\n",
        "                    'factor1': 'None',\n",
        "                    'factor2': 'None',\n",
        "                    'min_papers': 1,\n",
        "                    'min_obs': 1\n",
        "                }\n",
        "\n",
        "            ANALYSIS_CONFIG['effect_size_type'] = sel\n",
        "            ANALYSIS_CONFIG['es_config'] = es_configs[sel]\n",
        "            ANALYSIS_CONFIG['data_type'] = 'pre_calculated'  # Store mode\n",
        "            ANALYSIS_CONFIG['variance_type'] = globals().get('VARIANCE_TYPE_SELECTED', 'variance')\n",
        "\n",
        "            # Pre-set global vars\n",
        "            ANALYSIS_CONFIG['effect_col'] = es_configs[sel]['effect_col']\n",
        "            ANALYSIS_CONFIG['var_col'] = es_configs[sel]['var_col']\n",
        "            ANALYSIS_CONFIG['se_col'] = es_configs[sel]['se_col']\n",
        "\n",
        "            print(f\"✓ Configuration saved. Please run the next cell to prepare your data.\")\n",
        "\n",
        "    confirm_button.on_click(on_confirm_es_type)\n",
        "\n",
        "    # Display UI\n",
        "    display(widgets.VBox([\n",
        "        es_type_dropdown,\n",
        "        info_panel,\n",
        "        confirm_button,\n",
        "        confirm_output\n",
        "    ], layout=widgets.Layout(padding='10px')))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y_DSJBpuxpBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧮 CALCULATE EFFECT SIZES\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 5: EFFECT SIZE CALCULATION\n",
        "# Purpose: Calculate effect sizes (lnRR, g, d, logOR), variances, and weights.\n",
        "# Logic: Uses exact Gamma for Hedges' g point estimate, and large-sample\n",
        "#        approximation for variance (matching R 'metafor').\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from scipy.special import gamma\n",
        "\n",
        "# =============================================================================\n",
        "# VCV MATRIX CONSTRUCTION FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def build_vcv_matrices(df, effect_type, var_col_name):\n",
        "    \"\"\"\n",
        "    Constructs Variance-Covariance (VCV) matrices for studies with shared controls.\n",
        "\n",
        "    Logic:\n",
        "    - For each unique study ID:\n",
        "      - If NO shared controls: Create diagonal matrix from variances (vi)\n",
        "      - If HAS shared controls: Create full matrix with covariances\n",
        "\n",
        "    Formulas (Gleser & Olkin, 2009):\n",
        "    - lnRR: Cov = SD_c^2 / (N_c * Mean_c^2)\n",
        "    - Hedges' g / SMD: Cov ≈ 1 / N_c (simplified approximation)\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Dataframe with columns: id, shared_group_id, nc, xc, sdc, and variance column\n",
        "    effect_type : str\n",
        "        Type of effect size ('lnRR', 'hedges_g', 'cohen_d', 'log_or')\n",
        "    var_col_name : str\n",
        "        Name of the variance column (e.g., 'Vg', 'var_lnRR', 'Vd')\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    dict\n",
        "        Dictionary where Key = Study ID, Value = Numpy Matrix\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    vcv_matrices = {}\n",
        "\n",
        "    # Required columns check\n",
        "    if var_col_name not in df.columns or 'id' not in df.columns:\n",
        "        print(f\"⚠️ Warning: Missing required columns for VCV construction ({var_col_name}, id)\")\n",
        "        return vcv_matrices\n",
        "\n",
        "    # Group by study ID\n",
        "    for study_id, study_group in df.groupby('id'):\n",
        "        k = len(study_group)  # Number of effect sizes in this study\n",
        "\n",
        "        # Create k x k matrix\n",
        "        vcv = np.zeros((k, k))\n",
        "\n",
        "        # Get indices for this study in the dataframe\n",
        "        indices = study_group.index.tolist()\n",
        "\n",
        "        # Fill diagonal with variances\n",
        "        for i in range(k):\n",
        "            vcv[i, i] = study_group.iloc[i][var_col_name]\n",
        "\n",
        "        # Check if study has shared controls\n",
        "        if 'shared_group_id' in study_group.columns:\n",
        "            shared_groups = study_group[study_group['shared_group_id'].notna()].groupby('shared_group_id')\n",
        "\n",
        "            for shared_id, shared_rows in shared_groups:\n",
        "                # Get positions of rows sharing this control\n",
        "                shared_positions = [indices.index(idx) for idx in shared_rows.index]\n",
        "\n",
        "                # Calculate covariance for this shared control group\n",
        "                if len(shared_positions) >= 2:\n",
        "                    # Get control group statistics from first row (they should be identical)\n",
        "                    nc = shared_rows.iloc[0]['nc']\n",
        "\n",
        "                    if effect_type == 'lnRR':\n",
        "                        # For lnRR: Cov = SD_c^2 / (N_c * Mean_c^2)\n",
        "                        xc = shared_rows.iloc[0]['xc']\n",
        "\n",
        "                        # Use imputed SD if available, otherwise use original\n",
        "                        if 'sdc_imputed' in shared_rows.columns:\n",
        "                            sdc = shared_rows.iloc[0]['sdc_imputed']\n",
        "                        else:\n",
        "                            sdc = shared_rows.iloc[0]['sdc'] if 'sdc' in shared_rows.columns else 0\n",
        "\n",
        "                        if nc > 0 and xc != 0:\n",
        "                            cov = (sdc ** 2) / (nc * (xc ** 2))\n",
        "                        else:\n",
        "                            cov = 0\n",
        "\n",
        "                    elif effect_type in ['hedges_g', 'cohen_d']:\n",
        "                        # For SMD: Cov ≈ 1 / N_c (simplified)\n",
        "                        # More precise: Cov = 1/N_c + d1*d2/(2*N_total)\n",
        "                        # Using simplified version for now\n",
        "                        if nc > 0:\n",
        "                            cov = 1 / nc\n",
        "                        else:\n",
        "                            cov = 0\n",
        "\n",
        "                    else:\n",
        "                        # For other effect types, use simplified covariance\n",
        "                        if nc > 0:\n",
        "                            cov = 1 / nc\n",
        "                        else:\n",
        "                            cov = 0\n",
        "\n",
        "                    # Fill off-diagonal elements for this shared control group\n",
        "                    for i in shared_positions:\n",
        "                        for j in shared_positions:\n",
        "                            if i != j:\n",
        "                                vcv[i, j] = cov\n",
        "\n",
        "        # Store the matrix\n",
        "        vcv_matrices[study_id] = vcv\n",
        "\n",
        "    return vcv_matrices\n",
        "\n",
        "\n",
        "# --- 1. SETUP TABS ---\n",
        "tab_summary = widgets.Output()\n",
        "tab_diag = widgets.Output()\n",
        "tab_stats = widgets.Output()\n",
        "tab_interp = widgets.Output()\n",
        "tab_removed = widgets.Output()\n",
        "\n",
        "tabs = widgets.Tab(children=[tab_summary, tab_diag, tab_stats, tab_interp, tab_removed]) # <--- Added here\n",
        "tabs.set_title(0, '📊 Summary')\n",
        "tabs.set_title(1, '📉 Diagnostics')\n",
        "tabs.set_title(2, '📏 Detailed Stats')\n",
        "tabs.set_title(3, '🧠 Interpretation')\n",
        "tabs.set_title(4, '🗑️ Removed Data')\n",
        "\n",
        "# --- 2. CALCULATION ENGINE ---\n",
        "def run_calculation():\n",
        "    # Logs for different tabs\n",
        "    log_diag = []\n",
        "    log_summary = []\n",
        "\n",
        "    # --- CONFIG CHECK ---\n",
        "    try:\n",
        "        if 'ANALYSIS_CONFIG' not in globals():\n",
        "             print(\"❌ ERROR: ANALYSIS_CONFIG not found. Run previous config cell first.\")\n",
        "             return\n",
        "        effect_size_type = ANALYSIS_CONFIG['effect_size_type']\n",
        "        es_config = ANALYSIS_CONFIG['es_config']\n",
        "        log_summary.append(f\"Configuration: {es_config['effect_label']} ({es_config['effect_label_short']})\")\n",
        "    except KeyError as e:\n",
        "        print(f\"❌ ERROR: Configuration keys missing: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- DATA LOADING ---\n",
        "    if 'data_filtered' not in globals():\n",
        "        print(\"❌ ERROR: Data not found. Run filtering cell first.\")\n",
        "        return\n",
        "\n",
        "    df = data_filtered.copy()\n",
        "    initial_obs = len(df)\n",
        "    initial_papers = df['id'].nunique()\n",
        "\n",
        "    # --- VALIDATION ---\n",
        "    req_cols = ['xe', 'sde', 'ne', 'xc', 'sdc', 'nc']\n",
        "    missing = [c for c in req_cols if c not in df.columns]\n",
        "    if missing:\n",
        "        print(f\"❌ ERROR: Missing columns: {missing}\")\n",
        "        return\n",
        "\n",
        "    # --- IMPUTATION (SD) ---\n",
        "    log_diag.append(\"<b>1. Standard Deviation Imputation</b>\")\n",
        "\n",
        "    # Handle zeros in SD\n",
        "    if 'sde' in df.columns: df['sde'] = df['sde'].replace(0, np.nan)\n",
        "    if 'sdc' in df.columns: df['sdc'] = df['sdc'].replace(0, np.nan)\n",
        "\n",
        "    # Calculate CV\n",
        "    valid_e = (df['sde'] > 0) & (df['xe'] > 0)\n",
        "    valid_c = (df['sdc'] > 0) & (df['xc'] > 0)\n",
        "\n",
        "    # Calculate median CV for imputation (use default 0.1 if no valid data)\n",
        "    cv_e = (df.loc[valid_e, 'sde'] / df.loc[valid_e, 'xe']).median() if valid_e.any() else 0.1\n",
        "    cv_c = (df.loc[valid_c, 'sdc'] / df.loc[valid_c, 'xc']).median() if valid_c.any() else 0.1\n",
        "\n",
        "    df['sde_imputed'] = df['sde'].fillna(df['xe'] * cv_e)\n",
        "    df['sdc_imputed'] = df['sdc'].fillna(df['xc'] * cv_c)\n",
        "\n",
        "    n_imp_e = df['sde'].isna().sum()\n",
        "    n_imp_c = df['sdc'].isna().sum()\n",
        "\n",
        "    log_diag.append(f\"• Imputed {n_imp_e} Exp SDs (using CV={cv_e:.4f})\")\n",
        "    log_diag.append(f\"• Imputed {n_imp_c} Ctl SDs (using CV={cv_c:.4f})\")\n",
        "\n",
        "    # --- CLEANING (Negative/Zero) ---\n",
        "    log_diag.append(\"<br><b>2. Data Cleaning</b>\")\n",
        "\n",
        "    #List to store removed rows\n",
        "    removed_records = []\n",
        "\n",
        "    if effect_size_type in ['lnRR', 'log_or']:\n",
        "        # Remove negatives\n",
        "        neg_mask = (df['xe'] < 0) | (df['xc'] < 0)\n",
        "        n_neg = neg_mask.sum()\n",
        "        if n_neg > 0:\n",
        "            df = df[~neg_mask]\n",
        "            log_diag.append(f\"• Removed {n_neg} rows with negative values (invalid for {effect_size_type})\")\n",
        "\n",
        "        # Handle zeros\n",
        "        zero_mask = (df['xe'] == 0) | (df['xc'] == 0)\n",
        "        n_zero = zero_mask.sum()\n",
        "        if n_zero > 0:\n",
        "            df.loc[zero_mask, ['xe', 'xc']] += 0.001\n",
        "            log_diag.append(f\"• Adjusted {n_zero} rows with zero values (added 0.001)\")\n",
        "\n",
        "    # --- CALCULATION ---\n",
        "    # Initialize dynamic column names to avoid KeyErrors later\n",
        "    effect_col = es_config['effect_col']\n",
        "    var_col = es_config['var_col']\n",
        "    se_col = es_config['se_col']\n",
        "\n",
        "    if effect_size_type == 'lnRR':\n",
        "        df[effect_col] = np.log(df['xe'] / df['xc'])\n",
        "        df[var_col] = (df['sde_imputed']**2 / (df['ne']*df['xe']**2)) + (df['sdc_imputed']**2 / (df['nc']*df['xc']**2))\n",
        "        df[se_col] = np.sqrt(df[var_col])\n",
        "\n",
        "        # Fold Change for interpretation\n",
        "        df['Response_Ratio'] = np.exp(df[effect_col])\n",
        "        df['fold_change'] = df.apply(lambda r: r['Response_Ratio'] if r[effect_col]>=0 else -1/r['Response_Ratio'], axis=1)\n",
        "        df['Percent_Change'] = (df['Response_Ratio'] - 1) * 100\n",
        "\n",
        "    elif effect_size_type == 'hedges_g':\n",
        "        # 1. Calculate Cohen's d\n",
        "        df['df'] = df['ne'] + df['nc'] - 2\n",
        "        df['sp'] = np.sqrt(((df['ne']-1)*df['sde_imputed']**2 + (df['nc']-1)*df['sdc_imputed']**2) / df['df'])\n",
        "        df['d'] = (df['xe'] - df['xc']) / df['sp']\n",
        "\n",
        "        # 2. Calculate Correction Factor (J) - Exact Gamma Method\n",
        "        m = df['df']\n",
        "        df['J'] = gamma(m/2) / (np.sqrt(m/2) * gamma((m-1)/2))\n",
        "\n",
        "        # 3. Calculate Hedges' g\n",
        "        df[effect_col] = df['d'] * df['J']\n",
        "\n",
        "        # 4. Calculate Variance (Large Sample Approximation)\n",
        "        # Matches R (metafor) and Borenstein et al.\n",
        "        df[var_col] = (1/df['ne']) + (1/df['nc']) + (df[effect_col]**2 / (2*(df['ne'] + df['nc'])))\n",
        "        df[se_col] = np.sqrt(df[var_col])\n",
        "\n",
        "    elif effect_size_type == 'cohen_d':\n",
        "        df['df'] = df['ne'] + df['nc'] - 2\n",
        "        df['sp'] = np.sqrt(((df['ne']-1)*df['sde_imputed']**2 + (df['nc']-1)*df['sdc_imputed']**2) / df['df'])\n",
        "        df[effect_col] = (df['xe'] - df['xc']) / df['sp']\n",
        "        df[var_col] = (df['ne']+df['nc']) / (df['ne']*df['nc']) + (df[effect_col]**2)/(2*(df['ne']+df['nc']))\n",
        "        df[se_col] = np.sqrt(df[var_col])\n",
        "\n",
        "    elif effect_size_type == 'log_or':\n",
        "        # Simplified logOR\n",
        "        df[effect_col] = np.log(df['xe'] / df['xc'])\n",
        "        df[var_col] = (1/df['xe'] + 1/df['ne'] + 1/df['xc'] + 1/df['nc'])\n",
        "        df[se_col] = np.sqrt(df[var_col])\n",
        "\n",
        "    # --- CI & WEIGHTS ---\n",
        "    ci_lower_col = es_config.get('ci_lower_col', f\"CI_lower_{es_config['effect_label_short']}\")\n",
        "    ci_upper_col = es_config.get('ci_upper_col', f\"CI_upper_{es_config['effect_label_short']}\")\n",
        "\n",
        "    df[ci_lower_col] = df[effect_col] - 1.96 * df[se_col]\n",
        "    df[ci_upper_col] = df[effect_col] + 1.96 * df[se_col]\n",
        "    df['w_fixed'] = 1 / df[var_col]\n",
        "\n",
        "    # --- FINAL CLEANING ---\n",
        "    #df = df.dropna(subset=[effect_col, var_col]).copy()\n",
        "    #df = df[df[var_col] > 0].copy()\n",
        "\n",
        "    # 1. Check for NaN results\n",
        "    mask_nan = df[[effect_col, var_col]].isna().any(axis=1)\n",
        "    if mask_nan.sum() > 0:\n",
        "        dropped_nan = df[mask_nan].copy()\n",
        "        dropped_nan['Reason'] = 'Calculation Failed (Missing Data/NaN)'\n",
        "        removed_records.append(dropped_nan)\n",
        "        df = df[~mask_nan].copy()\n",
        "\n",
        "    # 2. Check for Zero Variance\n",
        "    mask_zero_var = df[var_col] <= 0\n",
        "    if mask_zero_var.sum() > 0:\n",
        "        dropped_zero = df[mask_zero_var].copy()\n",
        "        dropped_zero['Reason'] = 'Zero or Negative Variance'\n",
        "        removed_records.append(dropped_zero)\n",
        "        df = df[~mask_zero_var].copy()\n",
        "\n",
        "    # --- UPDATE CONFIG ---\n",
        "    ANALYSIS_CONFIG['analysis_data'] = df\n",
        "    ANALYSIS_CONFIG['effect_col'] = effect_col\n",
        "    ANALYSIS_CONFIG['var_col'] = var_col\n",
        "    ANALYSIS_CONFIG['se_col'] = se_col\n",
        "    ANALYSIS_CONFIG['ci_lower_col'] = ci_lower_col\n",
        "    ANALYSIS_CONFIG['ci_upper_col'] = ci_upper_col\n",
        "\n",
        "\n",
        "    # --- BUILD VCV MATRICES ---\n",
        "    # Construct variance-covariance matrices for studies with shared controls\n",
        "    vcv_matrices = build_vcv_matrices(df, effect_size_type, var_col)\n",
        "    ANALYSIS_CONFIG['vcv_matrices'] = vcv_matrices\n",
        "\n",
        "    # Log the number of studies with shared controls\n",
        "    n_studies_with_vcv = len([k for k, v in vcv_matrices.items() if v.shape[0] > 1 or (v.shape[0] > 0 and 'shared_group_id' in df.columns and df[df['id']==k]['shared_group_id'].notna().any())])\n",
        "    if n_studies_with_vcv > 0:\n",
        "        log_diag.append(f\"<br><b>3. VCV Matrices</b>\")\n",
        "        log_diag.append(f\"• Built {len(vcv_matrices)} VCV matrices ({n_studies_with_vcv} studies with shared controls)\")\n",
        "\n",
        "    # --- POPULATE TABS ---\n",
        "\n",
        "# 1. SUMMARY TAB (ENHANCED)\n",
        "    with tab_summary:\n",
        "        clear_output()\n",
        "        final_n = len(df)\n",
        "        final_papers = df['id'].nunique()\n",
        "\n",
        "        # --- 1. KPI Cards ---\n",
        "        html_sum = f\"\"\"\n",
        "        <div style='display:flex; gap:20px; margin-bottom:20px;'>\n",
        "            <div style='background:#e8f5e9; padding:15px; border-radius:8px; flex:1; text-align:center; border:1px solid #c3e6cb'>\n",
        "                <h2 style='margin:0; color:#2e7d32;'>{final_n}</h2>\n",
        "                <p style='margin:0; color:#1b5e20; font-size:12px; text-transform:uppercase; letter-spacing:1px;'>Observations</p>\n",
        "            </div>\n",
        "            <div style='background:#e3f2fd; padding:15px; border-radius:8px; flex:1; text-align:center; border:1px solid #bbdefb'>\n",
        "                <h2 style='margin:0; color:#1565c0;'>{final_papers}</h2>\n",
        "                <p style='margin:0; color:#0d47a1; font-size:12px; text-transform:uppercase; letter-spacing:1px;'>Studies</p>\n",
        "            </div>\n",
        "            <div style='background:#fff3e0; padding:15px; border-radius:8px; flex:1; text-align:center; border:1px solid #ffe0b2'>\n",
        "                <h2 style='margin:0; color:#e65100;'>{initial_obs - final_n}</h2>\n",
        "                <p style='margin:0; color:#bf360c; font-size:12px; text-transform:uppercase; letter-spacing:1px;'>Removed</p>\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(html_sum))\n",
        "\n",
        "        if not df.empty:\n",
        "            # --- 2. Statistics Table ---\n",
        "            desc = df[effect_col].describe()\n",
        "            stats_html = f\"\"\"\n",
        "            <table style='width:100%; border-collapse:collapse; margin-bottom:20px;'>\n",
        "                <tr style='border-bottom:2px solid #eee;'><th style='text-align:left; padding:8px; color:#555;'>Statistic</th><th style='text-align:right; padding:8px; color:#555;'>Value</th></tr>\n",
        "                <tr><td style='padding:8px;'><b>Mean Effect:</b></td><td style='text-align:right; padding:8px;'>{desc['mean']:.4f}</td></tr>\n",
        "                <tr style='background-color:#f9f9f9;'><td style='padding:8px;'><b>Median:</b></td><td style='text-align:right; padding:8px;'>{desc['50%']:.4f}</td></tr>\n",
        "                <tr><td style='padding:8px;'><b>Min / Max:</b></td><td style='text-align:right; padding:8px;'>{desc['min']:.4f} / {desc['max']:.4f}</td></tr>\n",
        "                <tr style='background-color:#f9f9f9;'><td style='padding:8px;'><b>Standard Deviation:</b></td><td style='text-align:right; padding:8px;'>{desc['std']:.4f}</td></tr>\n",
        "            </table>\n",
        "            \"\"\"\n",
        "            display(HTML(stats_html))\n",
        "\n",
        "            # --- 3. Distribution Plot (Histogram) ---\n",
        "            display(HTML(\"<h4 style='color:#2E86AB; margin:0 0 10px 0;'>📊 Distribution of Effect Sizes</h4>\"))\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(8, 3.5))\n",
        "\n",
        "            # Plot Histogram\n",
        "            counts, bins, patches = ax.hist(df[effect_col], bins='auto', color='#2E86AB', alpha=0.7, rwidth=0.9, edgecolor='black', linewidth=0.5)\n",
        "\n",
        "            # Add Mean Line\n",
        "            ax.axvline(desc['mean'], color='#e74c3c', linestyle='--', linewidth=2, label=f\"Mean: {desc['mean']:.2f}\")\n",
        "            ax.axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
        "\n",
        "            # Styling\n",
        "            ax.set_xlabel(es_config['effect_label'], fontsize=10, fontweight='bold')\n",
        "            ax.set_ylabel(\"Frequency\", fontsize=10)\n",
        "            ax.legend(frameon=False)\n",
        "            ax.grid(axis='y', linestyle=':', alpha=0.3)\n",
        "\n",
        "            # Remove top/right spines for cleaner look\n",
        "            ax.spines['top'].set_visible(False)\n",
        "            ax.spines['right'].set_visible(False)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "            print(\"⚠️ No valid data remaining.\")\n",
        "\n",
        "# 2. DIAGNOSTICS TAB\n",
        "    with tab_diag:\n",
        "        clear_output()\n",
        "\n",
        "        # --- Processing Log ---\n",
        "        display(HTML(\"<h4 style='color:#2E86AB; margin-bottom:5px;'>🔍 Processing Log</h4>\"))\n",
        "        for line in log_diag:\n",
        "            display(HTML(f\"<div style='margin-left:15px; font-size:13px;'>{line}</div>\"))\n",
        "\n",
        "        # --- Outlier Check ---\n",
        "        if not df.empty:\n",
        "            # Calculate IQR\n",
        "            q1, q3 = df[effect_col].quantile([0.25, 0.75])\n",
        "            iqr = q3 - q1\n",
        "            lower_fence = q1 - 1.5 * iqr\n",
        "            upper_fence = q3 + 1.5 * iqr\n",
        "\n",
        "            # Identify Outliers\n",
        "            outliers = df[(df[effect_col] < lower_fence) | (df[effect_col] > upper_fence)].copy()\n",
        "\n",
        "            display(HTML(\"<hr>\"))\n",
        "            display(HTML(\"<h4 style='color:#2E86AB; margin-bottom:10px;'>⚠️ Outlier Analysis (IQR Method)</h4>\"))\n",
        "\n",
        "            if len(outliers) > 0:\n",
        "                # 1. Explanation Box\n",
        "                outlier_msg = f\"\"\"\n",
        "                <div style='background-color:#fff3cd; border-left: 5px solid #ffc107; padding:15px; margin-bottom:15px; color:#856404;'>\n",
        "                    <b>⚠️ Found {len(outliers)} Potential Outliers</b><br>\n",
        "                    <div style='font-size:13px; margin-top:5px;'>\n",
        "                    These observations fall outside the standard statistical range (1.5 × IQR).<br>\n",
        "                    • <b>Acceptable Range:</b> {lower_fence:.3f} to {upper_fence:.3f}<br>\n",
        "                    • <b>Extreme Values:</b> {outliers[effect_col].min():.3f} to {outliers[effect_col].max():.3f}<br><br>\n",
        "                    <i><b>Action:</b> Check the table below. If these are typos (e.g., 100 instead of 10), fix your source file. If they are real biological variation, keep them.</i>\n",
        "                    </div>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "                display(HTML(outlier_msg))\n",
        "\n",
        "                # 2. Display Table (Select useful columns)\n",
        "                cols_to_show = ['id', effect_col, 'xe', 'xc', 'ne', 'nc']\n",
        "                # Add 'year' if it exists for context\n",
        "                if 'year' in outliers.columns:\n",
        "                    cols_to_show.insert(1, 'year')\n",
        "\n",
        "                # Filter list to only columns that actually exist\n",
        "                final_cols = [c for c in cols_to_show if c in outliers.columns]\n",
        "\n",
        "                # Sort by effect size (most extreme first)\n",
        "                display(outliers[final_cols].sort_values(by=effect_col, key=abs, ascending=False))\n",
        "\n",
        "            else:\n",
        "                # Success Message\n",
        "                display(HTML(f\"\"\"\n",
        "                <div style='padding:15px; background:#d4edda; border-radius:4px; color:#155724; border:1px solid #c3e6cb;'>\n",
        "                    <b>✓ No statistical outliers detected.</b><br>\n",
        "                    All {len(df)} effect sizes fall within the expected statistical range [{lower_fence:.3f}, {upper_fence:.3f}].\n",
        "                </div>\n",
        "                \"\"\"))\n",
        "\n",
        "# 3. DETAILED STATS TAB (WITH INTERPRETATION GUIDE)\n",
        "    with tab_stats:\n",
        "        clear_output()\n",
        "        if not df.empty:\n",
        "            # --- 1. Calculate Statistics ---\n",
        "            es_data = df[effect_col]\n",
        "            skew = es_data.skew()\n",
        "            kurt = es_data.kurt()\n",
        "\n",
        "            # Normality Test (Shapiro-Wilk)\n",
        "            if len(df) < 5000:\n",
        "                shapiro_stat, shapiro_p = stats.shapiro(es_data)\n",
        "                normality_str = f\"W = {shapiro_stat:.3f}, p = {shapiro_p:.3f}\"\n",
        "                is_normal = shapiro_p > 0.05\n",
        "            else:\n",
        "                normality_str = \"N > 5000 (Test omitted)\"\n",
        "                is_normal = True # Assume normal for large N\n",
        "\n",
        "            norm_verdict = \"Likely Normal\" if is_normal else \"Deviates from Normal\"\n",
        "            norm_color = \"#28a745\" if is_normal else \"#dc3545\"\n",
        "\n",
        "            # --- 2. Statistics Table ---\n",
        "            stats_html = f\"\"\"\n",
        "            <h4 style='color:#2E86AB;'>📏 Descriptive Statistics</h4>\n",
        "            <table style='width:100%; border-collapse:collapse; margin-bottom:20px; font-size:13px;'>\n",
        "                <tr style='background-color:#f1f3f5; border-bottom:2px solid #dee2e6;'>\n",
        "                    <th style='padding:8px; text-align:left;'>Metric</th>\n",
        "                    <th style='padding:8px; text-align:right;'>Effect Size</th>\n",
        "                    <th style='padding:8px; text-align:right;'>Standard Error</th>\n",
        "                    <th style='padding:8px; text-align:right;'>Variance</th>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td style='padding:8px; border-bottom:1px solid #eee;'><b>Count (N)</b></td>\n",
        "                    <td style='padding:8px; text-align:right;'>{es_data.count()}</td>\n",
        "                    <td style='padding:8px; text-align:right;'>{df[se_col].count()}</td>\n",
        "                    <td style='padding:8px; text-align:right;'>{df[var_col].count()}</td>\n",
        "                </tr>\n",
        "                <tr style='background-color:#f9f9f9;'>\n",
        "                    <td style='padding:8px; border-bottom:1px solid #eee;'><b>Mean</b></td>\n",
        "                    <td style='padding:8px; text-align:right;'>{es_data.mean():.4f}</td>\n",
        "                    <td style='padding:8px; text-align:right;'>{df[se_col].mean():.4f}</td>\n",
        "                    <td style='padding:8px; text-align:right;'>{df[var_col].mean():.4f}</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td style='padding:8px; border-bottom:1px solid #eee;'><b>Median</b></td>\n",
        "                    <td style='padding:8px; text-align:right;'>{es_data.median():.4f}</td>\n",
        "                    <td style='padding:8px; text-align:right;'>{df[se_col].median():.4f}</td>\n",
        "                    <td style='padding:8px; text-align:right;'>{df[var_col].median():.4f}</td>\n",
        "                </tr>\n",
        "                <tr style='background-color:#f9f9f9;'>\n",
        "                    <td style='padding:8px; border-bottom:1px solid #eee;'><b>Std. Dev.</b></td>\n",
        "                    <td style='padding:8px; text-align:right;'>{es_data.std():.4f}</td>\n",
        "                    <td style='padding:8px; text-align:right;'>{df[se_col].std():.4f}</td>\n",
        "                    <td style='padding:8px; text-align:right;'>{df[var_col].std():.4f}</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td style='padding:8px; border-bottom:1px solid #eee;'><b>Skewness</b></td>\n",
        "                    <td style='padding:8px; text-align:right;'>{skew:.3f}</td>\n",
        "                    <td style='padding:8px; text-align:right;'>-</td>\n",
        "                    <td style='padding:8px; text-align:right;'>-</td>\n",
        "                </tr>\n",
        "                <tr style='background-color:#f9f9f9;'>\n",
        "                    <td style='padding:8px; border-bottom:1px solid #eee;'><b>Kurtosis</b></td>\n",
        "                    <td style='padding:8px; text-align:right;'>{kurt:.3f}</td>\n",
        "                    <td style='padding:8px; text-align:right;'>-</td>\n",
        "                    <td style='padding:8px; text-align:right;'>-</td>\n",
        "                </tr>\n",
        "            </table>\n",
        "            \"\"\"\n",
        "            display(HTML(stats_html))\n",
        "\n",
        "            # --- 3. Normality Assessment Card ---\n",
        "            norm_html = f\"\"\"\n",
        "            <div style='display:flex; align-items:center; background-color:#fff; border:1px solid #ddd; border-left:5px solid {norm_color}; padding:15px; border-radius:5px; margin-bottom:20px;'>\n",
        "                <div style='flex:1;'>\n",
        "                    <h5 style='margin:0; color:#555;'>Normality Check (Shapiro-Wilk)</h5>\n",
        "                    <div style='font-size:14px; margin-top:5px;'>{normality_str}</div>\n",
        "                </div>\n",
        "                <div style='font-weight:bold; color:{norm_color}; font-size:16px;'>\n",
        "                    {norm_verdict}\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            display(HTML(norm_html))\n",
        "\n",
        "            # --- 4. Guidance Logic ---\n",
        "            advice_items = []\n",
        "            if not is_normal and len(df) < 30:\n",
        "                advice_items.append(\"<li><b>Small, non-normal sample:</b> Random-Effects models may be unstable. Check for outliers in the 'Diagnostics' tab.</li>\")\n",
        "            if abs(skew) > 1:\n",
        "                advice_items.append(\"<li><b>High Skewness:</b> The data is not symmetric. This often indicates a few strong outliers or a natural limit (e.g., values can't be negative).</li>\")\n",
        "            if abs(kurt) > 3:\n",
        "                advice_items.append(\"<li><b>High Kurtosis:</b> The data has 'heavy tails' (more extreme values than expected).</li>\")\n",
        "\n",
        "            if not advice_items:\n",
        "                advice_items.append(\"<li><b>Data looks good!</b> The distribution satisfies standard meta-analysis assumptions.</li>\")\n",
        "\n",
        "            advice_html = f\"\"\"\n",
        "            <div style='background-color:#e3f2fd; padding:15px; border-radius:5px; margin-bottom:20px;'>\n",
        "                <b style='color:#0d47a1;'>💡 What should I do?</b>\n",
        "                <ul style='margin-top:5px; margin-bottom:0; color:#0d47a1; padding-left:20px;'>\n",
        "                    {''.join(advice_items)}\n",
        "                </ul>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            display(HTML(advice_html))\n",
        "\n",
        "            # --- 5. Q-Q Plot ---\n",
        "            display(HTML(\"<h4 style='color:#2E86AB; margin:0 0 10px 0;'>📈 Q-Q Plot (Normality Check)</h4>\"))\n",
        "            fig, ax = plt.subplots(figsize=(6, 4))\n",
        "            stats.probplot(es_data, dist=\"norm\", plot=ax)\n",
        "            ax.get_lines()[0].set_marker('o')\n",
        "            ax.get_lines()[0].set_markersize(5.0)\n",
        "            ax.get_lines()[0].set_markerfacecolor('#4a90e2')\n",
        "            ax.get_lines()[0].set_markeredgecolor('white')\n",
        "            ax.get_lines()[1].set_linewidth(2.0)\n",
        "            ax.get_lines()[1].set_color('#e74c3c')\n",
        "            ax.set_title(\"\")\n",
        "            ax.set_xlabel(\"Theoretical Quantiles\", fontsize=10)\n",
        "            ax.set_ylabel(\"Ordered Values (Effect Sizes)\", fontsize=10)\n",
        "            ax.grid(True, linestyle=':', alpha=0.3)\n",
        "            sns.despine()\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # --- 6. Data Preview ---\n",
        "            display(HTML(\"<h4 style='color:#2E86AB; margin-top:20px;'>📋 Data Preview</h4>\"))\n",
        "            cols_show = ['id', 'xe', 'xc', 'ne', 'nc', effect_col, se_col]\n",
        "            cols_exist = [c for c in cols_show if c in df.columns]\n",
        "            display(df[cols_exist].head(5))\n",
        "\n",
        "        else:\n",
        "            print(\"⚠️ No valid data remaining.\")\n",
        "\n",
        "# 4. INTERPRETATION TAB (ENHANCED)\n",
        "    with tab_interp:\n",
        "        clear_output()\n",
        "        if not df.empty:\n",
        "            # --- 1. Logic: Categorize Studies ---\n",
        "            null_val = es_config.get('null_value', 0)\n",
        "\n",
        "            # A. Significance Classification\n",
        "            sig_pos = ((df[ci_lower_col] > null_val)).sum()\n",
        "            sig_neg = ((df[ci_upper_col] < null_val)).sum()\n",
        "            non_sig = len(df) - sig_pos - sig_neg\n",
        "\n",
        "            # B. Magnitude Classification (Cohen's Benchmarks)\n",
        "            # We use absolute values to measure \"strength\" regardless of direction\n",
        "            abs_eff = df[effect_col].abs()\n",
        "\n",
        "            if effect_size_type in ['hedges_g', 'cohen_d']:\n",
        "                # Standard benchmarks for SMD\n",
        "                bins = [-1, 0.2, 0.5, 0.8, 999]\n",
        "                labels = ['Negligible (<0.2)', 'Small (0.2-0.5)', 'Medium (0.5-0.8)', 'Large (>0.8)']\n",
        "            else:\n",
        "                # Benchmarks for lnRR (approximate)\n",
        "                bins = [-1, 0.1, 0.3, 0.5, 999]\n",
        "                labels = ['Negligible (<0.1)', 'Small (0.1-0.3)', 'Medium (0.3-0.5)', 'Large (>0.5)']\n",
        "\n",
        "            mag_counts = pd.cut(abs_eff, bins=bins, labels=labels).value_counts().sort_index()\n",
        "\n",
        "            # --- 2. HTML Headline Cards ---\n",
        "            # Determine dominant direction\n",
        "            if sig_pos > sig_neg:\n",
        "                direction_text = \"Favors Treatment\"\n",
        "                dir_color = \"#28a745\" # Green\n",
        "            elif sig_neg > sig_pos:\n",
        "                direction_text = \"Favors Control\"\n",
        "                dir_color = \"#dc3545\" # Red\n",
        "            else:\n",
        "                direction_text = \"Ambiguous / Balanced\"\n",
        "                dir_color = \"#6c757d\" # Gray\n",
        "\n",
        "            html_cards = f\"\"\"\n",
        "            <div style='display:flex; gap:20px; margin-bottom:20px;'>\n",
        "                <div style='flex:1; background:#f8f9fa; padding:15px; border-radius:8px; border-top: 5px solid {dir_color}; box-shadow: 0 2px 4px rgba(0,0,0,0.05);'>\n",
        "                    <h4 style='margin:0; color:#555; font-size:12px; text-transform:uppercase;'>Direction</h4>\n",
        "                    <div style='font-size:20px; font-weight:bold; color:{dir_color}; margin-top:5px;'>{direction_text}</div>\n",
        "                    <div style='font-size:12px; color:#777;'>Based on significant studies</div>\n",
        "                </div>\n",
        "                <div style='flex:1; background:#f8f9fa; padding:15px; border-radius:8px; border-top: 5px solid #17a2b8; box-shadow: 0 2px 4px rgba(0,0,0,0.05);'>\n",
        "                    <h4 style='margin:0; color:#555; font-size:12px; text-transform:uppercase;'>Significance Rate</h4>\n",
        "                    <div style='font-size:20px; font-weight:bold; color:#17a2b8; margin-top:5px;'>{((sig_pos+sig_neg)/len(df))*100:.1f}%</div>\n",
        "                    <div style='font-size:12px; color:#777;'>Of studies show a clear effect</div>\n",
        "                </div>\n",
        "                <div style='flex:1; background:#f8f9fa; padding:15px; border-radius:8px; border-top: 5px solid #6610f2; box-shadow: 0 2px 4px rgba(0,0,0,0.05);'>\n",
        "                    <h4 style='margin:0; color:#555; font-size:12px; text-transform:uppercase;'>Dominant Magnitude</h4>\n",
        "                    <div style='font-size:20px; font-weight:bold; color:#6610f2; margin-top:5px;'>{mag_counts.idxmax().split(' ')[0]}</div>\n",
        "                    <div style='font-size:12px; color:#777;'>Most common effect size strength</div>\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            display(HTML(html_cards))\n",
        "\n",
        "            # --- 3. Visualizations (Pie + Bar) ---\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4.5))\n",
        "\n",
        "            # Chart A: Significance Pie (Vote Counting)\n",
        "            sizes = [sig_pos, sig_neg, non_sig]\n",
        "            pie_labels = ['Sig. Positive', 'Sig. Negative', 'Non-Significant']\n",
        "            colors = ['#28a745', '#dc3545', '#e2e6ea'] # Green, Red, Gray\n",
        "            explode = (0.05, 0.05, 0)\n",
        "\n",
        "            # Filter out zeros for cleaner chart\n",
        "            pie_data = [(s, l, c, e) for s, l, c, e in zip(sizes, pie_labels, colors, explode) if s > 0]\n",
        "            if pie_data:\n",
        "                ax1.pie([x[0] for x in pie_data], labels=[x[1] for x in pie_data],\n",
        "                       colors=[x[2] for x in pie_data], explode=[x[3] for x in pie_data],\n",
        "                       autopct='%1.1f%%', startangle=90, textprops={'fontsize': 9})\n",
        "                ax1.set_title(\"Vote Counting (Significance)\", fontweight='bold', fontsize=11)\n",
        "\n",
        "            # Chart B: Magnitude Bar Chart\n",
        "            y_pos = np.arange(len(labels))\n",
        "            ax2.barh(y_pos, mag_counts.values, color='#6610f2', alpha=0.7, edgecolor='black', height=0.6)\n",
        "            ax2.set_yticks(y_pos)\n",
        "            ax2.set_yticklabels(labels)\n",
        "            ax2.set_xlabel(\"Number of Studies\")\n",
        "            ax2.set_title(f\"Effect Magnitude Distribution ({es_config['effect_label_short']})\", fontweight='bold', fontsize=11)\n",
        "            ax2.grid(axis='x', linestyle='--', alpha=0.3)\n",
        "\n",
        "            # Remove spines\n",
        "            for ax in [ax1, ax2]:\n",
        "                ax.spines['top'].set_visible(False)\n",
        "                ax.spines['right'].set_visible(False)\n",
        "                if ax == ax1: ax.axis('equal') # Equal aspect ratio ensures pie is drawn as a circle.\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # --- 4. Narrative Summary ---\n",
        "            narrative = f\"\"\"\n",
        "            <div style='margin-top:20px; padding:15px; background-color:#fff; border:1px solid #ddd; border-left:4px solid #2E86AB;'>\n",
        "                <h4 style='margin-top:0; color:#2E86AB;'>📝 Automated Interpretation</h4>\n",
        "                <p style='font-size:14px; line-height:1.6; color:#333;'>\n",
        "                    This meta-analysis includes <b>{len(df)} observations</b>.\n",
        "                    Analysis of the individual confidence intervals reveals that <b>{sig_pos} studies ({sig_pos/len(df):.1%})</b> show a statistically significant positive effect,\n",
        "                    while <b>{sig_neg} studies ({sig_neg/len(df):.1%})</b> show a significant negative effect.\n",
        "                    The remaining <b>{non_sig} studies ({non_sig/len(df):.1%})</b> did not reach statistical significance individually.\n",
        "                </p>\n",
        "                <p style='font-size:14px; line-height:1.6; color:#333; margin-bottom:0;'>\n",
        "                     regarding magnitude, the most common effect size category is <b>{mag_counts.idxmax()}</b>.\n",
        "                    This suggests that while results may vary, the typical strength of the observed phenomenon is often {mag_counts.idxmax().split(' ')[0].lower()}.\n",
        "                </p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            display(HTML(narrative))\n",
        "# 5. REMOVED DATA TAB (ENHANCED)\n",
        "    with tab_removed:\n",
        "        clear_output()\n",
        "        if removed_records:\n",
        "            all_removed = pd.concat(removed_records)\n",
        "            # Select relevant columns\n",
        "            cols_to_show = ['id', 'Reason', 'xe', 'xc', 'ne', 'nc']\n",
        "            extra_cols = [c for c in df.columns if c not in cols_to_show and c in ['Year', 'Species', 'Region']]\n",
        "            final_cols = cols_to_show + extra_cols\n",
        "            final_cols = [c for c in final_cols if c in all_removed.columns]\n",
        "\n",
        "            display(HTML(f\"<h4 style='color:#c0392b'>🗑️ {len(all_removed)} Rows Excluded from Analysis</h4>\"))\n",
        "\n",
        "            explanation_html = r\"\"\"\n",
        "            <div style='background-color:#fff3cd; border-left: 5px solid #ffeeba; padding:15px; margin-bottom:20px; color:#856404;'>\n",
        "                <b>💡 Common Reasons for Removal:</b>\n",
        "                <ul style='margin-top:5px; margin-bottom:0; padding-left:20px;'>\n",
        "                    <li><b>Calculation Failed (N=1):</b> Variance cannot be calculated if Sample Size is 1 (dividing by N-1 means dividing by zero). Hedges' g requires valid variance.</li>\n",
        "                    <li><b>Zero Variance:</b> If SD is 0, the inverse-variance weight becomes infinite.</li>\n",
        "                    <li><b>Negative Values:</b> Log-based metrics (lnRR) cannot process negative numbers.</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            display(HTML(explanation_html))\n",
        "\n",
        "            # Display the table\n",
        "            display(all_removed[final_cols])\n",
        "\n",
        "        else:\n",
        "            display(HTML(\"<div style='padding:15px; background:#d4edda; color:#155724;'><b>✅ Clean Run:</b> No observations were removed.</div>\"))\n",
        "# =============================================================================\n",
        "# EXECUTION: MODE BRANCHING\n",
        "# =============================================================================\n",
        "\n",
        "# Check which mode we're in\n",
        "data_type_mode = ANALYSIS_CONFIG.get('data_type', 'raw')\n",
        "\n",
        "if data_type_mode == 'raw':\n",
        "    # RAW MODE: Run existing calculation\n",
        "    run_calculation()\n",
        "    display(tabs)\n",
        "\n",
        "else:\n",
        "    # PRE-CALCULATED MODE: Standardize and validate\n",
        "    # Re-populate the same tabs with pre-calculated data\n",
        "\n",
        "    # --- CONFIG CHECK ---\n",
        "    try:\n",
        "        effect_size_type = ANALYSIS_CONFIG['effect_size_type']\n",
        "        es_config = ANALYSIS_CONFIG['es_config']\n",
        "        variance_type = ANALYSIS_CONFIG.get('variance_type', 'variance')\n",
        "    except KeyError as e:\n",
        "        print(f\"❌ ERROR: Configuration missing: {e}\")\n",
        "    else:\n",
        "        # --- DATA LOADING ---\n",
        "        if 'data_filtered' not in globals():\n",
        "            print(\"❌ ERROR: Data not found. Run filtering cell first.\")\n",
        "        else:\n",
        "            df = data_filtered.copy()\n",
        "            initial_obs = len(df)\n",
        "            removed_records = []\n",
        "            log_diag = []\n",
        "\n",
        "            log_diag.append(\"<b>1. Pre-calculated Data Mode</b>\")\n",
        "            log_diag.append(f\"• Effect Size Type: {es_config['effect_label']}\")\n",
        "\n",
        "            # --- VARIANCE HANDLING ---\n",
        "            if variance_type == 'se':\n",
        "                if 'se' in df.columns:\n",
        "                    df['se'] = pd.to_numeric(df['se'], errors='coerce')\n",
        "                    df['vi'] = df['se'] ** 2\n",
        "                    log_diag.append(\"• Converted SE to Variance (vi = se²)\")\n",
        "                else:\n",
        "                    print(\"❌ ERROR: SE column not found\")\n",
        "            elif variance_type == 'variance':\n",
        "                if 'variance' in df.columns:\n",
        "                    df['variance'] = pd.to_numeric(df['variance'], errors='coerce')\n",
        "                    df['vi'] = df['variance']\n",
        "                    log_diag.append(\"• Using Variance column\")\n",
        "                else:\n",
        "                    print(\"❌ ERROR: Variance column not found\")\n",
        "            elif variance_type == 'both':\n",
        "                if 'variance' in df.columns:\n",
        "                    df['variance'] = pd.to_numeric(df['variance'], errors='coerce')\n",
        "                    df['vi'] = df['variance']\n",
        "                    log_diag.append(\"• Using Variance (both provided)\")\n",
        "                elif 'se' in df.columns:\n",
        "                    df['se'] = pd.to_numeric(df['se'], errors='coerce')\n",
        "                    df['vi'] = df['se'] ** 2\n",
        "                    log_diag.append(\"• Using SE and converting to Variance\")\n",
        "\n",
        "            # --- STANDARDIZE NAMES ---\n",
        "            effect_col = es_config['effect_col']\n",
        "            var_col = es_config['var_col']\n",
        "            se_col = es_config['se_col']\n",
        "            ci_lower_col = es_config['ci_lower_col']\n",
        "            ci_upper_col = es_config['ci_upper_col']\n",
        "\n",
        "            if 'yi' in df.columns:\n",
        "                df[effect_col] = df['yi']\n",
        "            df[var_col] = df['vi']\n",
        "\n",
        "            # Convert to numeric (coerce errors to NaN)\n",
        "            df[effect_col] = pd.to_numeric(df[effect_col], errors='coerce')\n",
        "            df[var_col] = pd.to_numeric(df[var_col], errors='coerce')\n",
        "\n",
        "            log_diag.append(f\"• Renamed: yi → {effect_col}, vi → {var_col}\")\n",
        "\n",
        "            # --- CRITICAL VALIDATION ---\n",
        "            log_diag.append(\"<br><b>2. Data Validation</b>\")\n",
        "\n",
        "            # Check missing values\n",
        "            mask_missing = df[[effect_col, var_col]].isna().any(axis=1)\n",
        "            if mask_missing.sum() > 0:\n",
        "                log_diag.append(f\"• ⚠️ Removed {mask_missing.sum()} rows with missing values\")\n",
        "                dropped = df[mask_missing].copy()\n",
        "                dropped['Reason'] = 'Missing Effect Size or Variance'\n",
        "                removed_records.append(dropped)\n",
        "                df = df[~mask_missing].copy()\n",
        "\n",
        "            # Check negative variance (CRITICAL)\n",
        "            mask_neg_var = df[var_col] < 0\n",
        "            if mask_neg_var.sum() > 0:\n",
        "                print(\"\\n\" + \"=\"*60)\n",
        "                print(\"❌ CRITICAL ERROR: NEGATIVE VARIANCE DETECTED\")\n",
        "                print(\"=\"*60)\n",
        "                print(f\"Found {mask_neg_var.sum()} rows with negative variance.\")\n",
        "                print(\"Variance must always be positive. Please fix your data.\")\n",
        "                print(\"\\nProblematic rows:\")\n",
        "                print(df[mask_neg_var][['id', effect_col, var_col]].head(10))\n",
        "            else:\n",
        "                # Check zero variance\n",
        "                mask_zero_var = df[var_col] == 0\n",
        "                if mask_zero_var.sum() > 0:\n",
        "                    log_diag.append(f\"• ⚠️ Removed {mask_zero_var.sum()} rows with zero variance (infinite weight)\")\n",
        "                    dropped = df[mask_zero_var].copy()\n",
        "                    dropped['Reason'] = 'Zero Variance (Infinite Weight)'\n",
        "                    removed_records.append(dropped)\n",
        "                    df = df[~mask_zero_var].copy()\n",
        "\n",
        "                log_diag.append(\"• ✓ Validation passed\")\n",
        "\n",
        "                # --- CALCULATE DERIVED COLUMNS ---\n",
        "                log_diag.append(\"<br><b>3. Calculate Derived Statistics</b>\")\n",
        "\n",
        "                df[se_col] = np.sqrt(df[var_col])\n",
        "                df[ci_lower_col] = df[effect_col] - 1.96 * df[se_col]\n",
        "                df[ci_upper_col] = df[effect_col] + 1.96 * df[se_col]\n",
        "                df['w_fixed'] = 1 / df[var_col]\n",
        "\n",
        "                log_diag.append(f\"• Calculated SE, CI, and weights\")\n",
        "\n",
        "                # Handle optional n_total\n",
        "                if 'n_total' not in df.columns:\n",
        "                    df['n_total'] = np.nan\n",
        "\n",
        "                # Add interpretation columns for lnRR\n",
        "                if effect_size_type == 'lnRR':\n",
        "                    df['Response_Ratio'] = np.exp(df[effect_col])\n",
        "                    df['fold_change'] = df.apply(lambda r: r['Response_Ratio'] if r[effect_col]>=0 else -1/r['Response_Ratio'], axis=1)\n",
        "                    df['Percent_Change'] = (df['Response_Ratio'] - 1) * 100\n",
        "                    log_diag.append(\"• Added interpretation columns\")\n",
        "\n",
        "                # --- UPDATE CONFIG ---\n",
        "                ANALYSIS_CONFIG['analysis_data'] = df\n",
        "                ANALYSIS_CONFIG['effect_col'] = effect_col\n",
        "                ANALYSIS_CONFIG['var_col'] = var_col\n",
        "                ANALYSIS_CONFIG['se_col'] = se_col\n",
        "                ANALYSIS_CONFIG['ci_lower_col'] = ci_lower_col\n",
        "                ANALYSIS_CONFIG['ci_upper_col'] = ci_upper_col\n",
        "\n",
        "                final_n = len(df)\n",
        "\n",
        "\n",
        "                # --- BUILD VCV MATRICES ---\n",
        "                vcv_matrices = build_vcv_matrices(df, effect_size_type, var_col)\n",
        "                ANALYSIS_CONFIG['vcv_matrices'] = vcv_matrices\n",
        "                log_diag.append(f\"• Built VCV matrices for {len(vcv_matrices)} studies\")\n",
        "\n",
        "                # --- POPULATE TABS (Simplified version) ---\n",
        "                with tab_summary:\n",
        "                    clear_output()\n",
        "                    html_sum = f\"\"\"\n",
        "                    <div style='display:flex; gap:20px; margin-bottom:20px;'>\n",
        "                        <div style='background:#e8f5e9; padding:15px; border-radius:8px; flex:1; text-align:center; border:1px solid #c3e6cb'>\n",
        "                            <h2 style='margin:0; color:#2e7d32;'>{final_n}</h2>\n",
        "                            <p style='margin:0; color:#1b5e20; font-size:12px;'>OBSERVATIONS</p>\n",
        "                        </div>\n",
        "                        <div style='background:#e3f2fd; padding:15px; border-radius:8px; flex:1; text-align:center; border:1px solid #bbdefb'>\n",
        "                            <h2 style='margin:0; color:#1565c0;'>{df['id'].nunique()}</h2>\n",
        "                            <p style='margin:0; color:#0d47a1; font-size:12px;'>STUDIES</p>\n",
        "                        </div>\n",
        "                        <div style='background:#fff3e0; padding:15px; border-radius:8px; flex:1; text-align:center; border:1px solid #ffe0b2'>\n",
        "                            <h2 style='margin:0; color:#e65100;'>{initial_obs - final_n}</h2>\n",
        "                            <p style='margin:0; color:#bf360c; font-size:12px;'>REMOVED</p>\n",
        "                        </div>\n",
        "                    </div>\n",
        "                    <div style='background:#d1ecf1; padding:15px; border-radius:5px; color:#0c5460; margin-bottom:15px;'>\n",
        "                        <b>📊 Pre-calculated Mode:</b> Data standardized and validated successfully.\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "                    display(HTML(html_sum))\n",
        "\n",
        "                    # Show basic stats\n",
        "                    desc = df[effect_col].describe()\n",
        "                    display(HTML(f\"<p><b>Mean Effect:</b> {desc['mean']:.4f} | <b>Median:</b> {desc['50%']:.4f} | <b>Range:</b> [{desc['min']:.4f}, {desc['max']:.4f}]</p>\"))\n",
        "                    display(df[['id', effect_col, se_col, var_col]].head(5))\n",
        "\n",
        "                with tab_diag:\n",
        "                    clear_output()\n",
        "                    display(HTML(\"<h4 style='color:#2E86AB;'>Processing Log</h4>\"))\n",
        "                    for line in log_diag:\n",
        "                        display(HTML(f\"<div style='font-size:13px;'>{line}</div>\"))\n",
        "\n",
        "                with tab_removed:\n",
        "                    clear_output()\n",
        "                    if removed_records:\n",
        "                        all_removed = pd.concat(removed_records)\n",
        "                        display(HTML(f\"<h4>🗑️ {len(all_removed)} Rows Removed</h4>\"))\n",
        "                        display(all_removed[['id', 'Reason', effect_col, var_col]].head(20))\n",
        "                    else:\n",
        "                        display(HTML(\"<div style='padding:15px; background:#d4edda;'>✅ No rows removed</div>\"))\n",
        "\n",
        "                # --- STATS TAB ---\n",
        "                with tab_stats:\n",
        "                    clear_output()\n",
        "                    display(HTML(\"<h4 style='color:#2E86AB;'>📏 Descriptive Statistics</h4>\"))\n",
        "\n",
        "                    desc = df[effect_col].describe()\n",
        "                    stats_html = f\"\"\"\n",
        "                    <table style='width:100%; border-collapse:collapse;'>\n",
        "                        <tr style='background:#f1f3f5;'><th style='padding:8px;'>Stat</th><th style='padding:8px; text-align:right;'>Value</th></tr>\n",
        "                        <tr><td style='padding:8px;'>Mean</td><td style='padding:8px; text-align:right;'>{desc['mean']:.4f}</td></tr>\n",
        "                        <tr><td style='padding:8px;'>Median</td><td style='padding:8px; text-align:right;'>{desc['50%']:.4f}</td></tr>\n",
        "                        <tr><td style='padding:8px;'>Min / Max</td><td style='padding:8px; text-align:right;'>{desc['min']:.4f} / {desc['max']:.4f}</td></tr>\n",
        "                    </table>\n",
        "                    \"\"\"\n",
        "                    display(HTML(stats_html))\n",
        "                    display(df[['id', effect_col, se_col]].head(10))\n",
        "\n",
        "                # --- INTERPRETATION TAB ---\n",
        "                with tab_interp:\n",
        "                    clear_output()\n",
        "                    display(HTML(\"<h4 style='color:#2E86AB;'>🧠 Effect Size Interpretation</h4>\"))\n",
        "\n",
        "                    null_val = es_config.get('null_value', 0)\n",
        "                    sig_pos = ((df[ci_lower_col] > null_val)).sum()\n",
        "                    sig_neg = ((df[ci_upper_col] < null_val)).sum()\n",
        "                    non_sig = len(df) - sig_pos - sig_neg\n",
        "\n",
        "                    display(HTML(f\"\"\"\n",
        "                    <div style='background:#e3f2fd; padding:15px; border-radius:5px;'>\n",
        "                        <b>Significance Summary:</b><br>\n",
        "                        • Positive: {sig_pos} ({sig_pos/len(df)*100:.1f}%)<br>\n",
        "                        • Negative: {sig_neg} ({sig_neg/len(df)*100:.1f}%)<br>\n",
        "                        • Non-significant: {non_sig} ({non_sig/len(df)*100:.1f}%)<br>\n",
        "                        <br><b>Mean Effect:</b> {df[effect_col].mean():.3f}\n",
        "                    </div>\n",
        "                    \"\"\"))\n",
        "\n",
        "\n",
        "                display(tabs)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0k6tVUVh07VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 📊 OVERALL META-ANALYSIS V2\n",
        "\n",
        "# =============================================================================\n",
        "# CELL: OVERALL META-ANALYSIS (UNIFIED)\n",
        "# Purpose: Calculate effects, Heterogeneity, Model Fit (AIC), and Generate Text.\n",
        "# Features: Combines the rich UI/Text of V2 with the Statistical Rigor (AIC) of V2.1\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from scipy.stats import norm, chi2, t\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# --- 1. LAYOUT & WIDGETS ---\n",
        "tab_main = widgets.Output()\n",
        "tab_hetero = widgets.Output()\n",
        "tab_compare = widgets.Output()\n",
        "tab_settings = widgets.Output()\n",
        "tab_publication = widgets.Output()\n",
        "tab_export = widgets.Output()\n",
        "\n",
        "tabs = widgets.Tab(children=[tab_main, tab_hetero, tab_compare, tab_settings, tab_publication, tab_export])\n",
        "tabs.set_title(0, '📊 Primary Result')\n",
        "tabs.set_title(1, '📉 Heterogeneity')\n",
        "tabs.set_title(2, '⚖️ Model Selection')\n",
        "tabs.set_title(3, '⚙️ Settings')\n",
        "tabs.set_title(4, '📝 Publication Text')\n",
        "tabs.set_title(5, '💾 Export')\n",
        "\n",
        "# Settings Widgets\n",
        "alpha_widget = widgets.Dropdown(\n",
        "    options=[('95% CI (α=0.05)', 0.05),\n",
        "             ('99% CI (α=0.01)', 0.01),\n",
        "             ('90% CI (α=0.10)', 0.10)],\n",
        "    value=0.05,\n",
        "    description='Confidence Level:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "model_selector = widgets.Dropdown(\n",
        "    options=['Auto-Select (Best AIC)', 'Force 2-Level', 'Force 3-Level'],\n",
        "    value='Auto-Select (Best AIC)',\n",
        "    description='Model Selection:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "method_options = ['REML', 'DL', 'ML']\n",
        "tau_method_widget = widgets.Dropdown(options=method_options, value='REML', description='τ² Method:')\n",
        "use_kh_widget = widgets.Checkbox(value=True, description='Knapp-Hartung Correction')\n",
        "match_r_ll_widget = widgets.Checkbox(value=False, description='Use Full Log-Likelihood')\n",
        "ci_dist_widget = widgets.Dropdown(\n",
        "    options=[('t-distribution (Robust/Small Sample)', 't'),\n",
        "             ('Normal distribution (z) (Match R)', 'norm')],\n",
        "    value='t',\n",
        "    description='Inference:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "# --- 2. ENGINES (STATISTICAL CORE) ---\n",
        "\n",
        "def calculate_2level_fit(df, effect_col, var_col, tau2):\n",
        "    \"\"\"Calculate LogLik and AIC for standard 2-level RE model.\"\"\"\n",
        "    y = df[effect_col].values\n",
        "    v = df[var_col].values\n",
        "    k = len(y)\n",
        "\n",
        "    # Weights\n",
        "    w = 1.0 / (v + tau2)\n",
        "    sum_w = np.sum(w)\n",
        "    mu = np.sum(w * y) / sum_w\n",
        "\n",
        "    # Approximate REML Log-Likelihood\n",
        "    resid_sq = np.sum(w * (y - mu)**2)\n",
        "    ll = -0.5 * (np.sum(np.log(v + tau2)) + np.log(sum_w) + resid_sq)\n",
        "\n",
        "    # Params = 2 (mu, tau2)\n",
        "    aic = -2 * ll + 2 * 2\n",
        "    return {'ll': ll, 'aic': aic}\n",
        "\n",
        "def _run_three_level_reml(df, effect_col, var_col):\n",
        "    \"\"\"\n",
        "    Core 3-Level Optimizer (Robust Version).\n",
        "    Features: Matrix Support, Deterministic Sorting, Multi-Start Optimization.\n",
        "    \"\"\"\n",
        "    # 1. CRITICAL: Sort to ensure alignment with VCV construction order\n",
        "    df = df.sort_values('id').reset_index(drop=True)\n",
        "\n",
        "    grouped = df.groupby('id', sort=False)\n",
        "    y_all = [g[effect_col].values for _, g in grouped]\n",
        "\n",
        "    # --- VCV MATRIX PREPARATION ---\n",
        "    vcv_all = []\n",
        "    vcv_dict = ANALYSIS_CONFIG.get('vcv_matrices', {})\n",
        "\n",
        "    for study_id, group in grouped:\n",
        "        # Robust Key Lookup (String vs Int)\n",
        "        sid_str = str(study_id)\n",
        "\n",
        "        if sid_str in vcv_dict:\n",
        "            vcv_all.append(vcv_dict[sid_str])\n",
        "        elif study_id in vcv_dict:\n",
        "            vcv_all.append(vcv_dict[study_id])\n",
        "        else:\n",
        "            # Fallback: Create diagonal matrix\n",
        "            vi = group[var_col].values\n",
        "            # Validation: ensure non-empty and no NaN\n",
        "            if len(vi) == 0 or np.any(np.isnan(vi)):\n",
        "                raise ValueError(f\"Invalid variance data for study {study_id}\")\n",
        "            vcv_all.append(np.diag(vi))\n",
        "\n",
        "    # Validation check\n",
        "    assert len(y_all) == len(vcv_all), f\"Data mismatch: {len(y_all)} effects vs {len(vcv_all)} matrices\"\n",
        "\n",
        "    N, M = len(df), len(y_all)\n",
        "\n",
        "    # Internal Likelihood Function (Closure captures y_all, vcv_all)\n",
        "    def nll(params):\n",
        "        tau2, sigma2 = params\n",
        "        if tau2 < 0 or sigma2 < 0: return 1e10\n",
        "        ll = 0; sum_S = 0; sum_Sy = 0; sum_ySy = 0\n",
        "\n",
        "        for i in range(M):\n",
        "            y = y_all[i]\n",
        "            V_i = vcv_all[i]\n",
        "            k = len(y)\n",
        "\n",
        "            # Check if diagonal (efficiency check)\n",
        "            is_diag = (k == 1) or np.allclose(V_i, np.diag(np.diag(V_i)))\n",
        "\n",
        "            if is_diag:\n",
        "                # Sherman-Morrison Path\n",
        "                v = np.diag(V_i) if k > 1 else np.array([V_i[0, 0]])\n",
        "                A_inv = 1.0 / (v + sigma2)\n",
        "                sum_A_inv = np.sum(A_inv)\n",
        "                denom = 1 + tau2 * sum_A_inv\n",
        "\n",
        "                ll += np.sum(np.log(v + sigma2)) + np.log(denom)\n",
        "\n",
        "                w_y = A_inv * y - (tau2 * A_inv * np.sum(A_inv * y)) / denom\n",
        "                w_1 = A_inv - (tau2 * A_inv * sum_A_inv) / denom\n",
        "\n",
        "                sum_S += np.sum(w_1)\n",
        "                sum_Sy += np.sum(w_y)\n",
        "                sum_ySy += np.dot(y, w_y)\n",
        "            else:\n",
        "                # Matrix Path\n",
        "                Sigma_i = V_i.copy()\n",
        "                np.fill_diagonal(Sigma_i, np.diag(Sigma_i) + sigma2)\n",
        "                Sigma_i += tau2\n",
        "\n",
        "                try:\n",
        "                    # Cholesky for stability\n",
        "                    L = np.linalg.cholesky(Sigma_i)\n",
        "                    A_inv_mat = np.linalg.inv(Sigma_i)\n",
        "                    log_det = 2 * np.sum(np.log(np.diag(L)))\n",
        "                except np.linalg.LinAlgError:\n",
        "                    A_inv_mat = np.linalg.pinv(Sigma_i)\n",
        "                    _, log_det = np.linalg.slogdet(Sigma_i)\n",
        "\n",
        "                ones = np.ones(k)\n",
        "                w_1_vec = np.dot(A_inv_mat, ones)\n",
        "                w_y_vec = np.dot(A_inv_mat, y)\n",
        "\n",
        "                ll += log_det\n",
        "                sum_S += np.sum(w_1_vec)\n",
        "                sum_Sy += np.sum(w_y_vec)\n",
        "                sum_ySy += np.dot(y, w_y_vec)\n",
        "\n",
        "        mu = sum_Sy / sum_S\n",
        "        resid = sum_ySy - 2*mu*sum_Sy + mu**2 * sum_S\n",
        "        return 0.5 * (ll + np.log(sum_S) + resid)\n",
        "\n",
        "    # 2. Optimization Strategy (Multi-Start)\n",
        "    # Using larger start points to avoid local minima at 0\n",
        "    start_points = [\n",
        "        [0.01, 0.01], [0.1, 0.1], [0.5, 0.5],\n",
        "        [1.0, 1.0], [1.0, 0.1], [0.1, 1.0]\n",
        "    ]\n",
        "\n",
        "    best_res = None\n",
        "    best_fun = np.inf\n",
        "\n",
        "    for start in start_points:\n",
        "        try:\n",
        "            res = minimize(nll, start, bounds=[(1e-8, None)]*2, method='L-BFGS-B', options={'ftol':1e-11})\n",
        "            if res.success and res.fun < best_fun:\n",
        "                best_fun = res.fun\n",
        "                best_res = res\n",
        "        except: continue\n",
        "\n",
        "    if not best_res: return None\n",
        "\n",
        "    tau2, sigma2 = best_res.x\n",
        "    nll_val = best_res.fun\n",
        "    log_lik = -nll_val\n",
        "    aic = 2*3 - 2*log_lik\n",
        "\n",
        "    # Recalculate weights at optimum\n",
        "    sum_S = 0; sum_Sy = 0\n",
        "    for i in range(M):\n",
        "        y = y_all[i]; V_i = vcv_all[i]; k = len(y)\n",
        "        is_diag = (k == 1) or np.allclose(V_i, np.diag(np.diag(V_i)))\n",
        "\n",
        "        if is_diag:\n",
        "            v = np.diag(V_i) if k > 1 else np.array([V_i[0, 0]])\n",
        "            A_inv = 1.0 / (v + sigma2)\n",
        "            denom = 1 + tau2 * np.sum(A_inv)\n",
        "            w_y = A_inv * y - (tau2 * A_inv * np.sum(A_inv * y)) / denom\n",
        "            w_1 = A_inv - (tau2 * A_inv * np.sum(A_inv)) / denom\n",
        "            sum_S += np.sum(w_1); sum_Sy += np.sum(w_y)\n",
        "        else:\n",
        "            Sigma_i = V_i.copy()\n",
        "            np.fill_diagonal(Sigma_i, np.diag(Sigma_i) + sigma2)\n",
        "            Sigma_i += tau2\n",
        "            try: A_inv_mat = np.linalg.inv(Sigma_i)\n",
        "            except: A_inv_mat = np.linalg.pinv(Sigma_i)\n",
        "            sum_S += np.sum(np.dot(A_inv_mat, np.ones(k)))\n",
        "            sum_Sy += np.sum(np.dot(A_inv_mat, y))\n",
        "\n",
        "    mu = sum_Sy / sum_S\n",
        "    se = np.sqrt(1.0 / sum_S)\n",
        "    total_var = tau2 + sigma2\n",
        "    icc_l3 = (tau2 / total_var * 100) if total_var > 0 else 0\n",
        "    icc_l2 = (sigma2 / total_var * 100) if total_var > 0 else 0\n",
        "\n",
        "    return {'mu': mu, 'se': se, 'tau2': tau2, 'sigma2': sigma2,\n",
        "            'icc_l3': icc_l3, 'icc_l2': icc_l2, 'n': N, 'm': M, 'aic': aic, 'log_lik_reml': log_lik}\n",
        "\n",
        "\n",
        "\n",
        "# --- 3. PUBLICATION TEXT GENERATOR (MERGED) ---\n",
        "def generate_publication_text(mu_p, ci_lo_p, ci_hi_p, p_p, tau2_re, I2, Q, df_Q, p_Q, k_obs, k_studies,\n",
        "                               method, use_kh, res_3l, mu_re, ci_lo_re, ci_hi_re, es_config,\n",
        "                               best_model, aic_2l, aic_3l):\n",
        "    \"\"\"\n",
        "    Generate rich, publication-ready text with AIC justification and detailed interpretation.\n",
        "    Combines the narrative style of V2 with the statistical logic of V2.1.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define CI percentage (typically 95% for meta-analysis)\n",
        "    ci_pct = 95\n",
        "\n",
        "    # 1. Determine Effect Size Description\n",
        "    es_type = es_config.get('type', 'effect size')\n",
        "    es_map = {\n",
        "        \"Hedges' g\": \"Effect sizes were calculated as Hedges' g, a standardized mean difference corrected for small sample bias.\",\n",
        "        'lnRR': \"Effect sizes were expressed as log response ratios (lnRR), calculated as the natural logarithm of the ratio between treatment and control group means.\",\n",
        "        'SMD': \"Effect sizes were calculated as standardized mean differences (SMD).\",\n",
        "        \"Cohen's d\": \"Effect sizes were calculated as Cohen's d.\"\n",
        "    }\n",
        "    es_description = es_map.get(es_type, f\"Effect sizes were calculated as {es_type}.\")\n",
        "\n",
        "    # 2. Significance & formatting\n",
        "    sig_text = \"significant\" if p_p < 0.05 else \"non-significant\"\n",
        "    p_format = f\"< 0.001\" if p_p < 0.001 else f\"= {p_p:.3f}\"\n",
        "\n",
        "    # 3. Magnitude Interpretation (Cohen's conventions - customizable)\n",
        "    abs_mu = abs(mu_p)\n",
        "    if abs_mu < 0.2: effect_interp = \"indicating a negligible effect\"\n",
        "    elif abs_mu < 0.5: effect_interp = \"indicating a small effect\"\n",
        "    elif abs_mu < 0.8: effect_interp = \"indicating a moderate effect\"\n",
        "    else: effect_interp = \"indicating a large effect\"\n",
        "\n",
        "    # 4. Heterogeneity Interpretation\n",
        "    if I2 < 25: het_interp = \"indicating low heterogeneity\"\n",
        "    elif I2 < 50: het_interp = \"indicating moderate heterogeneity\"\n",
        "    elif I2 < 75: het_interp = \"indicating substantial heterogeneity\"\n",
        "    else: het_interp = \"indicating considerable heterogeneity\"\n",
        "\n",
        "    # --- BUILD HTML TEXT ---\n",
        "    text = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff;'>\n",
        "\n",
        "    <h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px;'>Meta-Analysis Results</h3>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    A total of <b>{k_obs}</b> effect sizes from <b>{k_studies}</b> independent studies were included in the meta-analysis. {es_description}\n",
        "    </p>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    \"\"\"\n",
        "\n",
        "    # 5. Model Selection Narrative (The AIC Logic)\n",
        "    if res_3l:\n",
        "        delta_aic = abs(aic_2l - aic_3l)\n",
        "        if best_model == \"3-Level\":\n",
        "            text += f\"\"\"\n",
        "            To account for the dependency structure arising from multiple effect sizes nested within studies, a three-level random-effects model was compared to a standard two-level model. Model selection based on the Akaike Information Criterion (AIC) favored the <b>three-level model</b> (AIC = {aic_3l:.1f}) over the two-level model (AIC = {aic_2l:.1f}; ΔAIC = {delta_aic:.1f}), indicating significant clustering of effects within studies.\n",
        "            \"\"\"\n",
        "        else:\n",
        "            text += f\"\"\"\n",
        "            A three-level random-effects model was initially fitted to account for potential dependency of effects within studies. However, model comparison based on the Akaike Information Criterion (AIC) favored the more parsimonious <b>two-level random-effects model</b> (AIC = {aic_2l:.1f}) over the three-level model (AIC = {aic_3l:.1f}; ΔAIC = {delta_aic:.1f}). Consequently, results from the standard two-level model are reported below.\n",
        "            \"\"\"\n",
        "    else:\n",
        "        text += \"A standard random-effects meta-analysis was conducted. \"\n",
        "\n",
        "    # 6. Primary Results (Winner Results)\n",
        "    text += f\"\"\"\n",
        "    The analysis revealed a <b>{sig_text}</b> overall pooled effect of <b>{mu_p:.3f}</b> ({ci_pct:.0f}% CI [{ci_lo_p:.3f}, {ci_hi_p:.3f}], <i>p</i> {p_format}), {effect_interp}.\n",
        "    </p>\n",
        "    \"\"\"\n",
        "\n",
        "    # 7. Fold Change Logic (Restored from Cell 1)\n",
        "    # Checks if 'has_fold_change' is in config OR if type is lnRR\n",
        "    if es_config.get('has_fold_change', False) or es_type == 'lnRR':\n",
        "        RR = np.exp(mu_p)\n",
        "        if mu_p >= 0:\n",
        "            fold_text = f\"{RR:.2f}× increase\"\n",
        "            pct = (RR - 1) * 100\n",
        "            pct_text = f\"{pct:.1f}% increase\"\n",
        "        else:\n",
        "            fold_text = f\"{1/RR:.2f}× decrease\"\n",
        "            pct = (1 - RR) * 100\n",
        "            pct_text = f\"{pct:.1f}% decrease\"\n",
        "\n",
        "        text += f\"\"\"\n",
        "        <p style='text-align: justify;'>\n",
        "        Transforming the log-ratio back to the original scale, this corresponds to a <b>{fold_text}</b> in the outcome variable relative to the control group (equivalent to a <b>{pct_text}</b>).\n",
        "        </p>\n",
        "        \"\"\"\n",
        "\n",
        "    # 8. Heterogeneity Paragraph\n",
        "    text += f\"\"\"\n",
        "    <p style='text-align: justify;'>\n",
        "    {het_interp.capitalize()} was observed among the effect sizes (<i>Q</i>({df_Q}) = {Q:.2f}, <i>p</i> < 0.001, <i>I</i>² = <b>{I2:.1f}%</b>). The between-study variance (τ²) was estimated at <b>{tau2_re:.4f}</b> using the <b>{method}</b> estimator.\n",
        "    </p>\n",
        "    \"\"\"\n",
        "\n",
        "    # 9. Variance Decomposition (Only if 3-Level Won)\n",
        "    if best_model == \"3-Level\" and res_3l:\n",
        "        text += f\"\"\"\n",
        "        <p style='text-align: justify;'>\n",
        "        Variance decomposition in the three-level model indicated that <b>{res_3l['icc_l3']:.1f}%</b> of the total heterogeneity was attributable to between-study differences (τ² = {res_3l['tau2']:.4f}), while <b>{res_3l['icc_l2']:.1f}%</b> was due to within-study variance (σ² = {res_3l['sigma2']:.4f}).\n",
        "        </p>\n",
        "        \"\"\"\n",
        "\n",
        "    # 10. Technical Footnote\n",
        "    kh_text = f\"Confidence intervals and <i>p</i>-values were adjusted using the Knapp-Hartung correction (<i>t</i>-distribution, df = {df_Q}).\" if use_kh else \"Confidence intervals were calculated using the normal distribution.\"\n",
        "    text += f\"\"\"\n",
        "    <p style='color: #666; font-size: 0.9em; margin-top: 10px; border-top: 1px solid #eee; padding-top: 5px;'>\n",
        "    <i>Statistical Note: {kh_text}</i>\n",
        "    </p>\n",
        "    \"\"\"\n",
        "\n",
        "    # 11. Guidance & Tips (Restored from Cell 1)\n",
        "    text += f\"\"\"\n",
        "    <div style='background-color: #ecf0f1; padding: 15px; border-left: 4px solid #3498db; margin-top: 20px;'>\n",
        "    <h4 style='margin-top: 0; color: #2c3e50;'>Interpretation Guidance:</h4>\n",
        "    <ul style='margin-bottom: 0; font-size: 0.95em;'>\n",
        "    <li><b>AIC Selection:</b> The text above automatically selected the statistical model that fits your data best.</li>\n",
        "    <li><b>Context:</b> Add specific biological/field context to the \"small/large effect\" interpretation.</li>\n",
        "    <li><b>Sensitivity:</b> If I² is high (>50%), consider discussing potential sources of heterogeneity (Subgroup Analysis).</li>\n",
        "    </ul>\n",
        "    </div>\n",
        "\n",
        "    <div style='background-color: #fff3cd; padding: 10px; border-left: 4px solid #ffc107; margin-top: 15px;'>\n",
        "    <p style='margin: 0;'><b>💡 Tip:</b> You can copy this text directly into your manuscript. It includes the math, the method, and the justification.</p>\n",
        "    </div>\n",
        "\n",
        "    </div>\"\"\"\n",
        "\n",
        "    return text\n",
        "\n",
        "def generate_methods_text_overall(es_config, overall_results):\n",
        "    \"\"\"\n",
        "    Generates a 'Materials and Methods' section with dynamic citations\n",
        "    and a placeholder for the software tool itself.\n",
        "    \"\"\"\n",
        "    import sys\n",
        "\n",
        "    # 1. Extract Settings\n",
        "    es_label = es_config.get('effect_label', 'Effect Size')\n",
        "    es_short = es_config.get('effect_label_short', 'ES')\n",
        "    use_kh = overall_results.get('knapp_hartung', {}).get('used', False)\n",
        "\n",
        "    # 2. Define Citation Database\n",
        "    # You can update the 'tool' entry with your actual paper/repo details later!\n",
        "    db = {\n",
        "        'hedges': \"Hedges, L. V. (1981). Distribution theory for Glass's estimator of effect size and related estimators. <i>Journal of Educational Statistics</i>, 6(2), 107-128.\",\n",
        "        'lnRR': \"Hedges, L. V., Gurevitch, J., & Curtis, P. S. (1999). The meta-analysis of response ratios in experimental ecology. <i>Ecology</i>, 80(4), 1150-1156.\",\n",
        "        'cohen': \"Cohen, J. (1988). <i>Statistical power analysis for the behavioral sciences</i> (2nd ed.). Hillsdale, NJ: Erlbaum.\",\n",
        "        'reml': \"Viechtbauer, W. (2005). Bias and efficiency of meta-analytic variance estimators in the random-effects model. <i>Journal of Educational and Behavioral Statistics</i>, 30(3), 261-293.\",\n",
        "        'i2': \"Higgins, J. P., & Thompson, S. G. (2002). Quantifying heterogeneity in a meta-analysis. <i>Statistics in Medicine</i>, 21(11), 1539-1558.\",\n",
        "        'kh': \"Knapp, G., & Hartung, J. (2003). Improved tests for a random effects meta-regression with a single covariate. <i>Statistics in Medicine</i>, 22(17), 2693-2710.\",\n",
        "        'python': \"Virtanen, P., et al. (2020). SciPy 1.0: fundamental algorithms for scientific computing in Python. <i>Nature Methods</i>, 17(3), 261-272.\",\n",
        "        'tool': \"<b>[CITATION FOR THIS TOOL]:</b> Author, A. A. (202X). <i>Co-Meta: An Interactive Pipeline for Meta-Analysis</i>. [Repository/DOI]. (Available at: https://github.com/...)\"\n",
        "    }\n",
        "\n",
        "    # 3. Build Dynamic Reference List\n",
        "    # We store them in order so the [1], [2] numbers in text match the list.\n",
        "    active_refs = []\n",
        "\n",
        "    # [1] Effect Size\n",
        "    es_key = 'hedges' if 'Hedges' in es_label else 'lnRR' if 'Ratio' in es_label else 'cohen'\n",
        "    active_refs.append(db[es_key])\n",
        "    ref_es = len(active_refs)\n",
        "\n",
        "    # [2] Model Estimator\n",
        "    active_refs.append(db['reml'])\n",
        "    ref_reml = len(active_refs)\n",
        "\n",
        "    # [3] Heterogeneity\n",
        "    active_refs.append(db['i2'])\n",
        "    ref_i2 = len(active_refs)\n",
        "\n",
        "    # [4] Knapp-Hartung (Conditional)\n",
        "    ref_kh = None\n",
        "    if use_kh:\n",
        "        active_refs.append(db['kh'])\n",
        "        ref_kh = len(active_refs)\n",
        "\n",
        "    # [Next] Python\n",
        "    active_refs.append(db['python'])\n",
        "    ref_py = len(active_refs)\n",
        "\n",
        "    # [Next] This Tool\n",
        "    active_refs.append(db['tool'])\n",
        "    ref_tool = len(active_refs)\n",
        "\n",
        "    # 4. Build the HTML Text\n",
        "    html = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff; border: 1px solid #eee; margin-bottom: 20px;'>\n",
        "\n",
        "    <h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-top: 0;'>Materials and Methods</h3>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Effect Size Calculation.</b> The effect size of interest was {es_label} ({es_short}). Estimates were calculated using the standard formula [{ref_es}].\n",
        "    \"\"\"\n",
        "\n",
        "    if es_key == 'hedges':\n",
        "        html += \" This metric includes a correction for small sample size bias (J-correction).\"\n",
        "    elif es_key == 'lnRR':\n",
        "        html += \" Log-transformation was applied to normalize the ratio of means.\"\n",
        "\n",
        "    html += f\"\"\"\n",
        "    </p>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Meta-Analytic Model.</b> To estimate the overall pooled effect, we fitted a random-effects model using the Restricted Maximum Likelihood (REML) estimator [{ref_reml}].\n",
        "    This approach accounts for both within-study sampling error and between-study heterogeneity.\n",
        "    Heterogeneity was quantified using the <i>I</i>² statistic [{ref_i2}] and tested using Cochran's <i>Q</i> test.\n",
        "    \"\"\"\n",
        "\n",
        "    if use_kh:\n",
        "        html += f\" Confidence intervals and test statistics were adjusted using the Knapp-Hartung method [{ref_kh}] to reduce Type I error rates and provide more robust inference.\"\n",
        "\n",
        "    html += f\"\"\"\n",
        "    </p>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Software.</b> All analyses were conducted using the Python programming language (v{sys.version.split()[0]}), utilizing the SciPy library for statistical computations [{ref_py}].\n",
        "    The complete analytical pipeline was implemented using the <b>Co-Meta</b> toolkit [{ref_tool}], which integrates data processing, statistical modeling, and visualization.\n",
        "    </p>\n",
        "\n",
        "    <h4 style='color: #34495e; margin-top: 20px; font-size: 11pt;'>References</h4>\n",
        "    <ol style='font-size: 10pt; color: #555;'>\n",
        "    \"\"\"\n",
        "\n",
        "    # 5. Print Bibliography\n",
        "    for ref in active_refs:\n",
        "        html += f\"<li>{ref}</li>\"\n",
        "\n",
        "    html += \"</ol></div>\"\n",
        "\n",
        "    return html\n",
        "\n",
        "# --- 4. MAIN ANALYSIS CONTROLLER ---\n",
        "# --- 4. MAIN ANALYSIS CONTROLLER (CORRECTED) ---\n",
        "def run_analysis(change=None):\n",
        "    # Clear all output tabs to prevent stacking\n",
        "    for tab in [tab_main, tab_hetero, tab_compare, tab_settings, tab_publication]:\n",
        "        tab.clear_output()\n",
        "\n",
        "    # --- CONFIGURATION CHECK ---\n",
        "    if 'ANALYSIS_CONFIG' not in globals():\n",
        "        with tab_main: print(\"❌ Config missing. Run Step 1.\")\n",
        "        return\n",
        "\n",
        "    # Update Global Settings based on widgets\n",
        "    if 'ANALYSIS_CONFIG' in globals():\n",
        "        ANALYSIS_CONFIG['settings'] = {\n",
        "            'alpha': alpha_widget.value,\n",
        "            'dist_type': ci_dist_widget.value,\n",
        "            'use_kh': use_kh_widget.value,\n",
        "            'match_r_ll': match_r_ll_widget.value,\n",
        "            'model_choice': model_selector.value,\n",
        "            'tau_method': tau_method_widget.value\n",
        "        }\n",
        "\n",
        "    eff_col = ANALYSIS_CONFIG.get('effect_col')\n",
        "    var_col = ANALYSIS_CONFIG.get('var_col')\n",
        "    es_config = ANALYSIS_CONFIG.get('es_config', {})\n",
        "\n",
        "    # Load Data\n",
        "    if 'analysis_data' in ANALYSIS_CONFIG: df = ANALYSIS_CONFIG['analysis_data'].copy()\n",
        "    elif 'data_filtered' in globals(): df = data_filtered.copy()\n",
        "    else: return\n",
        "\n",
        "    # Clean Data\n",
        "    df = df.dropna(subset=[eff_col, var_col])\n",
        "    df = df[df[var_col] > 0]\n",
        "    if len(df) == 0:\n",
        "        with tab_main: print(\"❌ No valid data points.\")\n",
        "        return\n",
        "\n",
        "    # --- A. STATISTICS ENGINE ---\n",
        "\n",
        "    # 1. Fixed Effect (Baseline)\n",
        "    w_fe = 1 / df[var_col]\n",
        "    mu_fe = np.average(df[eff_col], weights=w_fe)\n",
        "    se_fe = np.sqrt(1 / np.sum(w_fe))\n",
        "\n",
        "    # Calculate Fixed Effect CIs based on user settings\n",
        "    alpha = alpha_widget.value\n",
        "    q = 1 - (alpha / 2)\n",
        "    dist_type = ci_dist_widget.value\n",
        "\n",
        "    # Fixed effect usually uses Normal distribution, but we can apply t if requested\n",
        "    if dist_type == 't':\n",
        "        crit_val_fe = t.ppf(q, len(df)-1)\n",
        "    else:\n",
        "        crit_val_fe = norm.ppf(q)\n",
        "\n",
        "    ci_lower_fixed = mu_fe - crit_val_fe * se_fe\n",
        "    ci_upper_fixed = mu_fe + crit_val_fe * se_fe\n",
        "\n",
        "    # 2. Heterogeneity Stats (Standard DL based Q)\n",
        "    Q = np.sum(w_fe * (df[eff_col] - mu_fe)**2)\n",
        "    df_Q = len(df) - 1\n",
        "    C = np.sum(w_fe) - np.sum(w_fe**2)/np.sum(w_fe)\n",
        "\n",
        "    # Calculate I2 and p-value for Q\n",
        "    p_Q = 1 - chi2.cdf(Q, df_Q)\n",
        "    I2 = max(0, (Q - df_Q) / Q * 100) if Q > 0 else 0\n",
        "\n",
        "    # 3. Calculate Tau2 based on USER SELECTION (The Fix)\n",
        "    method = tau_method_widget.value\n",
        "\n",
        "    if method == 'DL':\n",
        "        tau2_re = max(0, (Q - df_Q) / C) if C > 0 else 0\n",
        "    else:\n",
        "        # Use the global calculator from Cell 4.5\n",
        "        # Note: If running ML, this function should return ML estimate\n",
        "        try:\n",
        "            tau2_re, _ = calculate_tau_squared(df, eff_col, var_col, method=method)\n",
        "        except Exception as e:\n",
        "            print(f\"Estimator error ({method}), falling back to DL: {e}\")\n",
        "            tau2_re = max(0, (Q - df_Q) / C) if C > 0 else 0\n",
        "\n",
        "    # Calculate AIC for 2-Level Model using the selected Tau2\n",
        "    fit_2l = calculate_2level_fit(df, eff_col, var_col, tau2_re)\n",
        "\n",
        "    # 4. Random Effects Pooled Estimate\n",
        "    w_re = 1 / (df[var_col] + tau2_re)\n",
        "    mu_re = np.average(df[eff_col], weights=w_re)\n",
        "    se_re = np.sqrt(1 / np.sum(w_re))\n",
        "\n",
        "    use_kh = use_kh_widget.value\n",
        "\n",
        "    # Knapp-Hartung Adjustment\n",
        "    if use_kh and len(df) > 1:\n",
        "        q_re = np.sum(w_re * (df[eff_col] - mu_re)**2)\n",
        "        # HK adjustment inflates SE\n",
        "        se_re = se_re * np.sqrt(max(1, q_re / df_Q))\n",
        "\n",
        "    # Distribution Selection for 2-Level\n",
        "    if dist_type == 't':\n",
        "        dist = t(df_Q)\n",
        "        crit_val = dist.ppf(q)\n",
        "    else:\n",
        "        dist = norm\n",
        "        crit_val = norm.ppf(q)\n",
        "\n",
        "    ci_lo_re = mu_re - crit_val * se_re\n",
        "    ci_hi_re = mu_re + crit_val * se_re\n",
        "    p_re = 2 * (1 - dist.cdf(abs(mu_re / se_re)))\n",
        "\n",
        "    # 5. 3-Level Analysis (Nested)\n",
        "    res_3l = None\n",
        "    if len(df) > df['id'].nunique():\n",
        "        try:\n",
        "            # Note: Currently _run_three_level_reml is hardcoded to REML.\n",
        "            # Ideally, pass 'method' here if the 3-level engine supports ML vs REML.\n",
        "            res_3l = _run_three_level_reml(df, eff_col, var_col)\n",
        "            mu_3l, se_3l = res_3l['mu'], res_3l['se']\n",
        "\n",
        "            # Degrees of Freedom (k - 1)\n",
        "            df_3l = df['id'].nunique() - 1\n",
        "\n",
        "            # Distribution Logic for 3-Level\n",
        "            if dist_type == 't':\n",
        "                crit_val_3l = t.ppf(q, df_3l)\n",
        "                p_3l = 2 * (1 - t.cdf(abs(mu_3l / se_3l), df_3l))\n",
        "            else:\n",
        "                crit_val_3l = norm.ppf(q)\n",
        "                p_3l = 2 * (1 - norm.cdf(abs(mu_3l / se_3l)))\n",
        "\n",
        "            ci_lo_3l = mu_3l - crit_val_3l * se_3l\n",
        "            ci_hi_3l = mu_3l + crit_val_3l * se_3l\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"3-Level Error: {e}\")\n",
        "            pass\n",
        "\n",
        "    # --- ADJUST FOR R COMPATIBILITY (CONSTANT TERM) ---\n",
        "    if match_r_ll_widget.value:\n",
        "        N_obs = len(df)\n",
        "        const_term = -0.5 * N_obs * np.log(2 * np.pi)\n",
        "\n",
        "        # Adjust 2-Level\n",
        "        fit_2l['ll'] += const_term\n",
        "        # Re-calculate AIC: 2*k - 2*LL (k=2 for standard RE)\n",
        "        fit_2l['aic'] = 4 - 2 * fit_2l['ll']\n",
        "\n",
        "        # Adjust 3-Level\n",
        "        if res_3l:\n",
        "            # Reconstruct AIC with constant\n",
        "            # Parameters = 3 (mu, tau2, sigma2)\n",
        "            log_lik_adj = res_3l['log_lik_reml'] + const_term\n",
        "            res_3l['log_lik_reml'] = log_lik_adj\n",
        "            res_3l['aic'] = 6 - 2 * log_lik_adj\n",
        "\n",
        "    # --- B. MODEL SELECTION ---\n",
        "    user_choice = model_selector.value\n",
        "    if user_choice == 'Force 2-Level':\n",
        "        best_model = \"2-Level\"\n",
        "    elif user_choice == 'Force 3-Level':\n",
        "        if res_3l:\n",
        "            best_model = \"3-Level\"\n",
        "        else:\n",
        "            best_model = \"2-Level\"\n",
        "    else:\n",
        "        # Auto-Select via AIC\n",
        "        best_model = \"2-Level\"\n",
        "        if res_3l and res_3l['aic'] < fit_2l['aic'] - 2:\n",
        "            best_model = \"3-Level\"\n",
        "\n",
        "    # Assign Primary Results\n",
        "    if best_model == \"3-Level\":\n",
        "        mu_p, ci_lo_p, ci_hi_p, p_p = mu_3l, ci_lo_3l, ci_hi_3l, p_3l\n",
        "        model_label = \"3-Level REML\"\n",
        "        model_desc = \"Adjusted for within-study dependency (AIC selected).\"\n",
        "    else:\n",
        "        mu_p, ci_lo_p, ci_hi_p, p_p = mu_re, ci_lo_re, ci_hi_re, p_re\n",
        "        model_label = f\"Random-Effects ({method})\"\n",
        "        model_desc = f\"Standard 2-Level model using {method} estimator.\"\n",
        "\n",
        "    # Save Global Settings\n",
        "    ANALYSIS_CONFIG['global_settings'] = {\n",
        "        'alpha': alpha,\n",
        "        'dist_type': dist_type,\n",
        "        'ci_percent': (1 - alpha)*100\n",
        "    }\n",
        "\n",
        "    # --- SAVE RESULTS ---\n",
        "    ANALYSIS_CONFIG['overall_results'] = {\n",
        "        'pooled_effect_fixed': mu_fe,\n",
        "        'pooled_SE_fixed': se_fe,\n",
        "        'ci_lower_fixed': ci_lower_fixed,\n",
        "        'ci_upper_fixed': ci_upper_fixed,\n",
        "\n",
        "        'pooled_effect_random': mu_re,\n",
        "        'pooled_SE_random_reported': se_re,\n",
        "        'ci_lower_random_reported': ci_lo_re,\n",
        "        'ci_upper_random_reported': ci_hi_re,\n",
        "        'p_value_random_reported': p_re,\n",
        "\n",
        "        'Qt': Q, 'I_squared': I2, 'tau_squared': tau2_re,\n",
        "        'k': len(df), 'k_papers': df['id'].nunique(),\n",
        "        'knapp_hartung': {'used': use_kh},\n",
        "        'aic_2level': fit_2l['aic'],\n",
        "        'best_model': best_model\n",
        "    }\n",
        "\n",
        "    if res_3l:\n",
        "        ANALYSIS_CONFIG['three_level_results'] = {\n",
        "            'status': 'completed',\n",
        "            'pooled_effect': res_3l['mu'],\n",
        "            'se': res_3l['se'],\n",
        "            'ci_lower': ci_lo_3l,\n",
        "            'ci_upper': ci_hi_3l,\n",
        "            'tau_squared': res_3l['tau2'],\n",
        "            'sigma_squared': res_3l['sigma2'],\n",
        "            'aic': res_3l['aic']\n",
        "        }\n",
        "\n",
        "    # --- C. RENDER TABS ---\n",
        "    # 1. PRIMARY RESULT TAB\n",
        "    with tab_main:\n",
        "        sig = \"***\" if p_p < 0.001 else \"**\" if p_p < 0.01 else \"*\" if p_p < 0.05 else \"ns\"\n",
        "        color = \"#28a745\" if p_p < 0.05 else \"#6c757d\"\n",
        "        ci_pct = (1 - alpha) * 100\n",
        "\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style='padding: 20px;'>\n",
        "        <h2 style='color: #2c3e50; margin-bottom: 20px;'>{model_label} <span style='font-size:0.5em; background: #28a745; color: white; padding: 4px 8px; border-radius: 4px; vertical-align: middle; margin-left: 10px;'>AIC WINNER 🏆</span></h2>\n",
        "\n",
        "        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white; margin-bottom: 20px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);'>\n",
        "            <h1 style='margin: 0; font-size: 3.5em; text-align: center; font-weight: 700;'>{mu_p:.3f}</h1>\n",
        "            <p style='margin: 5px 0 0 0; text-align: center; font-size: 1.2em; opacity: 0.9;'>Pooled Effect Size {sig}</p>\n",
        "        </div>\n",
        "\n",
        "        <div style='display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 20px;'>\n",
        "            <div style='background-color: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 5px solid #007bff; box-shadow: 0 2px 4px rgba(0,0,0,0.05);'>\n",
        "                <div style='color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;'>{ci_pct:.0f}% Confidence Interval</div>\n",
        "                <div style='font-size: 1.6em; font-weight: bold; color: #2c3e50;'>[{ci_lo_p:.3f}, {ci_hi_p:.3f}]</div>\n",
        "            </div>\n",
        "            <div style='background-color: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 5px solid {color}; box-shadow: 0 2px 4px rgba(0,0,0,0.05);'>\n",
        "                <div style='color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;'>P-value</div>\n",
        "                <div style='font-size: 1.6em; font-weight: bold; color: {color};'>{p_p:.4g}</div>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div style='background-color: #e7f3ff; padding: 15px; border-radius: 5px; color: #004085; font-size: 0.95em;'>\n",
        "            <p style='margin: 0;'><b>ℹ️ Model Note:</b> {model_desc} (k = {len(df)} observations from {df['id'].nunique()} studies)</p>\n",
        "        </div>\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "    # 2. HETEROGENEITY TAB\n",
        "    with tab_hetero:\n",
        "        het_color = \"#dc3545\" if I2 > 75 else \"#ffc107\" if I2 > 50 else \"#28a745\"\n",
        "        het_label = \"High\" if I2 > 75 else \"Substantial\" if I2 > 50 else \"Moderate\" if I2 > 25 else \"Low\"\n",
        "\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style='padding: 20px;'>\n",
        "        <h2 style='color: #2c3e50;'>Heterogeneity Assessment</h2>\n",
        "\n",
        "        <div style='display: flex; align-items: center; gap: 20px; margin-bottom: 25px;'>\n",
        "            <div style='background-color: {het_color}; padding: 20px; border-radius: 10px; color: white; min-width: 150px; text-align: center; box-shadow: 0 4px 6px rgba(0,0,0,0.1);'>\n",
        "                <h1 style='margin: 0; font-size: 2.5em;'>{I2:.1f}%</h1>\n",
        "                <p style='margin: 5px 0 0 0;'>I² Statistic</p>\n",
        "            </div>\n",
        "            <div style='font-size: 1.1em; color: #555;'>\n",
        "                The data shows <b>{het_label.lower()}</b> heterogeneity.<br>\n",
        "                <span style='font-size: 0.9em; color: #888;'>This suggests that {I2:.1f}% of the variance is due to real differences between studies, not just sampling error.</span>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <table style='width: 100%; border-collapse: collapse; border: 1px solid #dee2e6;'>\n",
        "            <thead style='background-color: #f1f3f5;'>\n",
        "                <tr>\n",
        "                    <th style='padding: 12px; text-align: left; border-bottom: 2px solid #dee2e6;'>Statistic</th>\n",
        "                    <th style='padding: 12px; text-align: left; border-bottom: 2px solid #dee2e6;'>Value</th>\n",
        "                    <th style='padding: 12px; text-align: left; border-bottom: 2px solid #dee2e6;'>Interpretation</th>\n",
        "                </tr>\n",
        "            </thead>\n",
        "            <tbody>\n",
        "                <tr>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6;'><b>Q-statistic</b></td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6;'>{Q:.2f} (df = {df_Q})</td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6;'>p = {p_Q:.4g} (Signif. if < 0.05)</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6;'><b>I² (Inconsistency)</b></td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6;'>{I2:.1f}%</td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6;'>{het_label} Heterogeneity</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6;'><b>τ² (Between-Study Variance)</b></td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6;'>{tau2_re:.4f}</td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6;'>Estimated via {method}</td>\n",
        "                </tr>\n",
        "            </tbody>\n",
        "        </table>\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "    # 3. MODEL COMPARISON TAB\n",
        "    with tab_compare:\n",
        "        c_2l = \"#d4edda\" if best_model == \"2-Level\" else \"#fff\"\n",
        "        c_3l = \"#d4edda\" if best_model == \"3-Level\" else \"#fff\"\n",
        "        b_2l = \"🏆 Best Fit\" if best_model == \"2-Level\" else \"\"\n",
        "        b_3l = \"🏆 Best Fit\" if best_model == \"3-Level\" else \"\"\n",
        "\n",
        "        aic_3l_disp = f\"{res_3l['aic']:.1f}\" if res_3l else \"N/A\"\n",
        "\n",
        "        html = f\"\"\"\n",
        "        <div style='padding: 20px;'>\n",
        "        <h3 style='color: #2c3e50;'>Model Selection</h3>\n",
        "        <p style='margin-bottom: 20px;'>We compare models using <b>AIC</b> (Akaike Information Criterion). Lower is better. The \"Best Fit\" is selected automatically.</p>\n",
        "\n",
        "        <table style='width: 100%; border-collapse: collapse; box-shadow: 0 2px 10px rgba(0,0,0,0.05); margin-bottom:30px;'>\n",
        "            <thead style='background-color: #2c3e50; color: white;'>\n",
        "                <tr>\n",
        "                    <th style='padding: 12px; text-align: left;'>Model</th>\n",
        "                    <th style='padding: 12px; text-align: center;'>Pooled Effect [{ci_pct:.0f}% CI]</th>\n",
        "                    <th style='padding: 12px; text-align: center;'>AIC</th>\n",
        "                    <th style='padding: 12px; text-align: center;'>Verdict</th>\n",
        "                </tr>\n",
        "            </thead>\n",
        "            <tbody>\n",
        "                <tr style='background-color: #f8f9fa; color: #6c757d;'>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6;'><i>Fixed-Effect (Baseline)</i></td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6; text-align: center;'>{mu_fe:.3f} [{ci_lower_fixed:.3f}, {ci_upper_fixed:.3f}]</td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6; text-align: center;'>-</td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6; text-align: center; font-size: 0.9em;'><i>Assumes I²=0</i></td>\n",
        "                </tr>\n",
        "                <tr style='background-color: {c_2l};'>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6;'><b>2-Level Random-Effects</b></td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6; text-align: center;'>{mu_re:.3f} [{ci_lo_re:.3f}, {ci_hi_re:.3f}]</td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6; text-align: center;'><b>{fit_2l['aic']:.1f}</b></td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6; text-align: center;'>{b_2l}</td>\n",
        "                </tr>\n",
        "        \"\"\"\n",
        "        if res_3l:\n",
        "            html += f\"\"\"\n",
        "                <tr style='background-color: {c_3l};'>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6;'><b>3-Level REML (Nested)</b></td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6; text-align: center;'>{res_3l['mu']:.3f} [{ci_lo_3l:.3f}, {ci_hi_3l:.3f}]</td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6; text-align: center;'><b>{aic_3l_disp}</b></td>\n",
        "                    <td style='padding: 12px; border-bottom: 1px solid #dee2e6; text-align: center;'>{b_3l}</td>\n",
        "                </tr>\n",
        "            \"\"\"\n",
        "        html += \"\"\"\n",
        "            </tbody>\n",
        "        </table>\n",
        "        </div>\"\"\"\n",
        "        display(HTML(html))\n",
        "\n",
        "        # --- Visual Sensitivity Plot ---\n",
        "        display(HTML(\"<h4 style='color:#2E86AB; margin-left:20px; margin-top:0;'>📉 Visual Sensitivity Analysis</h4>\"))\n",
        "\n",
        "        models = ['Fixed-Effect', f'Random ({method})', '3-Level (Nested)']\n",
        "        means = [mu_fe, mu_re]\n",
        "        lowers = [ci_lower_fixed, ci_lo_re]\n",
        "        uppers = [ci_upper_fixed, ci_hi_re]\n",
        "        colors = ['#95a5a6', '#95a5a6']\n",
        "\n",
        "        if res_3l:\n",
        "            means.append(res_3l['mu'])\n",
        "            lowers.append(ci_lo_3l)\n",
        "            uppers.append(ci_hi_3l)\n",
        "            colors.append('#95a5a6')\n",
        "        else:\n",
        "            models.pop()\n",
        "\n",
        "        winner_idx = 2 if best_model == \"3-Level\" and res_3l else 1\n",
        "        colors[winner_idx] = '#28a745'\n",
        "        labels = [f\"{m}\\n{'★ Winner' if i==winner_idx else ''}\" for i, m in enumerate(models)]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 3))\n",
        "        y_pos = np.arange(len(models))\n",
        "\n",
        "        for i, (m, l, u, c) in enumerate(zip(means, lowers, uppers, colors)):\n",
        "            ax.plot([l, u], [i, i], color=c, linewidth=2.5, marker='|', markersize=10)\n",
        "            ax.plot(m, i, 'o', color=c, markersize=9, markeredgecolor='white')\n",
        "            ax.text(m, i-0.25, f\"{m:.3f}\", ha='center', va='top', color=c, fontweight='bold', fontsize=9)\n",
        "\n",
        "        ax.set_yticks(y_pos)\n",
        "        ax.set_yticklabels(labels, fontsize=10, fontweight='bold')\n",
        "        ax.set_ylim(-0.5, len(models)-0.5)\n",
        "        ax.invert_yaxis()\n",
        "\n",
        "        null_val = es_config.get('null_value', 0)\n",
        "        ax.axvline(null_val, color='black', linestyle=':', alpha=0.3)\n",
        "\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['left'].set_visible(False)\n",
        "        ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
        "        ax.set_xlabel(f\"Pooled Effect Size ({es_config.get('effect_label_short', 'ES')})\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # 4. SETTINGS TAB\n",
        "    with tab_settings:\n",
        "        display(HTML(\"<h3>Analysis Settings</h3>\"))\n",
        "        display(HTML(\"<p>Configure statistical corrections:</p>\"))\n",
        "        display(model_selector)\n",
        "        display(tau_method_widget)\n",
        "        display(use_kh_widget)\n",
        "        display(ci_dist_widget)\n",
        "        display(alpha_widget)\n",
        "        display(match_r_ll_widget)\n",
        "\n",
        "        btn = widgets.Button(description=\"Re-Run Analysis\", button_style='primary', icon='refresh')\n",
        "        btn.on_click(run_analysis)\n",
        "        display(btn)\n",
        "\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-top: 20px; border: 1px solid #dee2e6;'>\n",
        "        <h4 style='margin-top: 0; color: #2c3e50;'>Current Data Configuration:</h4>\n",
        "        <ul style='margin-bottom: 0; padding-left: 20px;'>\n",
        "        <li><b>Effect Size Column:</b> <code>{eff_col}</code></li>\n",
        "        <li><b>Variance Column:</b> <code>{var_col}</code></li>\n",
        "        <li><b>Total Effect Sizes (k):</b> {len(df)}</li>\n",
        "        <li><b>Unique Studies:</b> {df['id'].nunique()}</li>\n",
        "        </ul>\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "    # 5. PUBLICATION TEXT TAB\n",
        "    with tab_publication:\n",
        "        aic_3l_val = res_3l['aic'] if res_3l else 9999\n",
        "\n",
        "        methods_html = generate_methods_text_overall(es_config, ANALYSIS_CONFIG['overall_results'])\n",
        "        results_html = generate_publication_text(\n",
        "            mu_p, ci_lo_p, ci_hi_p, p_p, tau2_re, I2, Q, df_Q, p_Q,\n",
        "            len(df), df['id'].nunique(),\n",
        "            method, use_kh, res_3l, mu_re, ci_lo_re, ci_hi_re, es_config,\n",
        "            best_model, fit_2l['aic'], aic_3l_val\n",
        "        )\n",
        "\n",
        "        display(HTML(methods_html))\n",
        "        display(HTML(results_html))\n",
        "\n",
        "        ANALYSIS_CONFIG['overall_text'] = methods_html + \"<br><hr><br>\" + results_html\n",
        "\n",
        "# Init\n",
        "tau_method_widget.observe(run_analysis, 'value')\n",
        "use_kh_widget.observe(run_analysis, 'value')\n",
        "match_r_ll_widget.observe(run_analysis, 'value')\n",
        "ci_dist_widget.observe(run_analysis, 'value')\n",
        "model_selector.observe(run_analysis, 'value')\n",
        "alpha_widget.observe(run_analysis, 'value')\n",
        "display(tabs)\n",
        "run_analysis()\n",
        "\n",
        "tab_export = widgets.Output()\n",
        "# Update tabs list safely\n",
        "tabs.children = list(tabs.children)[:-1] + [tab_export] if len(tabs.children) > 5 else list(tabs.children) + [tab_export]\n",
        "tabs.set_title(len(tabs.children)-1, '💾 Export')\n",
        "\n",
        "with tab_export:\n",
        "    display(HTML(\"<h3 style='color: #2E86AB;'>💾 Download Audit Report</h3>\"))\n",
        "    display(HTML(\"<p>Generate a full Excel audit trail including model settings, heterogeneity statistics, and the exclusion log.</p>\"))\n",
        "\n",
        "    btn_overall_export = widgets.Button(\n",
        "        description=\"📥 Download Overall Analysis Report\",\n",
        "        button_style='info', icon='file-excel', layout=widgets.Layout(width='300px', height='40px')\n",
        "    )\n",
        "\n",
        "    def on_overall_export_click(b):\n",
        "        export_analysis_report(report_type='overall', filename_prefix='Overall_Meta_Analysis')\n",
        "\n",
        "    btn_overall_export.on_click(on_overall_export_click)\n",
        "    display(btn_overall_export)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ehj6zFw1LNih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k_UgQywyi0vh"
      },
      "outputs": [],
      "source": [
        "#@title ⚙️ Subgroup Analysis - Configuration\n",
        "\n",
        "# =============================================================================\n",
        "# SUBGROUP ANALYSIS CONFIGURATION (DASHBOARD VERSION)\n",
        "# Purpose: Configure moderator variables with organized tabbed interface\n",
        "# Dependencies: Step 2 (overall_results, analysis_data)\n",
        "# Outputs: ANALYSIS_CONFIG['subgroup_config']\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import datetime\n",
        "\n",
        "# --- 1. CREATE TAB LAYOUT ---\n",
        "tab_config = widgets.VBox()\n",
        "tab_moderators = widgets.Output()\n",
        "tab_thresholds = widgets.Output()\n",
        "tab_details = widgets.Output()\n",
        "\n",
        "tabs = widgets.Tab(children=[tab_config, tab_moderators, tab_thresholds, tab_details])\n",
        "tabs.set_title(0, '📋 Configuration')\n",
        "tabs.set_title(1, '📊 Moderator Preview')\n",
        "tabs.set_title(2, '⚙️ Thresholds')\n",
        "tabs.set_title(3, '📝 Details')\n",
        "\n",
        "# --- 2. WIDGETS ---\n",
        "analysis_type_widget = widgets.RadioButtons(\n",
        "    options=[('Single-Factor Analysis', 'single'), ('Two-Factor Analysis (Interaction)', 'two_way')],\n",
        "    value='single', description='', layout=widgets.Layout(width='auto')\n",
        ")\n",
        "\n",
        "moderator1_widget = None\n",
        "moderator2_widget = None\n",
        "\n",
        "min_papers_widget = widgets.IntSlider(\n",
        "    value=3, min=1, max=10, step=1, description='Min Papers:',\n",
        "    style={'description_width': '120px'}, layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "min_obs_widget = widgets.IntSlider(\n",
        "    value=5, min=2, max=20, step=1, description='Min Observations:',\n",
        "    style={'description_width': '120px'}, layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "run_button = widgets.Button(\n",
        "    description='💾 Save Configuration & Proceed',\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='400px', height='50px'),\n",
        "    style={'font_weight': 'bold'},\n",
        "    tooltip='Click to save configuration for use in the next cell'\n",
        ")\n",
        "\n",
        "run_button_output = widgets.Output()\n",
        "status_output = widgets.Output()\n",
        "\n",
        "# --- 3. INITIALIZATION ---\n",
        "def initialize_configuration():\n",
        "    global moderator1_widget, moderator2_widget\n",
        "\n",
        "    with tab_details:\n",
        "        clear_output()\n",
        "        print(\"=\"*70)\n",
        "        print(\"INITIALIZATION & VALIDATION\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        try:\n",
        "            effect_col = ANALYSIS_CONFIG['effect_col']\n",
        "            var_col = ANALYSIS_CONFIG['var_col']\n",
        "            es_config = ANALYSIS_CONFIG['es_config']\n",
        "            overall_results = ANALYSIS_CONFIG['overall_results']\n",
        "\n",
        "            print(\"✓ Prerequisites Check:\")\n",
        "            print(f\"  • Effect: {es_config['effect_label']} ({es_config['effect_label_short']})\")\n",
        "            print(f\"  • Q: {overall_results['Qt']:.4f}, I²: {overall_results['I_squared']:.2f}%\")\n",
        "        except KeyError as e:\n",
        "            print(f\"❌ ERROR: {e}\")\n",
        "            print(\"  Please run Step 2 first\")\n",
        "            raise\n",
        "\n",
        "        # --- DATA LOADING (FIXED PRIORITY) ---\n",
        "        # 1. Always prioritize the ANALYSIS_CONFIG (where the new data lives)\n",
        "        if 'ANALYSIS_CONFIG' in globals() and 'analysis_data' in ANALYSIS_CONFIG:\n",
        "            analysis_data = ANALYSIS_CONFIG['analysis_data']\n",
        "\n",
        "            # CRITICAL FIX: Force update the global variable so it never gets stuck\n",
        "            globals()['analysis_data'] = analysis_data\n",
        "\n",
        "        # 2. Fallback if config is missing\n",
        "        elif 'analysis_data' in globals():\n",
        "            analysis_data = globals()['analysis_data']\n",
        "\n",
        "        # 3. Last resort fallback\n",
        "        elif 'data_filtered' in globals():\n",
        "            analysis_data = globals()['data_filtered']\n",
        "\n",
        "        else:\n",
        "            print(\"❌ ERROR: analysis_data not found\")\n",
        "            raise NameError(\"analysis_data not defined\")\n",
        "\n",
        "\n",
        "        k_total, k_papers = len(analysis_data), analysis_data['id'].nunique()\n",
        "        print(f\"\\n✓ Dataset: {k_total} obs, {k_papers} papers, {k_total/k_papers:.2f} avg\")\n",
        "\n",
        "        if k_total < 10:\n",
        "            print(f\"⚠️  WARNING: Limited data ({k_total} obs)\")\n",
        "        elif k_total < 20:\n",
        "            print(f\"⚠️  CAUTION: Moderate data ({k_total} obs)\")\n",
        "\n",
        "        excluded = ['xe', 'sde', 'ne', 'xc', 'sdc', 'nc', 'id', 'sde_imputed', 'sdc_imputed',\n",
        "                   'cv_e', 'cv_c', 'sde_was_imputed', 'sdc_was_imputed',\n",
        "                   effect_col, var_col, ANALYSIS_CONFIG.get('se_col', ''), 'w_fixed', 'w_random', 'ci_width']\n",
        "\n",
        "        if es_config.get('has_fold_change'):\n",
        "            excluded.extend(['Response_Ratio', 'RR_CI_lower', 'RR_CI_upper', 'fold_change',\n",
        "                           'Percent_Change', 'Odds_Ratio', 'OR_CI_lower', 'OR_CI_upper'])\n",
        "\n",
        "        if 'hedges_g' in effect_col or 'cohen_d' in effect_col:\n",
        "            excluded.extend(['df', 'sp', 'sp_squared', 'cohen_d', 'hedges_j'])\n",
        "\n",
        "        excluded.extend([c for c in analysis_data.columns if 'CI_' in c or 'ci_' in c])\n",
        "\n",
        "        # NEW CODE (Smart Detection)\n",
        "        available_moderators = []\n",
        "        for col in analysis_data.columns:\n",
        "            if col in excluded: continue\n",
        "\n",
        "            # Check 1: Is it text/string?\n",
        "            is_text = analysis_data[col].dtype == 'object'\n",
        "\n",
        "            # Check 2: Is it a Category? (Common in R data)\n",
        "            is_category = isinstance(analysis_data[col].dtype, pd.CategoricalDtype)\n",
        "\n",
        "            # Check 3: Is it a Number behaving like a Group? (e.g. Year, Grade)\n",
        "            is_group_like_number = False\n",
        "            if pd.api.types.is_numeric_dtype(analysis_data[col]):\n",
        "                # If it has fewer than 20 unique values, treat it as a subgroup\n",
        "                if analysis_data[col].nunique() < 20 and analysis_data[col].nunique() > 1:\n",
        "                    is_group_like_number = True\n",
        "\n",
        "            # If ANY check passes, keep it\n",
        "            if (is_text or is_category or is_group_like_number):\n",
        "                available_moderators.append(col)\n",
        "\n",
        "        print(f\"\\n✓ Found {len(available_moderators)} moderators:\")\n",
        "        for mod in available_moderators:\n",
        "            print(f\"  • {mod}: {analysis_data[mod].nunique()} categories\")\n",
        "\n",
        "        if not available_moderators:\n",
        "            print(\"❌ ERROR: No moderators found\")\n",
        "            raise ValueError(\"No moderators available\")\n",
        "\n",
        "        moderator1_widget = widgets.Dropdown(\n",
        "            options=available_moderators, value=available_moderators[0],\n",
        "            description='Moderator 1:', style={'description_width': '100px'},\n",
        "            layout=widgets.Layout(width='450px')\n",
        "        )\n",
        "\n",
        "        moderator2_widget = widgets.Dropdown(\n",
        "            options=['None'] + available_moderators, value='None',\n",
        "            description='Moderator 2:', style={'description_width': '100px'},\n",
        "            layout=widgets.Layout(width='450px')\n",
        "        )\n",
        "\n",
        "        analysis_type_widget.observe(update_all_tabs, names='value')\n",
        "        moderator1_widget.observe(update_all_tabs, names='value')\n",
        "        moderator2_widget.observe(update_all_tabs, names='value')\n",
        "        min_papers_widget.observe(update_thresholds_tab, names='value')\n",
        "        min_obs_widget.observe(update_thresholds_tab, names='value')\n",
        "        run_button.on_click(save_configuration)\n",
        "\n",
        "        print(\"\\n✓ Initialized successfully\")\n",
        "        return available_moderators\n",
        "\n",
        "# --- 4. TAB UPDATES ---\n",
        "def update_config_tab(change=None):\n",
        "    # Clear any previous save messages\n",
        "    with status_output:\n",
        "        clear_output()\n",
        "\n",
        "    items = []\n",
        "\n",
        "    analysis_type = analysis_type_widget.value\n",
        "\n",
        "    # 1. Header & Help (Reduced margin-bottom from 15px to 5px)\n",
        "    items.append(widgets.HTML(\"<h3 style='margin-top:0; margin-bottom:5px;'>Configure Subgroup Analysis</h3>\"))\n",
        "\n",
        "    if analysis_type == 'single':\n",
        "        help_text = \"\"\"<div style='background:#e7f3ff; padding:8px; border-radius:6px; border-left:4px solid #0066cc; margin-bottom:5px;'>\n",
        "            <b>📊 Single-Factor Subgroup Analysis</b><br>\n",
        "            <span style='font-size:12px; color:#555;'>Test if effect varies across ONE moderator. Best for primary hypotheses (10+ obs/group).</span></div>\"\"\"\n",
        "    else:\n",
        "        help_text = \"\"\"<div style='background:#fff3cd; padding:8px; border-radius:6px; border-left:4px solid #ff9800; margin-bottom:5px;'>\n",
        "            <b>📊 Two-Factor Analysis (Interaction)</b><br>\n",
        "            <span style='font-size:12px; color:#555;'>Test combinations of TWO moderators. Requires 3-5 studies/combo, 20+ total obs.</span></div>\"\"\"\n",
        "    items.append(widgets.HTML(help_text))\n",
        "\n",
        "    # 2. Controls (Reduced margin-top from 20px to 10px)\n",
        "    items.append(widgets.HTML(\"<h4 style='margin-top:10px; margin-bottom:5px;'>1. Select Analysis Type</h4>\"))\n",
        "    items.append(analysis_type_widget)\n",
        "\n",
        "    items.append(widgets.HTML(\"<h4 style='margin-top:10px; margin-bottom:5px;'>2. Select Moderator(s)</h4>\"))\n",
        "    if moderator1_widget:\n",
        "        items.append(moderator1_widget)\n",
        "\n",
        "    if analysis_type == 'two_way' and moderator2_widget:\n",
        "        items.append(moderator2_widget)\n",
        "\n",
        "    # 3. Thresholds (Reduced margins)\n",
        "    items.append(widgets.HTML(\"<h4 style='margin-top:10px; margin-bottom:5px;'>3. Set Quality Thresholds</h4>\"))\n",
        "    items.append(widgets.HTML(\"<p style='color:#666; font-size:12px; margin:0 0 5px 0;'>Adjust in <b>⚙️ Thresholds</b> tab</p>\"))\n",
        "\n",
        "    threshold_info = widgets.HBox([\n",
        "        widgets.HTML(f\"<div style='padding:5px 8px; background:#f0f0f0; border-radius:4px; margin-right:10px; font-size:13px;'>\"\n",
        "                    f\"<b>Min Papers:</b> {min_papers_widget.value}</div>\"),\n",
        "        widgets.HTML(f\"<div style='padding:5px 8px; background:#f0f0f0; border-radius:4px; font-size:13px;'>\"\n",
        "                    f\"<b>Min Obs:</b> {min_obs_widget.value}</div>\")\n",
        "    ])\n",
        "    items.append(threshold_info)\n",
        "\n",
        "    # 4. Save Section (Reduced margins)\n",
        "    items.append(widgets.HTML(\"<h4 style='margin-top:10px; margin-bottom:5px;'>4. Save Configuration</h4>\"))\n",
        "\n",
        "    items.append(run_button)\n",
        "    items.append(status_output)\n",
        "\n",
        "    # Update Children\n",
        "    tab_config.children = tuple(items)\n",
        "\n",
        "def update_moderators_tab(change=None):\n",
        "    with tab_moderators:\n",
        "        clear_output()\n",
        "\n",
        "        # --- FIX: LOAD DATA ---\n",
        "        if 'ANALYSIS_CONFIG' in globals() and 'analysis_data' in ANALYSIS_CONFIG:\n",
        "            analysis_data = ANALYSIS_CONFIG['analysis_data']\n",
        "        elif 'analysis_data' in globals():\n",
        "            analysis_data = globals()['analysis_data']\n",
        "        elif 'data_filtered' in globals():\n",
        "            analysis_data = globals()['data_filtered']\n",
        "        else:\n",
        "            return # Data not ready yet\n",
        "        if moderator1_widget is None:\n",
        "            print(\"Initializing...\")\n",
        "            return\n",
        "\n",
        "        mod1 = moderator1_widget.value\n",
        "        analysis_type = analysis_type_widget.value\n",
        "        mod2 = moderator2_widget.value if analysis_type == 'two_way' and moderator2_widget.value != 'None' else None\n",
        "\n",
        "        display(HTML(\"<h3 style='margin-top:0;'>Moderator Variable Preview</h3>\"))\n",
        "        display(HTML(f\"<h4>📊 {mod1}</h4>\"))\n",
        "\n",
        "        mod1_counts = analysis_data[mod1].value_counts().sort_index()\n",
        "\n",
        "        table_html = \"\"\"<table style='width:100%; border-collapse:collapse;'>\n",
        "            <tr style='background:#f0f0f0; border-bottom:2px solid #ddd;'>\n",
        "                <th style='text-align:left; padding:8px;'>Category</th>\n",
        "                <th style='text-align:right; padding:8px;'>Observations</th>\n",
        "                <th style='text-align:right; padding:8px;'>Papers</th>\n",
        "                <th style='text-align:right; padding:8px;'>Percent</th></tr>\"\"\"\n",
        "\n",
        "        for category, count in mod1_counts.items():\n",
        "            papers = analysis_data[analysis_data[mod1] == category]['id'].nunique()\n",
        "            pct = (count / len(analysis_data)) * 100\n",
        "            row_color = '#fff' if count >= 5 else '#fff3cd'\n",
        "            table_html += f\"\"\"<tr style='background:{row_color}; border-bottom:1px solid #eee;'>\n",
        "                <td style='padding:6px;'>{category}</td>\n",
        "                <td style='text-align:right; padding:6px;'><b>{count}</b></td>\n",
        "                <td style='text-align:right; padding:6px;'>{papers}</td>\n",
        "                <td style='text-align:right; padding:6px;'>{pct:.1f}%</td></tr>\"\"\"\n",
        "\n",
        "        table_html += \"</table>\"\n",
        "        display(HTML(table_html))\n",
        "\n",
        "        min_group = mod1_counts.min()\n",
        "        if min_group < 5:\n",
        "            display(HTML(f\"<div style='background:#fff3cd; padding:10px; margin-top:10px; border-radius:4px;'>\"\n",
        "                        f\"⚠️ Smallest group: {min_group} obs - consider adjusting thresholds</div>\"))\n",
        "        else:\n",
        "            display(HTML(\"<div style='background:#d4edda; padding:10px; margin-top:10px; border-radius:4px;'>\"\n",
        "                        \"✓ All groups have ≥5 observations</div>\"))\n",
        "\n",
        "        if mod2:\n",
        "            display(HTML(f\"<h4 style='margin-top:25px;'>📊 {mod2}</h4>\"))\n",
        "            mod2_counts = analysis_data[mod2].value_counts().sort_index()\n",
        "\n",
        "            table2_html = \"\"\"<table style='width:100%; border-collapse:collapse;'>\n",
        "                <tr style='background:#f0f0f0; border-bottom:2px solid #ddd;'>\n",
        "                    <th style='text-align:left; padding:8px;'>Category</th>\n",
        "                    <th style='text-align:right; padding:8px;'>Observations</th>\n",
        "                    <th style='text-align:right; padding:8px;'>Papers</th>\n",
        "                    <th style='text-align:right; padding:8px;'>Percent</th></tr>\"\"\"\n",
        "\n",
        "            for category, count in mod2_counts.items():\n",
        "                papers = analysis_data[analysis_data[mod2] == category]['id'].nunique()\n",
        "                pct = (count / len(analysis_data)) * 100\n",
        "                table2_html += f\"\"\"<tr style='border-bottom:1px solid #eee;'>\n",
        "                    <td style='padding:6px;'>{category}</td>\n",
        "                    <td style='text-align:right; padding:6px;'><b>{count}</b></td>\n",
        "                    <td style='text-align:right; padding:6px;'>{papers}</td>\n",
        "                    <td style='text-align:right; padding:6px;'>{pct:.1f}%</td></tr>\"\"\"\n",
        "\n",
        "            table2_html += \"</table>\"\n",
        "            display(HTML(table2_html))\n",
        "\n",
        "            display(HTML(f\"<h4 style='margin-top:25px;'>🔀 Combination Matrix: {mod1} × {mod2}</h4>\"))\n",
        "            crosstab = pd.crosstab(analysis_data[mod1], analysis_data[mod2], margins=True, margins_name='Total')\n",
        "            display(crosstab.style.background_gradient(cmap='Blues', subset=pd.IndexSlice[crosstab.index[:-1], crosstab.columns[:-1]]))\n",
        "\n",
        "            n_empty = (crosstab.iloc[:-1, :-1] == 0).sum().sum()\n",
        "            min_cell = crosstab.iloc[:-1, :-1].min().min()\n",
        "\n",
        "            if n_empty > 0:\n",
        "                display(HTML(f\"<div style='background:#f8d7da; padding:10px; margin-top:10px; border-radius:4px;'>\"\n",
        "                            f\"⚠️ {n_empty} empty combinations - will be excluded</div>\"))\n",
        "            elif min_cell < 3:\n",
        "                display(HTML(f\"<div style='background:#fff3cd; padding:10px; margin-top:10px; border-radius:4px;'>\"\n",
        "                            f\"⚠️ Min cell: {min_cell} - results may be unstable</div>\"))\n",
        "            elif min_cell < 5:\n",
        "                display(HTML(f\"<div style='background:#fff3cd; padding:10px; margin-top:10px; border-radius:4px;'>\"\n",
        "                            f\"⚠️ Some combinations limited (min: {min_cell})</div>\"))\n",
        "            else:\n",
        "                display(HTML(\"<div style='background:#d4edda; padding:10px; margin-top:10px; border-radius:4px;'>\"\n",
        "                            \"✓ All combinations have ≥5 obs</div>\"))\n",
        "\n",
        "def update_thresholds_tab(change=None):\n",
        "    with tab_thresholds:\n",
        "        clear_output()\n",
        "        if 'ANALYSIS_CONFIG' in globals() and 'analysis_data' in ANALYSIS_CONFIG:\n",
        "            analysis_data = ANALYSIS_CONFIG['analysis_data']\n",
        "        elif 'analysis_data' in globals():\n",
        "            analysis_data = globals()['analysis_data']\n",
        "        elif 'data_filtered' in globals():\n",
        "            analysis_data = globals()['data_filtered']\n",
        "        else:\n",
        "            return\n",
        "        if moderator1_widget is None:\n",
        "            print(\"Initializing...\")\n",
        "            return\n",
        "\n",
        "        display(HTML(\"<h3 style='margin-top:0;'>Quality Thresholds & Impact Analysis</h3>\"))\n",
        "        display(HTML(\"\"\"<div style='background:#f8f9fa; padding:12px; border-radius:6px; margin-bottom:15px;'>\n",
        "            <b>Purpose:</b> Ensure sufficient data for reliable estimation<br>\n",
        "            <span style='font-size:13px; color:#555;'>Higher = more reliable but fewer subgroups</span></div>\"\"\"))\n",
        "\n",
        "        display(min_papers_widget)\n",
        "        display(min_obs_widget)\n",
        "        display(HTML(\"<h4 style='margin-top:25px;'>Impact on Data Retention</h4>\"))\n",
        "\n",
        "        mod1 = moderator1_widget.value\n",
        "        analysis_type = analysis_type_widget.value\n",
        "        mod2 = moderator2_widget.value if analysis_type == 'two_way' and moderator2_widget.value != 'None' else None\n",
        "        min_papers, min_obs = min_papers_widget.value, min_obs_widget.value\n",
        "\n",
        "        groups_meeting, groups_failing = [], []\n",
        "\n",
        "        if analysis_type == 'single':\n",
        "            for cat in analysis_data[mod1].dropna().unique():\n",
        "                group_data = analysis_data[analysis_data[mod1] == cat]\n",
        "                n_papers, n_obs = group_data['id'].nunique(), len(group_data)\n",
        "\n",
        "                if n_papers >= min_papers and n_obs >= min_obs:\n",
        "                    groups_meeting.append((cat, n_obs, n_papers))\n",
        "                else:\n",
        "                    groups_failing.append((cat, n_obs, n_papers))\n",
        "        else:\n",
        "            if mod2:\n",
        "                for cat1 in analysis_data[mod1].dropna().unique():\n",
        "                    for cat2 in analysis_data[mod2].dropna().unique():\n",
        "                        cell_data = analysis_data[(analysis_data[mod1] == cat1) & (analysis_data[mod2] == cat2)]\n",
        "                        n_papers, n_obs = cell_data['id'].nunique(), len(cell_data)\n",
        "\n",
        "                        if n_papers >= min_papers and n_obs >= min_obs:\n",
        "                            groups_meeting.append((f\"{cat1} × {cat2}\", n_obs, n_papers))\n",
        "                        elif n_obs > 0:\n",
        "                            groups_failing.append((f\"{cat1} × {cat2}\", n_obs, n_papers))\n",
        "\n",
        "        total_retained = sum(obs for _, obs, _ in groups_meeting)\n",
        "        retention_pct = (total_retained / len(analysis_data)) * 100\n",
        "\n",
        "        cards_html = f\"\"\"<div style='display:flex; gap:15px; margin-bottom:20px;'>\n",
        "            <div style='flex:1; background:#d4edda; padding:15px; border-radius:6px; text-align:center;'>\n",
        "                <div style='font-size:28px; font-weight:bold; color:#155724;'>{len(groups_meeting)}</div>\n",
        "                <div style='font-size:13px; color:#155724;'>Groups Meeting Criteria</div></div>\n",
        "            <div style='flex:1; background:#{'#f8d7da' if len(groups_failing) > 0 else '#e2e3e5'}; padding:15px; border-radius:6px; text-align:center;'>\n",
        "                <div style='font-size:28px; font-weight:bold; color:#{'#721c24' if len(groups_failing) > 0 else '#6c757d'};'>{len(groups_failing)}</div>\n",
        "                <div style='font-size:13px; color:#{'#721c24' if len(groups_failing) > 0 else '#6c757d'};'>Groups Excluded</div></div>\n",
        "            <div style='flex:1; background:#{'#d4edda' if retention_pct >= 75 else '#fff3cd' if retention_pct >= 50 else '#f8d7da'}; padding:15px; border-radius:6px; text-align:center;'>\n",
        "                <div style='font-size:28px; font-weight:bold;'>{retention_pct:.0f}%</div>\n",
        "                <div style='font-size:13px;'>Data Retained</div></div></div>\"\"\"\n",
        "        display(HTML(cards_html))\n",
        "\n",
        "        if groups_meeting:\n",
        "            display(HTML(\"<h4>✓ Groups Meeting Criteria:</h4>\"))\n",
        "            meet_html = \"<ul style='margin-top:5px;'>\"\n",
        "            for cat, obs, papers in groups_meeting:\n",
        "                meet_html += f\"<li><b>{cat}:</b> {obs} obs, {papers} papers</li>\"\n",
        "            meet_html += \"</ul>\"\n",
        "            display(HTML(meet_html))\n",
        "\n",
        "        if groups_failing:\n",
        "            display(HTML(\"<h4 style='margin-top:20px;'>✗ Groups Excluded:</h4>\"))\n",
        "            fail_html = \"<ul style='margin-top:5px; color:#721c24;'>\"\n",
        "            for cat, obs, papers in groups_failing:\n",
        "                reason = []\n",
        "                if papers < min_papers:\n",
        "                    reason.append(f\"papers: {papers}<{min_papers}\")\n",
        "                if obs < min_obs:\n",
        "                    reason.append(f\"obs: {obs}<{min_obs}\")\n",
        "                fail_html += f\"<li><b>{cat}:</b> {obs} obs, {papers} papers ({', '.join(reason)})</li>\"\n",
        "            fail_html += \"</ul>\"\n",
        "            display(HTML(fail_html))\n",
        "\n",
        "        if len(groups_meeting) < 2:\n",
        "            display(HTML(\"<div style='background:#f8d7da; padding:12px; border-radius:6px; margin-top:15px;'>\"\n",
        "                        \"🔴 <b>ERROR:</b> Need ≥2 groups. Lower thresholds.</div>\"))\n",
        "        elif retention_pct < 50:\n",
        "            display(HTML(\"<div style='background:#fff3cd; padding:12px; border-radius:6px; margin-top:15px;'>\"\n",
        "                        \"⚠️ <b>WARNING:</b> <50% data retained. Consider lowering thresholds.</div>\"))\n",
        "\n",
        "def update_all_tabs(change=None):\n",
        "    update_config_tab()\n",
        "    update_moderators_tab()\n",
        "    update_thresholds_tab()\n",
        "\n",
        "# --- 5. SAVE CONFIGURATION ---\n",
        "def save_configuration(button):\n",
        "    with status_output:\n",
        "        clear_output()\n",
        "\n",
        "        # --- FIX: LOAD DATA INSIDE THE FUNCTION ---\n",
        "        # We must explicitly retrieve the data again because this function\n",
        "        # runs in a different scope than the initialization.\n",
        "        if 'ANALYSIS_CONFIG' in globals() and 'analysis_data' in ANALYSIS_CONFIG:\n",
        "            analysis_data = ANALYSIS_CONFIG['analysis_data']\n",
        "        elif 'analysis_data' in globals():\n",
        "            analysis_data = globals()['analysis_data']\n",
        "        elif 'data_filtered' in globals():\n",
        "            analysis_data = globals()['data_filtered']\n",
        "        else:\n",
        "            display(HTML(\"<div style='color:red'>❌ Error: Data not found. Please run Step 4/5 first.</div>\"))\n",
        "            return\n",
        "        analysis_type = analysis_type_widget.value\n",
        "        mod1 = moderator1_widget.value\n",
        "        mod2 = moderator2_widget.value if analysis_type == 'two_way' and moderator2_widget.value != 'None' else None\n",
        "        min_papers, min_obs = min_papers_widget.value, min_obs_widget.value\n",
        "\n",
        "        validation_errors = []\n",
        "\n",
        "        if analysis_type == 'two_way' and not mod2:\n",
        "            validation_errors.append(\"Two-way requires Moderator 2\")\n",
        "\n",
        "        if mod1 == mod2:\n",
        "            validation_errors.append(\"Moderators cannot be the same\")\n",
        "\n",
        "        valid_groups = []\n",
        "        if analysis_type == 'single':\n",
        "            for cat in analysis_data[mod1].dropna().unique():\n",
        "                group_data = analysis_data[analysis_data[mod1] == cat]\n",
        "                if group_data['id'].nunique() >= min_papers and len(group_data) >= min_obs:\n",
        "                    valid_groups.append(cat)\n",
        "        else:\n",
        "            if mod2:\n",
        "                for cat1 in analysis_data[mod1].dropna().unique():\n",
        "                    for cat2 in analysis_data[mod2].dropna().unique():\n",
        "                        cell_data = analysis_data[(analysis_data[mod1] == cat1) & (analysis_data[mod2] == cat2)]\n",
        "                        if cell_data['id'].nunique() >= min_papers and len(cell_data) >= min_obs:\n",
        "                            valid_groups.append((cat1, cat2))\n",
        "\n",
        "        if len(valid_groups) < 2:\n",
        "            validation_errors.append(f\"Only {len(valid_groups)} group(s) meet criteria. Need ≥2.\")\n",
        "\n",
        "        if validation_errors:\n",
        "            error_html = \"<div style='background:#f8d7da; padding:15px; border-radius:6px; border-left:4px solid #dc3545;'>\"\n",
        "            error_html += \"<h4 style='margin-top:0; color:#721c24;'>❌ Validation Failed</h4><ul style='margin-bottom:0; color:#721c24;'>\"\n",
        "            for err in validation_errors:\n",
        "                error_html += f\"<li>{err}</li>\"\n",
        "            error_html += \"</ul></div>\"\n",
        "            display(HTML(error_html))\n",
        "            return\n",
        "\n",
        "        if analysis_type == 'single':\n",
        "            retained_data = analysis_data[analysis_data[mod1].isin(valid_groups)]\n",
        "        else:\n",
        "            retained_data = analysis_data[analysis_data.apply(lambda row: (row[mod1], row[mod2]) in valid_groups, axis=1)]\n",
        "\n",
        "        retention_pct = (len(retained_data) / len(analysis_data)) * 100\n",
        "\n",
        "        ANALYSIS_CONFIG['subgroup_config'] = {\n",
        "            'timestamp': datetime.datetime.now(),\n",
        "            'analysis_type': analysis_type,\n",
        "            'moderator1': mod1,\n",
        "            'moderator2': mod2,\n",
        "            'min_papers': min_papers,\n",
        "            'min_obs': min_obs,\n",
        "            'expected_groups': len(valid_groups),\n",
        "            'valid_groups_list': valid_groups,\n",
        "            'data_retained': len(retained_data),\n",
        "            'retention_pct': retention_pct,\n",
        "            'has_empty_cells': analysis_type == 'two_way' and mod2 and\n",
        "                               (pd.crosstab(analysis_data[mod1], analysis_data[mod2]) == 0).sum().sum() > 0,\n",
        "            'n_empty_cells': (pd.crosstab(analysis_data[mod1], analysis_data[mod2]) == 0).sum().sum()\n",
        "                            if analysis_type == 'two_way' and mod2 else 0\n",
        "        }\n",
        "\n",
        "        ANALYSIS_CONFIG['subgroup_config']['moderator1_info'] = {\n",
        "            'name': mod1,\n",
        "            'n_categories': analysis_data[mod1].nunique(),\n",
        "            'categories': sorted(analysis_data[mod1].dropna().unique().tolist())\n",
        "        }\n",
        "\n",
        "        if mod2:\n",
        "            ANALYSIS_CONFIG['subgroup_config']['moderator2_info'] = {\n",
        "                'name': mod2,\n",
        "                'n_categories': analysis_data[mod2].nunique(),\n",
        "                'categories': sorted(analysis_data[mod2].dropna().unique().tolist())\n",
        "            }\n",
        "\n",
        "        success_html = f\"\"\"<div style='background:#d4edda; padding:15px; border-radius:6px; border-left:4px solid #28a745;'>\n",
        "            <h4 style='margin-top:0; color:#155724;'>✓ Configuration Saved Successfully</h4>\n",
        "            <table style='width:100%; margin-top:10px;'>\n",
        "                <tr><td><b>Analysis Type:</b></td><td>{analysis_type}</td></tr>\n",
        "                <tr><td><b>Primary Moderator:</b></td><td>{mod1}</td></tr>\n",
        "                {f'<tr><td><b>Secondary Moderator:</b></td><td>{mod2}</td></tr>' if mod2 else ''}\n",
        "                <tr><td><b>Valid Groups:</b></td><td>{len(valid_groups)}</td></tr>\n",
        "                <tr><td><b>Data Retained:</b></td><td>{len(retained_data)}/{len(analysis_data)} ({retention_pct:.1f}%)</td></tr>\n",
        "            </table>\n",
        "            <p style='margin:10px 0 0 0; color:#155724; font-size:14px;'><b>✅ Ready! Proceed to the next cell to run the subgroup analysis.</b></p></div>\"\"\"\n",
        "        display(HTML(success_html))\n",
        "\n",
        "        with tab_details:\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"✓ CONFIGURATION SAVED\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "            print(\"\\nConfiguration stored in ANALYSIS_CONFIG['subgroup_config']\")\n",
        "            print(\"Ready to proceed to subgroup analysis execution.\")\n",
        "\n",
        "# --- 6. INITIALIZE AND DISPLAY ---\n",
        "try:\n",
        "    available_mods = initialize_configuration()\n",
        "    update_all_tabs()\n",
        "    display(tabs)\n",
        "except Exception as e:\n",
        "    print(f\"❌ Initialization failed: {e}\")\n",
        "    print(\"\\nPlease ensure:\")\n",
        "    print(\"  1. Step 2 (Overall Meta-Analysis) has been run\")\n",
        "    print(\"  2. ANALYSIS_CONFIG is properly configured\")\n",
        "    print(\"  3. analysis_data is available\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mVbKzKpRjeOi"
      },
      "source": [
        "#@title 🔬 Subgroup Analysis - Execution\n",
        "\n",
        "# =============================================================================\n",
        "# CELL: SUBGROUP ANALYSIS WITH DASHBOARD\n",
        "# Purpose: Run three-level meta-analysis for each subgroup with organized output.\n",
        "# Enhancement: Uses tabbed interface for better readability.\n",
        "# Dependencies: Step 2 (Overall Meta-Analysis), Step 3a (Configuration)\n",
        "# Compatible with: Maintains same output structure as original for downstream cells\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from scipy.optimize import minimize, minimize_scalar\n",
        "from scipy.stats import norm, chi2\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import sys\n",
        "import traceback\n",
        "import warnings\n",
        "\n",
        "# --- 0. HELPER FUNCTIONS (FROM PREVIOUS CELLS) ---\n",
        "# Note: This cell expects calculate_tau_squared, _negative_log_likelihood_reml,\n",
        "# and _get_three_level_estimates to be defined from previous cells (4.5, 6.5)\n",
        "\n",
        "# If not already defined, provide fallbacks\n",
        "if '_negative_log_likelihood_reml' not in dir():\n",
        "    def _negative_log_likelihood_reml(params, y_all, vcv_all, N_total, M_studies):\n",
        "        \"\"\"Placeholder - should be defined in Cell 6.5\"\"\"\n",
        "        raise NotImplementedError(\"Please run Cell 6.5 first to define this function\")\n",
        "\n",
        "if '_get_three_level_estimates' not in dir():\n",
        "    def _get_three_level_estimates(params, y_all, vcv_all, N_total, M_studies):\n",
        "        \"\"\"Placeholder - should be defined in Cell 6.5\"\"\"\n",
        "        raise NotImplementedError(\"Please run Cell 6.5 first to define this function\")\n",
        "\n",
        "if 'calculate_tau_squared' not in dir():\n",
        "    def calculate_tau_squared(df, effect_col, var_col, method='REML'):\n",
        "        \"\"\"Fallback - should be defined in Cell 4.5\"\"\"\n",
        "        # Simple DL estimator as fallback\n",
        "        k = len(df)\n",
        "        if k < 2: return 0.0, {}\n",
        "        yi = df[effect_col].values\n",
        "        vi = df[var_col].values\n",
        "        wi = 1/vi\n",
        "        mu = np.average(yi, weights=wi)\n",
        "        Q = np.sum(wi * (yi - mu)**2)\n",
        "        C = np.sum(wi) - np.sum(wi**2)/np.sum(wi)\n",
        "        tau2 = max(0, (Q - (k-1)) / C) if C > 0 else 0\n",
        "        return tau2, {}\n",
        "\n",
        "def _run_three_level_reml_for_subgroup(analysis_data, effect_col, var_col):\n",
        "    \"\"\"\n",
        "    Robust 3-Level Optimization for Subgroups with Graceful Degradation.\n",
        "\n",
        "    Execution Strategy:\n",
        "      - Plan A: Full 3-Level Model with VCV Matrices (handles shared controls)\n",
        "      - Plan B: 3-Level Model with Diagonal VCV (independence assumption)\n",
        "      - Plan C: 2-Level Model (single variance component, for very small N)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (estimates_dict, debug_tuple) or (None, None) on failure\n",
        "               estimates_dict includes 'model_type' indicating which plan succeeded\n",
        "    \"\"\"\n",
        "    # 0. Input validation\n",
        "    if analysis_data is None or len(analysis_data) == 0:\n",
        "        return None, None\n",
        "\n",
        "    # 1. Sort for deterministic alignment\n",
        "    analysis_data = analysis_data.sort_values('id').reset_index(drop=True)\n",
        "\n",
        "    grouped = analysis_data.groupby('id', sort=False)\n",
        "    y_all = [np.asarray(group[effect_col].values, dtype=np.float64) for _, group in grouped]\n",
        "\n",
        "    # Check for invalid data\n",
        "    if any(len(y) == 0 or np.any(np.isnan(y)) for y in y_all):\n",
        "        return None, None\n",
        "\n",
        "    N_total = len(analysis_data)\n",
        "    M_studies = len(y_all)\n",
        "\n",
        "    # Minimum requirements\n",
        "    if M_studies < 2:\n",
        "        return None, None\n",
        "\n",
        "    # Count studies with multiple observations (needed for sigma² identification)\n",
        "    n_multi_obs_studies = sum(1 for y in y_all if len(y) > 1)\n",
        "\n",
        "    # --- PREPARE VCV DATA FOR DIFFERENT PLANS ---\n",
        "    vcv_dict = ANALYSIS_CONFIG.get('vcv_matrices', {})\n",
        "\n",
        "    vcv_all_matrix = []  # Plan A: Full matrices\n",
        "    vcv_all_diag = []    # Plan B: Diagonal only\n",
        "    has_off_diagonal = False  # Track if we actually have shared controls\n",
        "\n",
        "    for study_id, group in grouped:\n",
        "        vi = np.asarray(group[var_col].values, dtype=np.float64)\n",
        "\n",
        "        # Safety check\n",
        "        if len(vi) == 0 or np.any(np.isnan(vi)) or np.any(vi <= 0):\n",
        "            return None, None\n",
        "\n",
        "        # Diagonal is always the fallback\n",
        "        diag_matrix = np.diag(vi)\n",
        "        vcv_all_diag.append(diag_matrix)\n",
        "\n",
        "        # Try to find full matrix\n",
        "        sid_str = str(study_id)\n",
        "        if sid_str in vcv_dict:\n",
        "            full_matrix = np.asarray(vcv_dict[sid_str], dtype=np.float64)\n",
        "            vcv_all_matrix.append(full_matrix)\n",
        "            if not np.allclose(full_matrix, np.diag(np.diag(full_matrix))):\n",
        "                has_off_diagonal = True\n",
        "        elif study_id in vcv_dict:\n",
        "            full_matrix = np.asarray(vcv_dict[study_id], dtype=np.float64)\n",
        "            vcv_all_matrix.append(full_matrix)\n",
        "            if not np.allclose(full_matrix, np.diag(np.diag(full_matrix))):\n",
        "                has_off_diagonal = True\n",
        "        else:\n",
        "            vcv_all_matrix.append(diag_matrix)\n",
        "\n",
        "    # --- OPTIMIZER DEFINITIONS ---\n",
        "\n",
        "    def run_three_level_optimizer(vcv_input):\n",
        "        \"\"\"Run 3-level model (2 variance components: tau², sigma²).\"\"\"\n",
        "        start_points = [\n",
        "            [0.01, 0.01], [0.1, 0.1], [0.5, 0.5],\n",
        "            [1.0, 1.0], [1.0, 0.1], [0.1, 1.0]\n",
        "        ]\n",
        "\n",
        "        # Smart start from DL estimate\n",
        "        try:\n",
        "            tau_sq_dl, _ = calculate_tau_squared(analysis_data, effect_col, var_col, method='DL')\n",
        "            if tau_sq_dl is not None and tau_sq_dl > 0:\n",
        "                start_points.insert(0, [tau_sq_dl, 0.01])\n",
        "                start_points.insert(1, [tau_sq_dl, tau_sq_dl * 0.5])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        best_res = None\n",
        "        best_fun = np.inf\n",
        "\n",
        "        for start in start_points:\n",
        "            try:\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.simplefilter(\"ignore\")\n",
        "                    \"\"\"\n",
        "                    res = minimize(\n",
        "                        _negative_log_likelihood_reml,\n",
        "                        x0=start,\n",
        "                        args=(y_all, vcv_input, N_total, M_studies),\n",
        "                        method='L-BFGS-B',\n",
        "                        bounds=[(1e-8, 50.0), (1e-8, 50.0)],  # Upper bound for stability\n",
        "                        options={'ftol': 1e-11, 'maxiter': 1000}\n",
        "                    )\n",
        "                    \"\"\"\n",
        "                    res = minimize(\n",
        "                      _negative_log_likelihood_reml,\n",
        "                      x0=start,\n",
        "                      args=(y_all, vcv_input, N_total, M_studies),\n",
        "                      method='L-BFGS-B',\n",
        "                      bounds=[(1e-8, 50.0), (1e-8, 50.0)],\n",
        "                      options={\n",
        "                          'ftol': 1e-12,      # Tighter tolerance\n",
        "                          'gtol': 1e-10,      # Gradient tolerance\n",
        "                          'maxiter': 5000,    # More iterations\n",
        "                          'maxfun': 10000     # More function evaluations\n",
        "                      }\n",
        "                      )\n",
        "                if res.success and np.isfinite(res.fun) and res.fun < best_fun:\n",
        "                    best_fun = res.fun\n",
        "                    best_res = res\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        # Fallback: BFGS with transformed parameters (log-space for positivity)\n",
        "        if best_res is None:\n",
        "            try:\n",
        "                def neg_ll_log_params(log_params, *args):\n",
        "                    params = np.exp(log_params)  # Transform back\n",
        "                    return _negative_log_likelihood_reml(params, *args)\n",
        "\n",
        "                res = minimize(\n",
        "                    neg_ll_log_params,\n",
        "                    x0=np.log([0.1, 0.1]),\n",
        "                    args=(y_all, vcv_input, N_total, M_studies),\n",
        "                    method='BFGS',\n",
        "                    options={'gtol': 1e-8}\n",
        "                )\n",
        "                if res.success and np.isfinite(res.fun):\n",
        "                    # Transform result back\n",
        "                    res.x = np.exp(res.x)\n",
        "                    best_res = res\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return best_res\n",
        "\n",
        "    def run_two_level_optimizer(vcv_input):\n",
        "        \"\"\"\n",
        "        Run 2-level model (single variance component: tau² only, sigma²=0).\n",
        "        This is appropriate when all studies have single observations,\n",
        "        or when 3-level model fails to converge.\n",
        "        \"\"\"\n",
        "        start_points = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
        "\n",
        "        # Smart start\n",
        "        try:\n",
        "            tau_sq_dl, _ = calculate_tau_squared(analysis_data, effect_col, var_col, method='DL')\n",
        "            if tau_sq_dl is not None and tau_sq_dl > 0:\n",
        "                start_points.insert(0, tau_sq_dl)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        def neg_ll_two_level(tau_sq_scalar, y_all, vcv_input, N_total, M_studies):\n",
        "            \"\"\"Wrapper that fixes sigma²=0.\"\"\"\n",
        "            params = np.array([tau_sq_scalar[0], 1e-10])  # sigma² ≈ 0\n",
        "            return _negative_log_likelihood_reml(params, y_all, vcv_input, N_total, M_studies)\n",
        "\n",
        "        best_res = None\n",
        "        best_fun = np.inf\n",
        "\n",
        "        for start in start_points:\n",
        "            try:\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.simplefilter(\"ignore\")\n",
        "                    res = minimize(\n",
        "                        neg_ll_two_level,\n",
        "                        x0=[start],\n",
        "                        args=(y_all, vcv_input, N_total, M_studies),\n",
        "                        method='L-BFGS-B',\n",
        "                        bounds=[(1e-8, 50.0)],\n",
        "                        options={'ftol': 1e-11}\n",
        "                    )\n",
        "                if res.success and np.isfinite(res.fun) and res.fun < best_fun:\n",
        "                    best_fun = res.fun\n",
        "                    # Expand result to 2 parameters for consistency\n",
        "                    res.x = np.array([res.x[0], 1e-10])\n",
        "                    best_res = res\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        return best_res\n",
        "\n",
        "    # --- EXECUTION STRATEGY ---\n",
        "\n",
        "    best_res = None\n",
        "    final_vcv = None\n",
        "    model_type = None\n",
        "\n",
        "    # Determine if 3-level model is identifiable\n",
        "    # Need at least some studies with multiple observations for sigma² to be estimable\n",
        "    three_level_viable = n_multi_obs_studies >= 1 and N_total > M_studies\n",
        "\n",
        "    # Plan A: Full 3-Level with VCV Matrices (only if we have off-diagonal elements)\n",
        "    if three_level_viable and has_off_diagonal:\n",
        "        best_res = run_three_level_optimizer(vcv_all_matrix)\n",
        "        if best_res is not None:\n",
        "            final_vcv = vcv_all_matrix\n",
        "            model_type = '3-level-vcv'\n",
        "\n",
        "    # Plan B: 3-Level with Diagonal (independence assumption)\n",
        "    if best_res is None and three_level_viable:\n",
        "        best_res = run_three_level_optimizer(vcv_all_diag)\n",
        "        if best_res is not None:\n",
        "            final_vcv = vcv_all_diag\n",
        "            model_type = '3-level-diag'\n",
        "\n",
        "    # Plan C: 2-Level Model (single variance component)\n",
        "    if best_res is None:\n",
        "        best_res = run_two_level_optimizer(vcv_all_diag)\n",
        "        if best_res is not None:\n",
        "            final_vcv = vcv_all_diag\n",
        "            model_type = '2-level'\n",
        "\n",
        "    # All plans failed\n",
        "    if best_res is None:\n",
        "        return None, None\n",
        "\n",
        "    # --- FINAL CALCULATION ---\n",
        "    try:\n",
        "        final_estimates = _get_three_level_estimates(\n",
        "            best_res.x, y_all, final_vcv, N_total, M_studies\n",
        "        )\n",
        "\n",
        "        # Add metadata about which model was used\n",
        "        final_estimates['model_type'] = model_type\n",
        "        final_estimates['n_studies'] = M_studies\n",
        "        final_estimates['n_observations'] = N_total\n",
        "        final_estimates['n_multi_obs_studies'] = n_multi_obs_studies\n",
        "        final_estimates['optimizer_success'] = best_res.success\n",
        "        final_estimates['optimizer_message'] = getattr(best_res, 'message', '')\n",
        "\n",
        "        return final_estimates, (y_all, final_vcv, N_total, M_studies)\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, None\n",
        "\n",
        "    # --- EXECUTION STRATEGY ---\n",
        "\n",
        "    # Attempt 1: Plan A (Full Matrix)\n",
        "    best_res = run_optimizer(vcv_all_matrix)\n",
        "    final_vcv = vcv_all_matrix\n",
        "\n",
        "    # Attempt 2: Plan B (Diagonal Fallback) if Plan A failed\n",
        "    if best_res is None:\n",
        "        # print(\"⚠️ Matrix optimization failed. Falling back to diagonal independence model.\")\n",
        "        best_res = run_optimizer(vcv_all_diag)\n",
        "        final_vcv = vcv_all_diag\n",
        "\n",
        "    if best_res is None:\n",
        "        return None, None\n",
        "\n",
        "    # Final Calculation\n",
        "    try:\n",
        "        final_estimates = _get_three_level_estimates(\n",
        "            best_res.x, y_all, final_vcv, N_total, M_studies\n",
        "        )\n",
        "        return final_estimates, (y_all, final_vcv, N_total, M_studies)\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "\n",
        "# --- 0.5 PUBLICATION TEXT GENERATOR FOR SUBGROUPS ---\n",
        "def generate_subgroup_publication_text(results_df, moderator1, moderator2, QM, df_QM, p_value_QM,\n",
        "                                       R_squared, Qt_overall, QE_sum, df_QE):\n",
        "    \"\"\"Generate publication-ready text for subgroup analysis\"\"\"\n",
        "\n",
        "    M_groups = len(results_df)\n",
        "    sig_QM = \"significant\" if p_value_QM < 0.05 else \"non-significant\"\n",
        "    p_format_QM = f\"< 0.001\" if p_value_QM < 0.001 else f\"= {p_value_QM:.3f}\"\n",
        "\n",
        "    # Get confidence level from global settings\n",
        "    if 'ANALYSIS_CONFIG' in globals():\n",
        "        global_settings = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "        alpha = global_settings.get('alpha', 0.05)\n",
        "        ci_pct = (1 - alpha) * 100\n",
        "    else:\n",
        "        ci_pct = 95  # Default to 95% if ANALYSIS_CONFIG not available\n",
        "\n",
        "    # R² interpretation\n",
        "    if R_squared < 25:\n",
        "        r2_interp = \"low R² value suggests that this moderator explains only a small proportion of heterogeneity, and other unmeasured factors likely contribute to the observed variation in effect sizes\"\n",
        "    elif R_squared < 50:\n",
        "        r2_interp = \"moderate R² value indicates that this moderator partially explains the heterogeneity, though substantial unexplained variation remains\"\n",
        "    else:\n",
        "        r2_interp = \"high R² value indicates that this moderator is a substantial source of heterogeneity in the meta-analysis\"\n",
        "\n",
        "    # Build text\n",
        "    text = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff;'>\n",
        "\n",
        "<h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px;'>Subgroup Analysis Results</h3>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "To explore sources of heterogeneity, we conducted a subgroup analysis based on <b>{moderator1}</b>\"\"\"\n",
        "\n",
        "    if moderator2:\n",
        "        text += f\"\"\" and <b>{moderator2}</b>. We examined the interaction between these two moderators\"\"\"\n",
        "\n",
        "    text += f\"\"\". The dataset included <b>{M_groups}</b> subgroups with sufficient data for analysis (minimum of 2 studies per subgroup).\n",
        "</p>\n",
        "\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Overall Test for Subgroup Differences</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "The test for subgroup differences was <b>{sig_QM}</b> (<i>Q</i><sub>M</sub>({df_QM}) = <b>{QM:.2f}</b>, <i>p</i> {p_format_QM}), \"\"\"\n",
        "\n",
        "    if p_value_QM < 0.05:\n",
        "        text += f\"\"\"indicating that the moderator variable significantly explained variation in effect sizes across studies. The moderator accounted for <b>{R_squared:.1f}%</b> of the total heterogeneity (R² = {R_squared:.1f}%).\n",
        "</p>\n",
        "\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Heterogeneity Partitioning</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "Heterogeneity was partitioned into between-group (<i>Q</i><sub>M</sub>({df_QM}) = {QM:.2f}) and within-group components (<i>Q</i><sub>E</sub>({df_QE}) = {QE_sum:.2f}) from the total heterogeneity (<i>Q</i><sub>T</sub>({Qt_overall - 1:.0f}) = {Qt_overall:.2f}). The {r2_interp}.\n",
        "</p>\n",
        "\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Individual Subgroup Results</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "Results by subgroup were as follows (Table 1):\n",
        "</p>\n",
        "\n",
        "<ul style='line-height: 2.0;'>\n",
        "\"\"\"\n",
        "    else:\n",
        "        text += f\"\"\"suggesting that the moderator variable did not significantly explain variation in effect sizes across studies. The moderator accounted for only <b>{R_squared:.1f}%</b> of the total heterogeneity (R² = {R_squared:.1f}%).\n",
        "</p>\n",
        "\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Heterogeneity Partitioning</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "Heterogeneity was partitioned into between-group (<i>Q</i><sub>M</sub>({df_QM}) = {QM:.2f}) and within-group components (<i>Q</i><sub>E</sub>({df_QE}) = {QE_sum:.2f}) from the total heterogeneity (<i>Q</i><sub>T</sub>({Qt_overall - 1:.0f}) = {Qt_overall:.2f}). The {r2_interp}.\n",
        "</p>\n",
        "\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Individual Subgroup Results</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "Results by subgroup were as follows (Table 1):\n",
        "</p>\n",
        "\n",
        "<ul style='line-height: 2.0;'>\n",
        "\"\"\"\n",
        "\n",
        "    # Individual subgroup results\n",
        "    for _, row in results_df.iterrows():\n",
        "        group_name = row['group']\n",
        "        k = int(row['k'])\n",
        "        n_papers = int(row['n_papers'])\n",
        "        effect = row['pooled_effect_re']\n",
        "        ci_l = row['ci_lower_re']\n",
        "        ci_h = row['ci_upper_re']\n",
        "        p_val = row['p_value_re']\n",
        "        I2 = row['I_squared']\n",
        "        tau2 = row['tau_squared']\n",
        "        sigma2 = row['sigma_squared']\n",
        "\n",
        "        sig_text = \"significant\" if p_val < 0.05 else \"non-significant\"\n",
        "        p_format = f\"< 0.001\" if p_val < 0.001 else f\"= {p_val:.3f}\"\n",
        "\n",
        "        het_text = \"with\" if I2 >= 50 else \"without\"\n",
        "\n",
        "        text += f\"\"\"<li><b>{group_name}:</b> Based on {k} effect sizes from {n_papers} studies, the pooled effect was <b>{effect:.3f}</b> ({ci_pct:.0f}% CI [{ci_l:.3f}, {ci_h:.3f}], <i>p</i> {p_format}), {het_text} substantial heterogeneity (<i>I</i>² = {I2:.1f}%, τ² = {tau2:.4f}, σ² = {sigma2:.4f}).</li>\n",
        "\"\"\"\n",
        "\n",
        "    text += \"</ul>\"\n",
        "\n",
        "    # Comparative statements\n",
        "    max_effect_row = results_df.loc[results_df['pooled_effect_re'].idxmax()]\n",
        "    min_effect_row = results_df.loc[results_df['pooled_effect_re'].idxmin()]\n",
        "\n",
        "    text += f\"\"\"\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Comparative Interpretation</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "The largest effect was observed for <b>{max_effect_row['group']}</b> ({max_effect_row['pooled_effect_re']:.3f}, {ci_pct:.0f}% CI [{max_effect_row['ci_lower_re']:.3f}, {max_effect_row['ci_upper_re']:.3f}])\"\"\"\n",
        "\n",
        "    if M_groups > 1:\n",
        "        text += f\"\"\", while <b>{min_effect_row['group']}</b> showed the {'smallest' if min_effect_row['pooled_effect_re'] > 0 else 'most negative'} effect ({min_effect_row['pooled_effect_re']:.3f}, {ci_pct:.0f}% CI [{min_effect_row['ci_lower_re']:.3f}, {min_effect_row['ci_upper_re']:.3f}])\"\"\"\n",
        "\n",
        "    text += \".\"\n",
        "    text += \"</p>\"\n",
        "\n",
        "    # Interpretation based on Q_M significance\n",
        "    if p_value_QM < 0.05:\n",
        "        text += f\"\"\"\n",
        "<p style='text-align: justify;'>\n",
        "These results demonstrate that <b>{moderator1}</b>\"\"\"\n",
        "        if moderator2:\n",
        "            text += f\"\"\" and <b>{moderator2}</b>\"\"\"\n",
        "        text += f\"\"\" is an important moderator of the outcome, with differential effects observed across subgroups. [<i>Add mechanistic explanation or theoretical context specific to your research domain</i>]\n",
        "</p>\n",
        "\"\"\"\n",
        "    else:\n",
        "        text += f\"\"\"\n",
        "<p style='text-align: justify;'>\n",
        "Although numerical differences were observed among subgroups, these differences were not statistically significant. This suggests that <b>{moderator1}</b>\"\"\"\n",
        "        if moderator2:\n",
        "            text += f\"\"\" and <b>{moderator2}</b>\"\"\"\n",
        "        text += f\"\"\" may not be a primary driver of heterogeneity in this meta-analysis, or that insufficient statistical power limits our ability to detect subgroup differences. [<i>Consider discussing alternative explanations or limitations</i>]\n",
        "</p>\n",
        "\"\"\"\n",
        "\n",
        "    # Within-subgroup heterogeneity\n",
        "    avg_I2 = results_df['I_squared'].mean()\n",
        "\n",
        "    text += f\"\"\"\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Within-Subgroup Heterogeneity</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "\"\"\"\n",
        "\n",
        "    if avg_I2 >= 50:\n",
        "        text += f\"\"\"Substantial heterogeneity remained within subgroups on average (<i>Q</i><sub>E</sub> = {QE_sum:.2f}), indicating that additional moderators not examined in this analysis likely contribute to variation in effect sizes. Future research should investigate [<i>suggest other potential moderators based on your domain knowledge</i>].\n",
        "\"\"\"\n",
        "    else:\n",
        "        text += f\"\"\"Residual heterogeneity within subgroups was low to moderate on average, suggesting that the moderator variable successfully captured much of the systematic variation in effect sizes.\n",
        "\"\"\"\n",
        "\n",
        "    text += \"\"\"</p>\n",
        "\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Statistical Methods</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "Each subgroup analysis employed a three-level random-effects model to account for the nested structure of effect sizes within studies, providing robust estimates that accommodate within-study dependencies. All analyses were conducted using [<i>specify your software/package</i>].\n",
        "</p>\n",
        "\n",
        "<hr style='margin: 30px 0; border: none; border-top: 1px solid #bdc3c7;'>\n",
        "\n",
        "<div style='background-color: #ecf0f1; padding: 20px; border-left: 4px solid #3498db; margin-top: 25px;'>\n",
        "<h4 style='margin-top: 0; color: #2c3e50;'>📊 Table 1. Summary of Subgroup Analysis Results</h4>\n",
        "<table style='width: 100%; border-collapse: collapse; margin-top: 15px; background-color: white;'>\n",
        "<thead style='background-color: #34495e; color: white;'>\n",
        "<tr>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: left;'>Subgroup</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>k</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>Studies</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>Effect</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>{ci_pct:.0f}% CI</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'><i>p</i>-value</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'><i>I</i>²</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "\"\"\"\n",
        "\n",
        "    for idx, row in results_df.iterrows():\n",
        "        bg_color = \"#f8f9fa\" if idx % 2 == 0 else \"white\"\n",
        "        sig_style = \"font-weight: bold;\" if row['p_value_re'] < 0.05 else \"\"\n",
        "\n",
        "        text += f\"\"\"<tr style='background-color: {bg_color};'>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>{row['group']}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{int(row['k'])}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{int(row['n_papers'])}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center; {sig_style}'>{row['pooled_effect_re']:.3f}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>[{row['ci_lower_re']:.3f}, {row['ci_upper_re']:.3f}]</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{row['p_value_re']:.3g}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{row['I_squared']:.1f}%</td>\n",
        "</tr>\n",
        "\"\"\"\n",
        "\n",
        "    text += f\"\"\"</tbody>\n",
        "</table>\n",
        "<p style='margin-top: 10px; font-size: 0.9em; color: #6c757d;'><i>Note:</i> k = number of effect sizes; Studies = number of independent studies; CI = confidence interval; <i>I</i>² = heterogeneity statistic.</p>\n",
        "</div>\n",
        "\n",
        "<hr style='margin: 30px 0; border: none; border-top: 1px solid #bdc3c7;'>\n",
        "\n",
        "<div style='background-color: #ecf0f1; padding: 15px; border-left: 4px solid #3498db; margin-top: 20px;'>\n",
        "<h4 style='margin-top: 0; color: #2c3e50;'>Interpretation Guidance:</h4>\n",
        "<ul style='margin-bottom: 0;'>\n",
        "<li>Customize subgroup descriptions based on your specific moderator variables and research context</li>\n",
        "<li>Add domain-specific interpretations of why certain subgroups show different effects</li>\n",
        "<li>Include relevant post-hoc pairwise comparisons if appropriate for your analysis</li>\n",
        "<li>Discuss potential confounding factors or limitations (e.g., unbalanced sample sizes across subgroups)</li>\n",
        "<li>Link findings to your theoretical framework or prior research in the field</li>\n",
        "<li>Consider conducting sensitivity analyses to test the robustness of subgroup differences</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "<div style='background-color: #fff3cd; padding: 10px; border-left: 4px solid #ffc107; margin-top: 15px;'>\n",
        "<p style='margin: 0;'><b>💡 Tip:</b> Select all text (Ctrl+A / Cmd+A), copy (Ctrl+C / Cmd+C), and paste into your word processor. Most formatting will be preserved. Edit the [<i>bracketed notes</i>] to add your specific interpretations.</p>\n",
        "</div>\n",
        "\n",
        "</div>\"\"\"\n",
        "\n",
        "    return text\n",
        "\n",
        "# --- 1. LAYOUT & WIDGETS ---\n",
        "tab_results = widgets.Output()\n",
        "tab_hetero = widgets.Output()\n",
        "tab_details = widgets.Output()\n",
        "tab_config = widgets.Output()\n",
        "tab_publication = widgets.Output()\n",
        "\n",
        "tab_export = widgets.Output()\n",
        "tabs = widgets.Tab(children=[tab_results, tab_hetero, tab_details, tab_config, tab_publication, tab_export])\n",
        "\n",
        "# Set titles (update indices as needed)\n",
        "tabs.set_title(0, '📊 Results Summary')\n",
        "tabs.set_title(1, '📉 Heterogeneity')\n",
        "tabs.set_title(2, '🔍 Subgroup Details')\n",
        "tabs.set_title(3, '⚙️ Configuration')\n",
        "tabs.set_title(4, '📝 Publication Text')\n",
        "tabs.set_title(5, '💾 Export')\n",
        "\n",
        "def generate_methods_text_subgroup(es_config, subgroup_results):\n",
        "    \"\"\"\n",
        "    Generates a 'Materials and Methods' section for Subgroup Analysis.\n",
        "    \"\"\"\n",
        "    import sys\n",
        "\n",
        "    # 1. Extract Settings\n",
        "    mod1 = subgroup_results.get('moderator1', 'Moderator')\n",
        "    mod2 = subgroup_results.get('moderator2')\n",
        "    n_groups = len(subgroup_results.get('results_df', []))\n",
        "\n",
        "    # 2. Define Citation Database\n",
        "    db = {\n",
        "        'borenstein': \"Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2009). <i>Introduction to Meta-Analysis</i>. Chichester, UK: John Wiley & Sons.\",\n",
        "        'viechtbauer': \"Viechtbauer, W. (2010). Conducting meta-analyses in R with the metafor package. <i>Journal of Statistical Software</i>, 36(3), 1-48.\",\n",
        "        'q_test': \"Cochran, W. G. (1954). The combination of estimates from different experiments. <i>Biometrics</i>, 10(1), 101-129.\",\n",
        "        'r2': \"Raudenbush, S. W. (2009). Analyzing effect sizes: Random-effects models. In H. Cooper, L. V. Hedges, & J. C. Valentine (Eds.), <i>The Handbook of Research Synthesis and Meta-Analysis</i> (2nd ed., pp. 295-315). New York: Russell Sage Foundation.\",\n",
        "        'tool': \"<b>[CITATION FOR THIS TOOL]:</b> Author, A. A. (202X). <i>Co-Meta: An Interactive Pipeline for Meta-Analysis</i>. [Repository/DOI].\"\n",
        "    }\n",
        "\n",
        "    # 3. Build HTML\n",
        "    html = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff; border: 1px solid #eee; margin-bottom: 20px;'>\n",
        "\n",
        "    <h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-top: 0;'>Materials and Methods</h3>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Subgroup Analysis.</b> To investigate potential sources of heterogeneity, we conducted a mixed-effects subgroup analysis [1].\n",
        "    The categorical moderator(s) tested were <b>{mod1}</b>\"\"\"\n",
        "\n",
        "    if mod2:\n",
        "        html += f\" and <b>{mod2}</b>\"\n",
        "\n",
        "    html += f\"\"\". The analysis partitioned the total heterogeneity into within-group variance and between-group variance.\n",
        "    </p>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Statistical Tests.</b> Differences between subgroups were assessed using the omnibus <i>Q</i>-test for moderation ($Q_M$) [2].\n",
        "    A significant $Q_M$ indicates that the effect sizes vary systematically across the categories of the moderator.\n",
        "    We also calculated the $R^2$ statistic [3] to quantify the proportion of between-study variance explained by the moderator.\n",
        "    </p>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Model Specification.</b> A separate random-effects model was fitted within each of the {n_groups} subgroups to estimate group-specific pooled effects and heterogeneity ($I^2$, $\\\\tau^2$).\n",
        "    Common between-study variance was not assumed; each subgroup was allowed to have its own heterogeneity estimate.\n",
        "    All computations were performed using the Co-Meta toolkit [4].\n",
        "    </p>\n",
        "\n",
        "    <h4 style='color: #34495e; margin-top: 20px; font-size: 11pt;'>References</h4>\n",
        "    <ol style='font-size: 10pt; color: #555;'>\n",
        "        <li>{db['borenstein']}</li>\n",
        "        <li>{db['q_test']}</li>\n",
        "        <li>{db['r2']}</li>\n",
        "        <li>{db['tool']}</li>\n",
        "    </ol>\n",
        "    </div>\"\"\"\n",
        "\n",
        "    return html\n",
        "\n",
        "# --- 2. MAIN ANALYSIS FUNCTION ---\n",
        "def run_subgroup_analysis():\n",
        "    \"\"\"Main analysis engine that populates all tabs\"\"\"\n",
        "\n",
        "    # Clear all tabs\n",
        "    for tab in [tab_results, tab_hetero, tab_details, tab_config, tab_publication]:\n",
        "        tab.clear_output()\n",
        "\n",
        "    # --- TAB 4: CONFIGURATION (Display first for context) ---\n",
        "    with tab_config:\n",
        "        display(HTML(\"<h3>⚙️ Analysis Configuration</h3>\"))\n",
        "\n",
        "        try:\n",
        "            if 'ANALYSIS_CONFIG' not in globals():\n",
        "                display(HTML(\"<div style='color: red;'>❌ ANALYSIS_CONFIG not found. Run previous cells first.</div>\"))\n",
        "                return\n",
        "\n",
        "            # Load configuration\n",
        "            config_html = \"<div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 10px 0;'>\"\n",
        "            config_html += f\"<b>Timestamp:</b> {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br>\"\n",
        "\n",
        "            if 'subgroup_config' in ANALYSIS_CONFIG:\n",
        "                sc = ANALYSIS_CONFIG['subgroup_config']\n",
        "                config_html += f\"<b>Analysis Type:</b> {sc['analysis_type']}<br>\"\n",
        "                config_html += f\"<b>Moderator 1:</b> {sc['moderator1']}<br>\"\n",
        "                if sc.get('moderator2'):\n",
        "                    config_html += f\"<b>Moderator 2:</b> {sc['moderator2']}<br>\"\n",
        "                config_html += f\"<b>Number of Subgroups:</b> {len(sc['valid_groups_list'])}<br>\"\n",
        "\n",
        "            if 'effect_col' in ANALYSIS_CONFIG:\n",
        "                config_html += f\"<b>Effect Column:</b> {ANALYSIS_CONFIG['effect_col']}<br>\"\n",
        "                config_html += f\"<b>Variance Column:</b> {ANALYSIS_CONFIG['var_col']}<br>\"\n",
        "\n",
        "            config_html += \"</div>\"\n",
        "            display(HTML(config_html))\n",
        "\n",
        "            # Check prerequisites\n",
        "            required = ['overall_results', 'three_level_results', 'subgroup_config']\n",
        "            missing = [k for k in required if k not in ANALYSIS_CONFIG]\n",
        "\n",
        "            if missing:\n",
        "                display(HTML(f\"<div style='color: red;'>❌ Missing: {', '.join(missing)}</div>\"))\n",
        "                display(HTML(\"<p>Please run:</p><ul><li>Step 2: Overall Meta-Analysis</li><li>Step 3a: Subgroup Configuration</li></ul>\"))\n",
        "                return\n",
        "\n",
        "            display(HTML(\"<div style='color: green;'>✅ All prerequisites met</div>\"))\n",
        "\n",
        "        except Exception as e:\n",
        "            display(HTML(f\"<div style='color: red;'>❌ Configuration Error: {e}</div>\"))\n",
        "            return\n",
        "\n",
        "# --- MAIN ANALYSIS ---\n",
        "    try:\n",
        "        # Load data\n",
        "        if 'analysis_data' in globals():\n",
        "            analysis_data = globals()['analysis_data'].copy()\n",
        "        elif 'data_filtered' in globals():\n",
        "            analysis_data = globals()['data_filtered'].copy()\n",
        "        else:\n",
        "            with tab_results:\n",
        "                display(HTML(\"<div style='color: red;'>❌ Data not found</div>\"))\n",
        "            return\n",
        "\n",
        "        # Load configuration\n",
        "        effect_col = ANALYSIS_CONFIG['effect_col']\n",
        "        var_col = ANALYSIS_CONFIG['var_col']\n",
        "        es_config = ANALYSIS_CONFIG['es_config']\n",
        "        overall_results = ANALYSIS_CONFIG['overall_results']\n",
        "        subgroup_config = ANALYSIS_CONFIG['subgroup_config']\n",
        "\n",
        "        # --- Load Global Settings  ---\n",
        "        global_settings = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "        alpha = global_settings.get('alpha', 0.05)\n",
        "        dist_type = global_settings.get('dist_type', 'norm')\n",
        "        ci_pct = (1 - alpha) * 100\n",
        "\n",
        "        analysis_type = subgroup_config['analysis_type']\n",
        "        moderator1 = subgroup_config['moderator1']\n",
        "        moderator2 = subgroup_config.get('moderator2')\n",
        "        valid_groups_list = subgroup_config['valid_groups_list']\n",
        "\n",
        "        # Clean moderator columns\n",
        "        analysis_data[moderator1] = analysis_data[moderator1].astype(str).str.strip()\n",
        "        if moderator2:\n",
        "            analysis_data[moderator2] = analysis_data[moderator2].astype(str).str.strip()\n",
        "\n",
        "        # --- TAB 3: SUBGROUP DETAILS (Stream progress) ---\n",
        "        with tab_details:\n",
        "            display(HTML(\"<h3>🔍 Subgroup Analysis Progress</h3>\"))\n",
        "            details_output = widgets.Output()\n",
        "            display(details_output)\n",
        "\n",
        "        subgroup_results_list = []\n",
        "        total_Q_within_fe = 0.0\n",
        "\n",
        "        # Analyze each subgroup\n",
        "        for idx, group_item in enumerate(valid_groups_list, 1):\n",
        "            with tab_details:\n",
        "                with details_output:\n",
        "                    # Get group data\n",
        "                    if analysis_type == 'single':\n",
        "                        group_name = str(group_item)\n",
        "                        group_data = analysis_data[analysis_data[moderator1] == group_name].copy()\n",
        "                    else:\n",
        "                        group_tuple = group_item\n",
        "                        group_name = f\"{group_tuple[0]} x {group_tuple[1]}\"\n",
        "                        group_data = analysis_data[\n",
        "                            (analysis_data[moderator1] == group_tuple[0]) &\n",
        "                            (analysis_data[moderator2] == group_tuple[1])\n",
        "                        ].copy()\n",
        "\n",
        "                    print(f\"\\n{'='*60}\")\n",
        "                    print(f\"Subgroup {idx}/{len(valid_groups_list)}: {group_name}\")\n",
        "                    print(f\"{'='*60}\")\n",
        "\n",
        "                    k_group = len(group_data)\n",
        "                    n_papers_group = group_data['id'].nunique()\n",
        "                    print(f\"📊 Observations: {k_group} | Studies: {n_papers_group}\")\n",
        "\n",
        "                    # Run Robust Model (Plan A -> B -> C)\n",
        "                    print(\"🔄 Running optimization...\")\n",
        "                    output = _run_three_level_reml_for_subgroup(group_data, effect_col, var_col)\n",
        "\n",
        "                    if output is None or output[0] is None:\n",
        "                        print(\"❌ Optimization failed (Insufficient data)\")\n",
        "                        continue\n",
        "\n",
        "                    estimates, _ = output\n",
        "\n",
        "                    # Extract results\n",
        "                    mu_re = estimates['mu']\n",
        "                    se_re = estimates['se_mu']\n",
        "                    var_re = estimates['var_mu']\n",
        "                    model_used = estimates.get('model_type', 'Unknown')\n",
        "\n",
        "                    # Determine critical value\n",
        "                    q_val = 1 - (alpha / 2)\n",
        "                    df_sub = max(1, n_papers_group - 1)\n",
        "\n",
        "                    if dist_type == 't':\n",
        "                        crit_val = t.ppf(q_val, df_sub)\n",
        "                        p_value_re = 2 * (1 - t.cdf(abs(mu_re / se_re), df_sub))\n",
        "                    else:\n",
        "                        crit_val = norm.ppf(q_val)\n",
        "                        p_value_re = 2 * (1 - norm.cdf(abs(mu_re / se_re)))\n",
        "\n",
        "                    ci_lower_re = mu_re - crit_val * se_re\n",
        "                    ci_upper_re = mu_re + crit_val * se_re\n",
        "\n",
        "                    tau_sq_re = estimates['tau_sq']\n",
        "                    sigma_sq_re = estimates['sigma_sq']\n",
        "\n",
        "                    # Calculate I-squared\n",
        "                    mean_v_i = np.mean(group_data[var_col])\n",
        "                    total_variance_est = tau_sq_re + sigma_sq_re + mean_v_i\n",
        "                    I_squared_re = ((tau_sq_re + sigma_sq_re) / total_variance_est) * 100 if total_variance_est > 0 else 0\n",
        "\n",
        "                    # FE model for Q-statistics\n",
        "                    w_fe = 1 / group_data[var_col]\n",
        "                    sum_w_fe = w_fe.sum()\n",
        "                    pooled_effect_fe = (w_fe * group_data[effect_col]).sum() / sum_w_fe\n",
        "                    Q_within_group = (w_fe * (group_data[effect_col] - pooled_effect_fe)**2).sum()\n",
        "                    total_Q_within_fe += Q_within_group\n",
        "\n",
        "                    # Fold change\n",
        "                    if es_config.get('has_fold_change', False):\n",
        "                        RR = np.exp(mu_re)\n",
        "                        fold_change_re = RR if mu_re >= 0 else -1/RR\n",
        "                    else:\n",
        "                        fold_change_re = np.nan\n",
        "\n",
        "                    print(f\"✅ Effect: {mu_re:.3f} | Model: {model_used}\")\n",
        "\n",
        "                    # Store results\n",
        "                    result_dict = {\n",
        "                        'group': group_name,\n",
        "                        'k': k_group,\n",
        "                        'n_papers': n_papers_group,\n",
        "                        'pooled_effect_re': mu_re,\n",
        "                        'pooled_se_re': se_re,\n",
        "                        'pooled_var_re': var_re,\n",
        "                        'ci_lower_re': ci_lower_re,\n",
        "                        'ci_upper_re': ci_upper_re,\n",
        "                        'p_value_re': p_value_re,\n",
        "                        'I_squared': I_squared_re,\n",
        "                        'tau_squared': tau_sq_re,\n",
        "                        'sigma_squared': sigma_sq_re,\n",
        "                        'fold_change_re': fold_change_re,\n",
        "                        'Q_within': Q_within_group,\n",
        "                        'df_Q': k_group - 1,\n",
        "                        'model_type': model_used  # <--- Storing the Plan type\n",
        "                    }\n",
        "\n",
        "                    if analysis_type == 'two_way':\n",
        "                        result_dict[moderator1] = group_tuple[0]\n",
        "                        result_dict[moderator2] = group_tuple[1]\n",
        "\n",
        "                    subgroup_results_list.append(result_dict)\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results_df = pd.DataFrame(subgroup_results_list)\n",
        "\n",
        "        if results_df.empty:\n",
        "            with tab_results:\n",
        "                display(HTML(\"<div style='color: red;'>❌ No subgroups were successfully analyzed</div>\"))\n",
        "            return\n",
        "\n",
        "        # --- HETEROGENEITY PARTITIONING ---\n",
        "        Qt_overall = overall_results['Qt']\n",
        "        k_overall = overall_results['k']\n",
        "        Qe_sum = results_df['Q_within'].sum()\n",
        "        df_Qe = results_df['df_Q'].sum()\n",
        "        M_groups = len(results_df)\n",
        "        df_QM = M_groups - 1\n",
        "        QM = max(0, Qt_overall - Qe_sum)\n",
        "        p_value_QM = 1 - chi2.cdf(QM, df_QM) if df_QM > 0 else np.nan\n",
        "        R_squared = max(0, (QM / Qt_overall) * 100) if Qt_overall > 0 else 0\n",
        "\n",
        "        # --- TAB 1: RESULTS SUMMARY (UPDATED) ---\n",
        "        with tab_results:\n",
        "            display(HTML(\"<h3>📊 Subgroup Analysis Results</h3>\"))\n",
        "\n",
        "            # Summary stats\n",
        "            summary_html = \"<div style='background-color: #e7f3ff; padding: 15px; border-radius: 5px; margin: 10px 0;'>\"\n",
        "            summary_html += f\"<b>Moderator:</b> {moderator1}\"\n",
        "            if moderator2:\n",
        "                summary_html += f\" × {moderator2}\"\n",
        "            summary_html += f\"<br><b>Subgroups Analyzed:</b> {len(results_df)}<br>\"\n",
        "            summary_html += f\"<b>Test for Subgroup Differences:</b> Q<sub>M</sub> = {QM:.2f} (df={df_QM}, p = {p_value_QM:.4g})<br>\"\n",
        "            summary_html += f\"<b>Variance Explained (R²):</b> {R_squared:.1f}%\"\n",
        "            summary_html += \"</div>\"\n",
        "            display(HTML(summary_html))\n",
        "\n",
        "            # Results table\n",
        "            table_html = \"<table style='width: 100%; border-collapse: collapse; margin: 20px 0; font-size: 13px;'>\"\n",
        "            table_html += \"<thead style='background-color: #f8f9fa;'><tr>\"\n",
        "            table_html += \"<th style='border: 1px solid #dee2e6; padding: 8px; text-align: left;'>Subgroup</th>\"\n",
        "            table_html += \"<th style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>k</th>\"\n",
        "            table_html += \"<th style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>Studies</th>\"\n",
        "            table_html += \"<th style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>Effect</th>\"\n",
        "            table_html += \"<th style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>\" + f\"{ci_pct:.0f}% CI\" + \"</th>\"\n",
        "            table_html += \"<th style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>p-value</th>\"\n",
        "            table_html += \"<th style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>I²</th>\"\n",
        "            table_html += \"<th style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>Model*</th>\" # New Column\n",
        "            table_html += \"</tr></thead><tbody>\"\n",
        "\n",
        "            for _, row in results_df.iterrows():\n",
        "                sig = \"***\" if row['p_value_re'] < 0.001 else \"**\" if row['p_value_re'] < 0.01 else \"*\" if row['p_value_re'] < 0.05 else \"\"\n",
        "                sig_style = \"font-weight: bold; color: #28a745;\" if sig else \"\"\n",
        "\n",
        "                # Format Model Type for Display\n",
        "                mod_type = row.get('model_type', 'Unk')\n",
        "                if '3-level-vcv' in mod_type: mod_disp = \"3L-VCV\"\n",
        "                elif '3-level-diag' in mod_type: mod_disp = \"3L-Diag\"\n",
        "                elif '2-level' in mod_type: mod_disp = \"2L\"\n",
        "                else: mod_disp = mod_type\n",
        "\n",
        "                table_html += \"<tr>\"\n",
        "                table_html += f\"<td style='border: 1px solid #dee2e6; padding: 8px;'>{row['group']}</td>\"\n",
        "                table_html += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>{row['k']}</td>\"\n",
        "                table_html += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>{row['n_papers']}</td>\"\n",
        "                table_html += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center; {sig_style}'>{row['pooled_effect_re']:.3f} {sig}</td>\"\n",
        "                table_html += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>[{row['ci_lower_re']:.3f}, {row['ci_upper_re']:.3f}]</td>\"\n",
        "                table_html += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>{row['p_value_re']:.4g}</td>\"\n",
        "                table_html += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>{row['I_squared']:.1f}%</td>\"\n",
        "                table_html += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center; color: #666; font-size: 11px;'>{mod_disp}</td>\"\n",
        "                table_html += \"</tr>\"\n",
        "\n",
        "            table_html += \"</tbody></table>\"\n",
        "            display(HTML(table_html))\n",
        "\n",
        "            # Significance legend with Model explanation\n",
        "            legend = \"\"\"<div style='font-size: 0.9em; color: #555; margin-top: 10px; background-color: #f8f9fa; padding: 10px; border-radius: 4px;'>\n",
        "            <b>Legend:</b> *** p < 0.001; ** p < 0.01; * p < 0.05<br>\n",
        "            <b>Model Codes:</b><br>\n",
        "            • <b>3L-VCV (Plan A):</b> 3-Level model with full Variance-Covariance Matrix (Shared Controls).<br>\n",
        "            • <b>3L-Diag (Plan B):</b> 3-Level model assuming independence (fallback for stability).<br>\n",
        "            • <b>2L (Plan C):</b> 2-Level model (single variance component) used for very small subgroups.\n",
        "            </div>\"\"\"\n",
        "            display(HTML(legend))\n",
        "\n",
        "        # --- TAB 2: HETEROGENEITY ---\n",
        "        with tab_hetero:\n",
        "            display(HTML(\"<h3>📉 Heterogeneity Partitioning</h3>\"))\n",
        "\n",
        "            # Explanation\n",
        "            explanation = \"<div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 10px 0;'>\"\n",
        "            explanation += \"<p><b>Understanding Heterogeneity Decomposition:</b></p>\"\n",
        "            explanation += \"<ul>\"\n",
        "            explanation += \"<li><b>Q<sub>T</sub> (Total):</b> Overall heterogeneity across all studies</li>\"\n",
        "            explanation += \"<li><b>Q<sub>M</sub> (Between-Groups):</b> Heterogeneity explained by the moderator</li>\"\n",
        "            explanation += \"<li><b>Q<sub>E</sub> (Within-Groups):</b> Residual heterogeneity within subgroups</li>\"\n",
        "            explanation += \"<li><b>R²:</b> Proportion of total heterogeneity explained by the moderator</li>\"\n",
        "            explanation += \"</ul></div>\"\n",
        "            display(HTML(explanation))\n",
        "\n",
        "            # Q-statistics table\n",
        "            q_table = \"<table style='width: 100%; border-collapse: collapse; margin: 20px 0;'>\"\n",
        "            q_table += \"<thead style='background-color: #f8f9fa;'><tr>\"\n",
        "            q_table += \"<th style='border: 1px solid #dee2e6; padding: 8px; text-align: left;'>Component</th>\"\n",
        "            q_table += \"<th style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>Q</th>\"\n",
        "            q_table += \"<th style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>df</th>\"\n",
        "            q_table += \"<th style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>p-value</th>\"\n",
        "            q_table += \"</tr></thead><tbody>\"\n",
        "\n",
        "            q_table += \"<tr>\"\n",
        "            q_table += f\"<td style='border: 1px solid #dee2e6; padding: 8px;'><b>Total (Q<sub>T</sub>)</b></td>\"\n",
        "            q_table += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>{Qt_overall:.2f}</td>\"\n",
        "            q_table += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>{k_overall-1}</td>\"\n",
        "            q_table += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>—</td>\"\n",
        "            q_table += \"</tr>\"\n",
        "\n",
        "            sig_qm = \"***\" if p_value_QM < 0.001 else \"**\" if p_value_QM < 0.01 else \"*\" if p_value_QM < 0.05 else \"ns\"\n",
        "            sig_style = \"font-weight: bold; color: #28a745;\" if sig_qm != \"ns\" else \"\"\n",
        "\n",
        "            q_table += \"<tr style='background-color: #e7f3ff;'>\"\n",
        "            q_table += f\"<td style='border: 1px solid #dee2e6; padding: 8px;'><b>Between-Groups (Q<sub>M</sub>)</b></td>\"\n",
        "            q_table += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center; {sig_style}'>{QM:.2f}</td>\"\n",
        "            q_table += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>{df_QM}</td>\"\n",
        "            q_table += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center; {sig_style}'>{p_value_QM:.4g} {sig_qm}</td>\"\n",
        "            q_table += \"</tr>\"\n",
        "\n",
        "            q_table += \"<tr>\"\n",
        "            q_table += f\"<td style='border: 1px solid #dee2e6; padding: 8px;'><b>Within-Groups (Q<sub>E</sub>)</b></td>\"\n",
        "            q_table += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>{Qe_sum:.2f}</td>\"\n",
        "            q_table += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>{df_Qe}</td>\"\n",
        "            q_table += f\"<td style='border: 1px solid #dee2e6; padding: 8px; text-align: center;'>—</td>\"\n",
        "            q_table += \"</tr>\"\n",
        "\n",
        "            q_table += \"</tbody></table>\"\n",
        "            display(HTML(q_table))\n",
        "\n",
        "            # R-squared interpretation\n",
        "            r2_html = \"<div style='background-color: #fff3cd; padding: 15px; border-radius: 5px; margin: 10px 0;'>\"\n",
        "            r2_html += f\"<p style='margin: 0; font-size: 1.1em;'><b>Variance Explained (R²): {R_squared:.1f}%</b></p>\"\n",
        "            r2_html += f\"<p style='margin: 5px 0 0 0;'>The moderator <b>{moderator1}</b>\"\n",
        "            if moderator2:\n",
        "                r2_html += f\" × <b>{moderator2}</b>\"\n",
        "            r2_html += f\" explains {R_squared:.1f}% of the total heterogeneity.\"\n",
        "\n",
        "            if R_squared < 25:\n",
        "                r2_html += \" <span style='color: #856404;'>(Low explanatory power)</span>\"\n",
        "            elif R_squared < 50:\n",
        "                r2_html += \" <span style='color: #856404;'>(Moderate explanatory power)</span>\"\n",
        "            else:\n",
        "                r2_html += \" <span style='color: #155724;'>(High explanatory power)</span>\"\n",
        "\n",
        "            r2_html += \"</p></div>\"\n",
        "            display(HTML(r2_html))\n",
        "\n",
        "        # --- SAVE RESULTS ---\n",
        "        ANALYSIS_CONFIG['subgroup_results'] = {\n",
        "            'timestamp': datetime.datetime.now(),\n",
        "            'results_df': results_df,\n",
        "            'analysis_type': analysis_type,\n",
        "            'moderator1': moderator1,\n",
        "            'moderator2': moderator2,\n",
        "            'Qt_overall': Qt_overall,\n",
        "            'QM': QM,\n",
        "            'Qe': Qe_sum,\n",
        "            'df_QM': df_QM,\n",
        "            'df_Qe': df_Qe,\n",
        "            'p_value_QM': p_value_QM,\n",
        "            'R_squared': R_squared\n",
        "        }\n",
        "\n",
        "\n",
        "        # --- TAB 4: PUBLICATION TEXT ---\n",
        "        with tab_publication:\n",
        "            display(HTML(\"<h3 style='color: #2c3e50;'>📝 Publication-Ready Results Text</h3>\"))\n",
        "            display(HTML(\"<p style='color: #6c757d;'>Copy and paste this formatted text into your manuscript:</p>\"))\n",
        "\n",
        "            # A. Generate METHODS Text (New)\n",
        "            methods_html = generate_methods_text_subgroup(es_config, ANALYSIS_CONFIG['subgroup_results'])\n",
        "\n",
        "            # B. Generate RESULTS Text (Existing)\n",
        "            results_html = generate_subgroup_publication_text(\n",
        "                results_df, moderator1, moderator2, QM, df_QM, p_value_QM,\n",
        "                R_squared, Qt_overall, Qe_sum, df_Qe\n",
        "            )\n",
        "\n",
        "            # C. Display Both\n",
        "            display(HTML(methods_html))\n",
        "            display(HTML(results_html))\n",
        "\n",
        "            # D. Save Combined Text for Audit Report\n",
        "            ANALYSIS_CONFIG['subgroup_text'] = methods_html + \"<br><hr><br>\" + results_html\n",
        "\n",
        "        with tab_details:\n",
        "            with details_output:\n",
        "                print(f\"\\n{'='*60}\")\n",
        "                print(\"✅ ANALYSIS COMPLETE\")\n",
        "                print(f\"{'='*60}\")\n",
        "                print(\"Results saved to ANALYSIS_CONFIG['subgroup_results']\")\n",
        "                print(\"▶️  Ready for next step: Forest Plot visualization\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_html = f\"<div style='color: red; background-color: #f8d7da; padding: 15px; border-radius: 5px; margin: 10px 0;'>\"\n",
        "        error_html += f\"<b>❌ Error:</b> {type(e).__name__}<br>\"\n",
        "        error_html += f\"<b>Message:</b> {str(e)}<br>\"\n",
        "        error_html += f\"<pre>{traceback.format_exc()}</pre>\"\n",
        "        error_html += \"</div>\"\n",
        "        with tab_results:\n",
        "            display(HTML(error_html))\n",
        "\n",
        "# --- 3. INITIAL CHECK & DISPLAY ---\n",
        "try:\n",
        "    # Display tabs immediately\n",
        "    display(tabs)\n",
        "\n",
        "    # Run analysis\n",
        "    run_subgroup_analysis()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Initialization Error: {e}\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "with tab_export:\n",
        "    display(HTML(\"<h3 style='color: #2E86AB;'>💾 Download Audit Report</h3>\"))\n",
        "    display(HTML(\"<p>Generate an Excel file containing the subgroup table, heterogeneity breakdown, and specific group data.</p>\"))\n",
        "\n",
        "    btn_sub_export = widgets.Button(\n",
        "        description=\"📥 Download Subgroup Report\",\n",
        "        button_style='info', icon='file-excel', layout=widgets.Layout(width='300px', height='40px')\n",
        "    )\n",
        "\n",
        "    def on_sub_export_click(b):\n",
        "        export_analysis_report(report_type='subgroup', filename_prefix='Subgroup_Analysis')\n",
        "\n",
        "    btn_sub_export.on_click(on_sub_export_click)\n",
        "    display(btn_sub_export)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 📊 Dynamic Forest Plot preset\n",
        "# =============================================================================\n",
        "# CELL 9: PUBLICATION-READY FOREST PLOT\n",
        "# Purpose: Create customizable forest plots for meta-analysis results\n",
        "# Fix: Updated result keys to match the robust Cell 6 output.\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "import datetime\n",
        "from matplotlib.patches import Patch, Rectangle\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# --- 1. LOAD CONFIGURATION ---\n",
        "print(\"=\"*70)\n",
        "print(\"FOREST PLOT CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    if 'ANALYSIS_CONFIG' not in locals() and 'ANALYSIS_CONFIG' not in globals():\n",
        "        raise NameError(\"ANALYSIS_CONFIG not found.\")\n",
        "\n",
        "    subgroup_results = ANALYSIS_CONFIG.get('subgroup_results', {})\n",
        "    overall_results = ANALYSIS_CONFIG['overall_results']\n",
        "    es_config = ANALYSIS_CONFIG['es_config']\n",
        "\n",
        "    # Determine if we have subgroup analysis\n",
        "    has_subgroups = bool(subgroup_results) and 'results_df' in subgroup_results\n",
        "\n",
        "    if has_subgroups:\n",
        "        analysis_type = subgroup_results['analysis_type']\n",
        "        moderator1 = subgroup_results['moderator1']\n",
        "        moderator2 = subgroup_results.get('moderator2', None)\n",
        "        results_df = subgroup_results['results_df']\n",
        "\n",
        "        # Set dynamic defaults\n",
        "        if analysis_type == 'two_way':\n",
        "            default_title = f'Forest Plot: {moderator1} × {moderator2}'\n",
        "            default_y_label = moderator2\n",
        "        else:\n",
        "            default_title = f'Forest Plot: {moderator1}'\n",
        "            default_y_label = moderator1\n",
        "    else:\n",
        "        # Overall only (no subgroups)\n",
        "        analysis_type = 'overall_only'\n",
        "        default_title = 'Forest Plot: Overall Effect'\n",
        "        default_y_label = 'Study'\n",
        "        moderator1 = None\n",
        "        moderator2 = None\n",
        "\n",
        "    default_x_label = es_config.get('effect_label', \"Effect Size\")\n",
        "\n",
        "    print(f\"✓ Analysis type: {analysis_type}\")\n",
        "    print(f\"✓ Has subgroups: {has_subgroups}\")\n",
        "    print(f\"✓ Configuration loaded successfully\")\n",
        "\n",
        "except (KeyError, NameError) as e:\n",
        "    print(f\"❌ ERROR: Failed to load configuration: {e}\")\n",
        "    print(\"   Please run Cell 6 (overall analysis) first\")\n",
        "    raise\n",
        "\n",
        "# --- 2. DEFINE CUSTOMIZATION WIDGETS ---\n",
        "\n",
        "# ========== TAB 1: PLOT STYLE ==========\n",
        "style_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>Plot Style & Layout</h3>\")\n",
        "\n",
        "model_widget = widgets.Dropdown(\n",
        "    options=[('Random-Effects', 'RE'), ('Fixed-Effects', 'FE')],\n",
        "    value='RE',\n",
        "    description='Model:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "width_widget = widgets.FloatSlider(\n",
        "    value=8.0, min=6.0, max=14.0, step=0.5,\n",
        "    description='Plot Width (in):',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "height_widget = widgets.FloatSlider(\n",
        "    value=0.4, min=0.2, max=1.0, step=0.05,\n",
        "    description='Height per Row (in):',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "title_fontsize_widget = widgets.IntSlider(\n",
        "    value=12, min=8, max=18, step=1,\n",
        "    description='Title Font Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "label_fontsize_widget = widgets.IntSlider(\n",
        "    value=11, min=8, max=16, step=1,\n",
        "    description='Axis Label Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "tick_fontsize_widget = widgets.IntSlider(\n",
        "    value=9, min=6, max=14, step=1,\n",
        "    description='Tick Label Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "annot_fontsize_widget = widgets.IntSlider(\n",
        "    value=8, min=6, max=12, step=1,\n",
        "    description='Annotation Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "color_scheme_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Grayscale (Publication)', 'gray'),\n",
        "        ('Color (Presentation)', 'color'),\n",
        "        ('Black & White Only', 'bw')\n",
        "    ],\n",
        "    value='gray',\n",
        "    description='Color Scheme:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "marker_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Circle/Diamond (●/◆)', 'circle_diamond'),\n",
        "        ('Square/Diamond (■/◆)', 'square_diamond'),\n",
        "        ('Circle/Star (●/★)', 'circle_star')\n",
        "    ],\n",
        "    value='circle_diamond',\n",
        "    description='Marker Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "ci_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Solid Line', 'solid'),\n",
        "        ('Dashed Line', 'dashed'),\n",
        "        ('Solid with Caps', 'caps')\n",
        "    ],\n",
        "    value='solid',\n",
        "    description='CI Line Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "style_tab = widgets.VBox([\n",
        "    style_header,\n",
        "    preset_widget,\n",
        "    model_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Dimensions:</b>\"),\n",
        "    width_widget,\n",
        "    height_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Typography:</b>\"),\n",
        "    title_fontsize_widget,\n",
        "    label_fontsize_widget,\n",
        "    tick_fontsize_widget,\n",
        "    annot_fontsize_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Visual Style:</b>\"),\n",
        "    color_scheme_widget,\n",
        "    marker_style_widget,\n",
        "    ci_style_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 2: TEXT & LABELS ==========\n",
        "text_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>Text & Labels</h3>\")\n",
        "\n",
        "show_title_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Plot Title',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "title_widget = widgets.Text(\n",
        "    value=default_title,\n",
        "    description='Plot Title:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "xlabel_widget = widgets.Text(\n",
        "    value=default_x_label,\n",
        "    description='X-Axis Label:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "ylabel_widget = widgets.Text(\n",
        "    value=default_y_label,\n",
        "    description='Y-Axis Label:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "show_ylabel_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Y-Axis Label',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "text_tab = widgets.VBox([\n",
        "    text_header,\n",
        "    show_title_widget,\n",
        "    title_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    xlabel_widget,\n",
        "    show_ylabel_widget,\n",
        "    ylabel_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 3: ANNOTATIONS ==========\n",
        "annot_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>Annotations</h3>\")\n",
        "\n",
        "show_k_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show k (observations)',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "show_papers_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show paper count',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "show_fold_change_widget = widgets.Checkbox(\n",
        "    value=es_config.get('has_fold_change', False),\n",
        "    description='Show Fold-Change',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "annot_pos_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Right of CI', 'right'),\n",
        "        ('Above Marker', 'above'),\n",
        "        ('Below Marker', 'below')\n",
        "    ],\n",
        "    value='right',\n",
        "    description='Position:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "annot_offset_widget = widgets.FloatSlider(\n",
        "    value=0.0, min=-1.0, max=1.0, step=0.05,\n",
        "    description='H-Offset:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    readout_format='.2f'\n",
        ")\n",
        "\n",
        "group_label_box = widgets.VBox()\n",
        "# Group label widgets (always defined, conditionally displayed)\n",
        "group_label_h_offset_widget = widgets.FloatSlider(\n",
        "    value=0.0, min=-2.0, max=2.0, step=0.1,\n",
        "    description='Group H-Offset:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "group_label_v_offset_widget = widgets.FloatSlider(\n",
        "    value=0.0, min=-5.0, max=5.0, step=0.5,\n",
        "    description='Group V-Offset:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "group_label_fontsize_widget = widgets.IntSlider(\n",
        "    value=10, min=6, max=20, step=1,\n",
        "    description='Group Fontsize:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Only display group label widgets for two-way analysis\n",
        "if has_subgroups and analysis_type == 'two_way':\n",
        "    group_label_box = widgets.VBox([\n",
        "        widgets.HTML(\"<h4>Group Label Positioning (Two-Way Only)</h4>\"),\n",
        "        group_label_h_offset_widget,\n",
        "        group_label_v_offset_widget,\n",
        "        group_label_fontsize_widget\n",
        "    ])\n",
        "else:\n",
        "    group_label_box = widgets.VBox()\n",
        "\n",
        "\n",
        "annot_tab = widgets.VBox([\n",
        "    annot_header,\n",
        "    widgets.HTML(\"<b>Show in Annotations:</b>\"),\n",
        "    show_k_widget,\n",
        "    show_papers_widget,\n",
        "    show_fold_change_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Position:</b>\"),\n",
        "    annot_pos_widget,\n",
        "    annot_offset_widget,\n",
        "    group_label_box\n",
        "])\n",
        "\n",
        "# ========== TAB 4: AXES & SCALE ==========\n",
        "axes_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>Axes & Scaling</h3>\")\n",
        "\n",
        "auto_scale_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Auto-Scale X-Axis',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "x_min_widget = widgets.FloatText(\n",
        "    value=-2.0,\n",
        "    description='X-Min:',\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width='220px', visibility='hidden')\n",
        ")\n",
        "\n",
        "x_max_widget = widgets.FloatText(\n",
        "    value=2.0,\n",
        "    description='X-Max:',\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width='220px', visibility='hidden')\n",
        ")\n",
        "\n",
        "manual_scale_box = widgets.HBox([x_min_widget, x_max_widget])\n",
        "\n",
        "def toggle_manual_scale(change):\n",
        "    if change['new']:\n",
        "        x_min_widget.layout.visibility = 'hidden'\n",
        "        x_max_widget.layout.visibility = 'hidden'\n",
        "    else:\n",
        "        x_min_widget.layout.visibility = 'visible'\n",
        "        x_max_widget.layout.visibility = 'visible'\n",
        "\n",
        "auto_scale_widget.observe(toggle_manual_scale, names='value')\n",
        "\n",
        "show_grid_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Grid',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "grid_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Dashed (Light)', 'dashed_light'),\n",
        "        ('Dotted (Light)', 'dotted_light'),\n",
        "        ('Solid (Light)', 'solid_light')\n",
        "    ],\n",
        "    value='dashed_light',\n",
        "    description='Grid Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "show_null_line_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Null Effect Line',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "show_fold_axis_widget = widgets.Checkbox(\n",
        "    value=es_config.get('has_fold_change', False) and show_fold_change_widget.value,\n",
        "    description='Show Fold-Change Axis (Top)',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "axes_tab = widgets.VBox([\n",
        "    axes_header,\n",
        "    auto_scale_widget,\n",
        "    manual_scale_box,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Grid & Reference Lines:</b>\"),\n",
        "    show_grid_widget,\n",
        "    grid_style_widget,\n",
        "    show_null_line_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    show_fold_axis_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 5: EXPORT OPTIONS ==========\n",
        "export_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>Export Options</h3>\")\n",
        "\n",
        "save_pdf_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Save as PDF',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "save_png_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Save as PNG',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "png_dpi_widget = widgets.IntSlider(\n",
        "    value=300, min=150, max=600, step=50,\n",
        "    description='PNG DPI:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "filename_prefix_widget = widgets.Text(\n",
        "    value='ForestPlot',\n",
        "    description='Filename Prefix:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "transparent_bg_widget = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Transparent Background',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "export_tab = widgets.VBox([\n",
        "    export_header,\n",
        "    save_pdf_widget,\n",
        "    save_png_widget,\n",
        "    png_dpi_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    filename_prefix_widget,\n",
        "    transparent_bg_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 6: LABEL EDITOR ==========\n",
        "label_editor_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>Label Editor</h3>\")\n",
        "label_editor_desc = widgets.HTML(\n",
        "    \"<p style='color: #666;'><i>Customize display names for all groups and subgroups in the plot</i></p>\"\n",
        ")\n",
        "\n",
        "print(f\"\\n🔍 Identifying labels for editor...\")\n",
        "\n",
        "unique_labels = set()\n",
        "label_widgets_dict = {}\n",
        "\n",
        "try:\n",
        "    if has_subgroups:\n",
        "        if analysis_type == 'single':\n",
        "            unique_labels.update(results_df['group'].astype(str).unique())\n",
        "        else:  # two_way\n",
        "            unique_labels.update(results_df[moderator1].astype(str).unique())\n",
        "            unique_labels.update(results_df[moderator2].astype(str).unique())\n",
        "\n",
        "    unique_labels.add('Overall')\n",
        "    sorted_labels = sorted(list(unique_labels))\n",
        "\n",
        "    print(f\"  ✓ Found {len(sorted_labels)} unique labels\")\n",
        "\n",
        "    label_editor_widgets = []\n",
        "    for label in sorted_labels:\n",
        "        widget_label = f\"Overall Effect:\" if label == 'Overall' else f\"{label}:\"\n",
        "        text_widget = widgets.Text(\n",
        "            value=str(label),\n",
        "            description=widget_label,\n",
        "            layout=widgets.Layout(width='500px'),\n",
        "            style={'description_width': '200px'}\n",
        "        )\n",
        "        label_editor_widgets.append(text_widget)\n",
        "        label_widgets_dict[str(label)] = text_widget\n",
        "\n",
        "    label_editor_tab = widgets.VBox([\n",
        "        label_editor_header,\n",
        "        label_editor_desc,\n",
        "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "        widgets.HTML(\n",
        "            \"<p><b>Instructions:</b> Edit the text on the right to change how labels appear in the plot. \"\n",
        "            \"The original coded names are shown on the left.</p>\"\n",
        "        ),\n",
        "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "        *label_editor_widgets\n",
        "    ])\n",
        "\n",
        "    print(f\"  ✓ Label editor created\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  ⚠️  Error creating label editor: {e}\")\n",
        "    label_editor_tab = widgets.VBox([\n",
        "        label_editor_header,\n",
        "        widgets.HTML(\"<p style='color: red;'>Error creating label editor.</p>\")\n",
        "    ])\n",
        "    label_widgets_dict = {}\n",
        "\n",
        "# ========== CREATE TAB WIDGET ==========\n",
        "tab_children = [style_tab, text_tab, annot_tab, axes_tab, export_tab, label_editor_tab]\n",
        "tab = widgets.Tab(children=tab_children)\n",
        "tab.set_title(0, '🎨 Style')\n",
        "tab.set_title(1, '📝 Text')\n",
        "tab.set_title(2, '🏷️ Annotations')\n",
        "tab.set_title(3, '📏 Axes')\n",
        "tab.set_title(4, '💾 Export')\n",
        "tab.set_title(5, '✏️ Labels')\n",
        "\n",
        "# --- 3. DEFINE PLOT GENERATION FUNCTION ---\n",
        "plot_output = widgets.Output()\n",
        "\n",
        "def generate_plot(b):\n",
        "    with plot_output:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"GENERATING FOREST PLOT\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        try:\n",
        "            # --- GET WIDGET VALUES ---\n",
        "            plot_model = model_widget.value\n",
        "            plot_width = width_widget.value\n",
        "            height_per_row = height_widget.value\n",
        "            title_fontsize = title_fontsize_widget.value\n",
        "            label_fontsize = label_fontsize_widget.value\n",
        "            tick_fontsize = tick_fontsize_widget.value\n",
        "            annot_fontsize = annot_fontsize_widget.value\n",
        "            color_scheme = color_scheme_widget.value\n",
        "            marker_style = marker_style_widget.value\n",
        "            ci_style = ci_style_widget.value\n",
        "\n",
        "            show_title = show_title_widget.value\n",
        "            graph_title = title_widget.value\n",
        "            x_label = xlabel_widget.value\n",
        "            show_ylabel = show_ylabel_widget.value\n",
        "            y_label = ylabel_widget.value\n",
        "\n",
        "            show_k = show_k_widget.value\n",
        "            show_papers = show_papers_widget.value\n",
        "            show_fold_change = show_fold_change_widget.value\n",
        "            annot_pos = annot_pos_widget.value\n",
        "            annot_offset = annot_offset_widget.value\n",
        "\n",
        "            auto_scale = auto_scale_widget.value\n",
        "            x_min_manual = x_min_widget.value\n",
        "            x_max_manual = x_max_widget.value\n",
        "            show_grid = show_grid_widget.value\n",
        "            grid_style = grid_style_widget.value\n",
        "            show_null_line = show_null_line_widget.value\n",
        "            show_fold_axis = show_fold_axis_widget.value\n",
        "\n",
        "            save_pdf = save_pdf_widget.value\n",
        "            save_png = save_png_widget.value\n",
        "            png_dpi = png_dpi_widget.value\n",
        "            filename_prefix = filename_prefix_widget.value\n",
        "            transparent_bg = transparent_bg_widget.value\n",
        "\n",
        "            # Group label offsets (two-way only)\n",
        "            if has_subgroups and analysis_type == 'two_way':\n",
        "                group_label_h_offset = group_label_h_offset_widget.value\n",
        "                group_label_v_offset = group_label_v_offset_widget.value\n",
        "                group_label_fontsize = group_label_fontsize_widget.value\n",
        "            else:\n",
        "                group_label_h_offset = 0\n",
        "                group_label_v_offset = 0\n",
        "                group_label_fontsize = 10\n",
        "\n",
        "            # --- BUILD LABEL MAPPING FROM EDITOR ---\n",
        "            label_mapping = {}\n",
        "            for original_label, widget in label_widgets_dict.items():\n",
        "                custom_label = widget.value\n",
        "                label_mapping[original_label] = custom_label\n",
        "                label_mapping[str(original_label)] = custom_label\n",
        "\n",
        "            print(f\"📊 Configuration:\")\n",
        "            print(f\"  Model: {plot_model}\")\n",
        "            print(f\"  Dimensions: {plot_width}\\\" × auto\")\n",
        "            print(f\"  Color scheme: {color_scheme}\")\n",
        "            print(f\"  Has subgroups: {has_subgroups}\")\n",
        "\n",
        "            # Show custom labels if any were changed\n",
        "            changed_labels = {k: v for k, v in label_mapping.items() if k != v}\n",
        "            if changed_labels:\n",
        "                print(f\"\\n📝 Custom labels ({len(changed_labels)} changed):\")\n",
        "                for orig, custom in list(changed_labels.items())[:5]:\n",
        "                    print(f\"  '{orig}' → '{custom}'\")\n",
        "                if len(changed_labels) > 5:\n",
        "                    print(f\"  ... and {len(changed_labels)-5} more\")\n",
        "\n",
        "            overall_label_text = label_mapping.get('Overall', 'Overall Effect')\n",
        "\n",
        "            # --- DETERMINE COLUMN NAMES BASED ON MODEL ---\n",
        "            if plot_model == 'FE':\n",
        "                effect_col = 'pooled_effect_fe'\n",
        "                se_col = 'pooled_se_fe'\n",
        "                ci_lower_col = 'ci_lower_fe'\n",
        "                ci_upper_col = 'ci_upper_fe'\n",
        "                fold_col = 'fold_change_fe'\n",
        "\n",
        "                overall_effect_key = 'pooled_effect_fixed'\n",
        "                overall_se_key = 'pooled_SE_fixed'\n",
        "                overall_ci_lower_key = 'ci_lower_fixed'\n",
        "                overall_ci_upper_key = 'ci_upper_fixed'\n",
        "                overall_fold_key = 'pooled_fold_fixed' # Assuming this exists\n",
        "            else:  # RE\n",
        "                effect_col = 'pooled_effect_re'\n",
        "                se_col = 'pooled_se_re'\n",
        "                ci_lower_col = 'ci_lower_re'\n",
        "                ci_upper_col = 'ci_upper_re'\n",
        "                fold_col = 'fold_change_re'\n",
        "\n",
        "                overall_effect_key = 'pooled_effect_random'\n",
        "                # FIX: Use keys that exist in Cell 6 output\n",
        "                overall_se_key = 'pooled_SE_random_reported'\n",
        "                overall_ci_lower_key = 'ci_lower_random_reported'\n",
        "                overall_ci_upper_key = 'ci_upper_random_reported'\n",
        "                overall_fold_key = 'pooled_fold_random'\n",
        "\n",
        "            # --- PREPARE DATA ---\n",
        "            if has_subgroups:\n",
        "                plot_df_subgroups = results_df.copy()\n",
        "\n",
        "                plot_df_subgroups = plot_df_subgroups.rename(columns={\n",
        "                    effect_col: 'EffectSize',\n",
        "                    se_col: 'SE',\n",
        "                    ci_lower_col: 'CI_Lower',\n",
        "                    ci_upper_col: 'CI_Upper',\n",
        "                    fold_col: 'FoldChange',\n",
        "                    'k': 'k',\n",
        "                    'n_papers': 'nPapers'\n",
        "                })\n",
        "\n",
        "                if analysis_type == 'two_way':\n",
        "                    plot_df_subgroups['GroupVar'] = plot_df_subgroups[moderator1].astype(str)\n",
        "                    plot_df_subgroups['LabelVar'] = plot_df_subgroups[moderator2].astype(str)\n",
        "                else:  # single\n",
        "                    plot_df_subgroups['GroupVar'] = 'Subgroup'\n",
        "                    plot_df_subgroups['LabelVar'] = plot_df_subgroups['group'].astype(str)\n",
        "\n",
        "                required_cols = ['GroupVar', 'LabelVar', 'k', 'nPapers',\n",
        "                               'EffectSize', 'SE', 'CI_Lower', 'CI_Upper', 'FoldChange']\n",
        "                plot_df_subgroups = plot_df_subgroups[required_cols]\n",
        "                plot_df_subgroups.dropna(subset=['EffectSize', 'SE'], inplace=True)\n",
        "\n",
        "                print(f\"  Subgroups: {len(plot_df_subgroups)}\")\n",
        "            else:\n",
        "                plot_df_subgroups = pd.DataFrame(columns=[\n",
        "                    'GroupVar', 'LabelVar', 'k', 'nPapers',\n",
        "                    'EffectSize', 'SE', 'CI_Lower', 'CI_Upper', 'FoldChange'\n",
        "                ])\n",
        "\n",
        "            # --- ADD OVERALL EFFECT ---\n",
        "            overall_effect_val = overall_results[overall_effect_key]\n",
        "            # FIX: Safely get values or default to Z-test version if reported missing\n",
        "            overall_se_val = overall_results.get(overall_se_key, overall_results.get('pooled_SE_random_Z'))\n",
        "            overall_ci_lower_val = overall_results.get(overall_ci_lower_key, overall_results.get('ci_lower_random_Z'))\n",
        "            overall_ci_upper_val = overall_results.get(overall_ci_upper_key, overall_results.get('ci_upper_random_Z'))\n",
        "\n",
        "            overall_k_val = overall_results['k']\n",
        "            overall_papers_val = overall_results['k_papers']\n",
        "            overall_fold_val = overall_results.get(overall_fold_key, np.nan)\n",
        "\n",
        "            overall_row = pd.DataFrame([{\n",
        "                'GroupVar': 'Overall',\n",
        "                'LabelVar': 'Overall',\n",
        "                'k': overall_k_val,\n",
        "                'nPapers': overall_papers_val,\n",
        "                'EffectSize': overall_effect_val,\n",
        "                'SE': overall_se_val,\n",
        "                'CI_Lower': overall_ci_lower_val,\n",
        "                'CI_Upper': overall_ci_upper_val,\n",
        "                'FoldChange': overall_fold_val\n",
        "            }])\n",
        "\n",
        "            print(f\"  Overall: k={overall_k_val}, papers={overall_papers_val}\")\n",
        "\n",
        "            # --- COMBINE DATA (OVERALL ON TOP) ---\n",
        "            plot_df = pd.concat([overall_row, plot_df_subgroups], ignore_index=True)\n",
        "\n",
        "            plot_df['SortKey_Group'] = plot_df['GroupVar'].apply(\n",
        "                lambda x: 'AAAAA' if x == 'Overall' else str(x)\n",
        "            )\n",
        "            plot_df['SortKey_Label'] = plot_df['LabelVar'].apply(\n",
        "                lambda x: 'AAAAA' if x == 'Overall' else str(x)\n",
        "            )\n",
        "            plot_df.sort_values(by=['SortKey_Group', 'SortKey_Label'], inplace=True)\n",
        "            plot_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            if plot_df.empty:\n",
        "                print(\"❌ ERROR: No data to plot\")\n",
        "                return\n",
        "\n",
        "            print(f\"  Total rows: {len(plot_df)}\")\n",
        "\n",
        "            # --- CALCULATE PLOT DIMENSIONS ---\n",
        "            num_rows = len(plot_df)\n",
        "            y_positions = np.arange(num_rows)\n",
        "\n",
        "            base_height = 2.5\n",
        "            plot_height = max(base_height, num_rows * height_per_row + 1.5)\n",
        "\n",
        "            y_margin_top = 0.75\n",
        "            y_margin_bottom = 0.75\n",
        "            y_lim_bottom = y_positions[0] - y_margin_bottom\n",
        "            y_lim_top = y_positions[-1] + y_margin_top\n",
        "\n",
        "            # --- Y-TICK LABELS (USE CUSTOM MAPPING) ---\n",
        "            y_tick_labels = []\n",
        "            for i, row in plot_df.iterrows():\n",
        "                if row['GroupVar'] == 'Overall':\n",
        "                    y_tick_labels.append(overall_label_text)\n",
        "                else:\n",
        "                    original_label = str(row['LabelVar'])\n",
        "                    display_label = label_mapping.get(original_label, original_label)\n",
        "                    y_tick_labels.append(display_label)\n",
        "\n",
        "            # --- CALCULATE X-AXIS LIMITS (FIXED - USE ALL DATA) ---\n",
        "            min_ci = plot_df['CI_Lower'].min()\n",
        "            max_ci = plot_df['CI_Upper'].max()\n",
        "            min_effect = plot_df['EffectSize'].min()\n",
        "            max_effect = plot_df['EffectSize'].max()\n",
        "\n",
        "            plot_min = min(min_ci, 0)\n",
        "            plot_max = max(max_ci, 0)\n",
        "            x_range = plot_max - plot_min\n",
        "\n",
        "            if x_range == 0:\n",
        "                x_range = 1\n",
        "\n",
        "            print(f\"\\n📏 Data range:\")\n",
        "            print(f\"  Effect sizes: [{min_effect:.3f}, {max_effect:.3f}]\")\n",
        "            print(f\"  CI range: [{min_ci:.3f}, {max_ci:.3f}]\")\n",
        "            print(f\"  Plot range: [{plot_min:.3f}, {plot_max:.3f}]\")\n",
        "\n",
        "            # --- ESTIMATE ANNOTATION SPACE NEEDED ---\n",
        "            max_k = int(plot_df['k'].max())\n",
        "            max_np = int(plot_df['nPapers'].max()) if 'nPapers' in plot_df.columns else 0\n",
        "\n",
        "            annot_parts = []\n",
        "            if show_k:\n",
        "                annot_parts.append(f\"k={max_k}\")\n",
        "            if show_papers:\n",
        "                annot_parts.append(f\"({max_np})\")\n",
        "            if show_fold_change and es_config.get('has_fold_change', False):\n",
        "                max_fold = plot_df['FoldChange'].abs().max() if 'FoldChange' in plot_df.columns else 10\n",
        "                annot_parts.append(f\"[-{max_fold:.2f}×]\")\n",
        "\n",
        "            example_annot = \" \".join(annot_parts) if annot_parts else \"k=100 (10)\"\n",
        "\n",
        "            char_width_fraction = (annot_fontsize / 8.0) * 0.006\n",
        "            annot_space_fraction = len(example_annot) * char_width_fraction\n",
        "\n",
        "            print(f\"  Annotation example: '{example_annot}' ({len(example_annot)} chars)\")\n",
        "\n",
        "            # --- CALCULATE SPACE FOR GROUP LABELS (TWO-WAY) ---\n",
        "            group_label_space = 0\n",
        "            if has_subgroups and analysis_type == 'two_way':\n",
        "                max_group_len = 0\n",
        "                for group_val in plot_df[plot_df['GroupVar'] != 'Overall']['GroupVar'].unique():\n",
        "                    custom_label = label_mapping.get(str(group_val), str(group_val))\n",
        "                    max_group_len = max(max_group_len, len(custom_label))\n",
        "\n",
        "                char_width_group = (group_label_fontsize / 8.0) * 0.006\n",
        "                group_label_space = max_group_len * char_width_group\n",
        "\n",
        "                print(f\"  Group label max: {max_group_len} chars\")\n",
        "\n",
        "            # --- AUTO-SCALE CALCULATION ---\n",
        "            if auto_scale:\n",
        "                left_padding = 0.05\n",
        "                annot_distance = 0.015\n",
        "                right_padding = 0.03\n",
        "\n",
        "                total_right_fraction = (annot_distance +\n",
        "                                       annot_space_fraction +\n",
        "                                       group_label_space +\n",
        "                                       right_padding)\n",
        "\n",
        "                x_min_auto = plot_min - x_range * left_padding\n",
        "                x_max_auto = plot_max + x_range * (total_right_fraction / (1 - total_right_fraction))\n",
        "\n",
        "                x_limits = (x_min_auto, x_max_auto)\n",
        "                print(f\"  X-axis (auto): [{x_min_auto:.3f}, {x_max_auto:.3f}]\")\n",
        "            else:\n",
        "                x_limits = (x_min_manual, x_max_manual)\n",
        "                print(f\"  X-axis (manual): [{x_min_manual:.3f}, {x_max_manual:.3f}]\")\n",
        "\n",
        "            # --- DETERMINE COLORS AND MARKERS ---\n",
        "            if color_scheme == 'gray':\n",
        "                subgroup_color = 'dimgray'\n",
        "                overall_color = 'black'\n",
        "                ci_color_subgroup = 'gray'\n",
        "                ci_color_overall = 'black'\n",
        "            elif color_scheme == 'color':\n",
        "                subgroup_color = '#4A90E2'\n",
        "                overall_color = '#E74C3C'\n",
        "                ci_color_subgroup = '#4A90E2'\n",
        "                ci_color_overall = '#E74C3C'\n",
        "            else:  # bw\n",
        "                subgroup_color = 'black'\n",
        "                overall_color = 'black'\n",
        "                ci_color_subgroup = 'black'\n",
        "                ci_color_overall = 'black'\n",
        "\n",
        "            if marker_style == 'circle_diamond':\n",
        "                subgroup_marker = 'o'\n",
        "                overall_marker = 'D'\n",
        "            elif marker_style == 'square_diamond':\n",
        "                subgroup_marker = 's'\n",
        "                overall_marker = 'D'\n",
        "            else:  # circle_star\n",
        "                subgroup_marker = 'o'\n",
        "                overall_marker = '*'\n",
        "\n",
        "            subgroup_marker_size = 6\n",
        "            overall_marker_size = 8\n",
        "            subgroup_ci_width = 1.5\n",
        "            overall_ci_width = 2.0\n",
        "\n",
        "            if ci_style == 'solid':\n",
        "                capsize = 0\n",
        "            elif ci_style == 'dashed':\n",
        "                capsize = 0\n",
        "            else:  # caps\n",
        "                capsize = 4\n",
        "\n",
        "            # --- CREATE FIGURE ---\n",
        "            fig, ax = plt.subplots(figsize=(plot_width, plot_height))\n",
        "\n",
        "            if transparent_bg:\n",
        "                fig.patch.set_alpha(0)\n",
        "                ax.patch.set_alpha(0)\n",
        "\n",
        "            print(f\"\\n🎨 Plotting {num_rows} rows...\")\n",
        "\n",
        "            # --- PLOT DATA POINTS AND ERROR BARS ---\n",
        "            for i, row in plot_df.iterrows():\n",
        "                is_overall = (row['GroupVar'] == 'Overall')\n",
        "\n",
        "                marker = overall_marker if is_overall else subgroup_marker\n",
        "                msize = overall_marker_size if is_overall else subgroup_marker_size\n",
        "                color = overall_color if is_overall else subgroup_color\n",
        "                ci_color = ci_color_overall if is_overall else ci_color_subgroup\n",
        "                ci_width = overall_ci_width if is_overall else subgroup_ci_width\n",
        "                zorder = 5 if is_overall else 3\n",
        "\n",
        "                linestyle = '-' if ci_style != 'dashed' else '--'\n",
        "\n",
        "                ax.errorbar(\n",
        "                    x=row['EffectSize'],\n",
        "                    y=y_positions[i],\n",
        "                    xerr=[[row['EffectSize'] - row['CI_Lower']],\n",
        "                          [row['CI_Upper'] - row['EffectSize']]],\n",
        "                    fmt='none',\n",
        "                    capsize=capsize,\n",
        "                    color=ci_color,\n",
        "                    linewidth=ci_width,\n",
        "                    linestyle=linestyle,\n",
        "                    alpha=0.9,\n",
        "                    zorder=zorder-1\n",
        "                )\n",
        "\n",
        "                ax.plot(\n",
        "                    row['EffectSize'],\n",
        "                    y_positions[i],\n",
        "                    marker=marker,\n",
        "                    markersize=msize,\n",
        "                    markerfacecolor=color,\n",
        "                    markeredgecolor='black' if color_scheme != 'bw' else 'black',\n",
        "                    markeredgewidth=1.0,\n",
        "                    linestyle='none',\n",
        "                    zorder=zorder\n",
        "                )\n",
        "\n",
        "            # --- SET AXIS LIMITS FIRST ---\n",
        "            ax.set_xlim(x_limits[0], x_limits[1])\n",
        "            ax.set_ylim(y_lim_top, y_lim_bottom)  # Inverted\n",
        "\n",
        "            final_xlims = ax.get_xlim()\n",
        "            final_xrange = final_xlims[1] - final_xlims[0]\n",
        "\n",
        "            print(f\"  Final X-axis: [{final_xlims[0]:.3f}, {final_xlims[1]:.3f}]\")\n",
        "\n",
        "            # --- ADD ANNOTATIONS ---\n",
        "            print(f\"  Adding annotations...\")\n",
        "\n",
        "            annot_x_offset = annot_distance * final_xrange\n",
        "\n",
        "            for i, row in plot_df.iterrows():\n",
        "                is_overall = (row['GroupVar'] == 'Overall')\n",
        "                font_weight = 'bold' if is_overall else 'normal'\n",
        "\n",
        "                annot_parts = []\n",
        "                if show_k:\n",
        "                    annot_parts.append(f\"k={int(row['k'])}\")\n",
        "                if show_papers and pd.notna(row['nPapers']):\n",
        "                    annot_parts.append(f\"({int(row['nPapers'])})\")\n",
        "                if show_fold_change and pd.notna(row['FoldChange']) and es_config.get('has_fold_change', False):\n",
        "                    fold_sign = \"+\" if row['FoldChange'] > 0 else \"\"\n",
        "                    annot_parts.append(f\"[{fold_sign}{row['FoldChange']:.2f}×]\")\n",
        "\n",
        "                annotation_text = \" \".join(annot_parts) if annot_parts else \"\"\n",
        "\n",
        "                if annotation_text:\n",
        "                    if annot_pos == 'right':\n",
        "                        x_pos = row['CI_Upper'] + annot_x_offset + (annot_offset * final_xrange * 0.1)\n",
        "                        y_pos = y_positions[i]\n",
        "                        va = 'center'\n",
        "                        ha = 'left'\n",
        "                    elif annot_pos == 'above':\n",
        "                        x_pos = row['EffectSize'] + (annot_offset * final_xrange * 0.1)\n",
        "                        y_pos = y_positions[i] - 0.2\n",
        "                        va = 'bottom'\n",
        "                        ha = 'center'\n",
        "                    else:  # below\n",
        "                        x_pos = row['EffectSize'] + (annot_offset * final_xrange * 0.1)\n",
        "                        y_pos = y_positions[i] + 0.2\n",
        "                        va = 'top'\n",
        "                        ha = 'center'\n",
        "\n",
        "                    ax.text(\n",
        "                        x_pos, y_pos,\n",
        "                        annotation_text,\n",
        "                        va=va, ha=ha,\n",
        "                        fontsize=annot_fontsize,\n",
        "                        fontweight=font_weight,\n",
        "                        clip_on=False\n",
        "                    )\n",
        "\n",
        "            # --- ADD GROUP LABELS (TWO-WAY) ---\n",
        "            if has_subgroups and analysis_type == 'two_way':\n",
        "                print(f\"  Adding group labels...\")\n",
        "\n",
        "                current_group = None\n",
        "                first_subgroup_idx = 1 if 'Overall' in plot_df['GroupVar'].values else 0\n",
        "                group_label_x_base = final_xlims[1] - (right_padding * final_xrange)\n",
        "\n",
        "                for i, row in plot_df.iterrows():\n",
        "                    group_val = str(row['GroupVar'])\n",
        "\n",
        "                    if group_val != 'Overall' and group_val != current_group:\n",
        "                        if i > first_subgroup_idx:\n",
        "                            ax.axhline(\n",
        "                                y=y_positions[i] - 0.5,\n",
        "                                color='darkgray',\n",
        "                                linewidth=0.8,\n",
        "                                linestyle='-',\n",
        "                                xmin=0.01,\n",
        "                                xmax=0.99,\n",
        "                                zorder=1\n",
        "                            )\n",
        "\n",
        "                        group_indices = plot_df[plot_df['GroupVar'] == group_val].index\n",
        "                        label_y = (y_positions[group_indices[0]] + y_positions[group_indices[-1]]) / 2.0\n",
        "\n",
        "                        label_x = group_label_x_base + (group_label_h_offset * final_xrange * 0.05)\n",
        "                        label_y = label_y + group_label_v_offset\n",
        "\n",
        "                        display_group_label = label_mapping.get(group_val, group_val)\n",
        "\n",
        "                        ax.text(\n",
        "                            label_x, label_y,\n",
        "                            display_group_label,\n",
        "                            va='center',\n",
        "                            ha='right',\n",
        "                            fontweight='bold',\n",
        "                            fontsize=group_label_fontsize,\n",
        "                            color='black',\n",
        "                            clip_on=False\n",
        "                        )\n",
        "\n",
        "                        current_group = group_val\n",
        "\n",
        "            # --- ADD SEPARATOR LINE BELOW OVERALL ---\n",
        "            if len(plot_df) > 1:\n",
        "                separator_y = y_positions[0] + 0.5\n",
        "                ax.axhline(\n",
        "                    y=separator_y,\n",
        "                    color='black',\n",
        "                    linewidth=1.5,\n",
        "                    linestyle='-'\n",
        "                )\n",
        "\n",
        "            # --- CUSTOMIZE AXES ---\n",
        "            print(f\"  Customizing axes...\")\n",
        "\n",
        "            if show_null_line:\n",
        "                ax.axvline(\n",
        "                    x=0,\n",
        "                    color='black',\n",
        "                    linestyle='-',\n",
        "                    linewidth=1.5,\n",
        "                    alpha=0.8,\n",
        "                    zorder=1\n",
        "                )\n",
        "\n",
        "            ax.set_xlabel(x_label, fontsize=label_fontsize, fontweight='bold')\n",
        "            if show_ylabel:\n",
        "                ax.set_ylabel(y_label, fontsize=label_fontsize, fontweight='bold')\n",
        "\n",
        "            if show_title:\n",
        "                ax.set_title(graph_title, fontweight='bold', fontsize=title_fontsize, pad=15)\n",
        "\n",
        "            ax.set_yticks(y_positions)\n",
        "            ax.set_yticklabels(y_tick_labels, fontsize=tick_fontsize)\n",
        "            ax.tick_params(axis='x', labelsize=tick_fontsize)\n",
        "\n",
        "            if show_grid:\n",
        "                if grid_style == 'dashed_light':\n",
        "                    ax.grid(axis='x', alpha=0.3, linestyle='--', linewidth=0.5)\n",
        "                elif grid_style == 'dotted_light':\n",
        "                    ax.grid(axis='x', alpha=0.3, linestyle=':', linewidth=0.5)\n",
        "                else:  # solid_light\n",
        "                    ax.grid(axis='x', alpha=0.2, linestyle='-', linewidth=0.5)\n",
        "\n",
        "            # --- ADD FOLD-CHANGE AXIS (TOP) ---\n",
        "            if show_fold_axis and es_config.get('has_fold_change', False):\n",
        "                print(f\"  Adding fold-change axis...\")\n",
        "\n",
        "                ax2 = ax.twiny()\n",
        "\n",
        "                fold_ticks_lnRR = np.array([-2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2])\n",
        "                fold_ticks_RR = np.exp(fold_ticks_lnRR)\n",
        "\n",
        "                valid_mask = ((fold_ticks_lnRR >= final_xlims[0]) &\n",
        "                             (fold_ticks_lnRR <= final_xlims[1]))\n",
        "                fold_ticks_lnRR = fold_ticks_lnRR[valid_mask]\n",
        "                fold_ticks_RR = fold_ticks_RR[valid_mask]\n",
        "\n",
        "                ax2.set_xlim(final_xlims[0], final_xlims[1])\n",
        "                ax2.set_xticks(fold_ticks_lnRR)\n",
        "\n",
        "                fold_labels = []\n",
        "                for rr in fold_ticks_RR:\n",
        "                    if rr < 1:\n",
        "                        fold_labels.append(f\"{1/rr:.1f}× ↓\")\n",
        "                    elif rr > 1:\n",
        "                        fold_labels.append(f\"{rr:.1f}× ↑\")\n",
        "                    else:\n",
        "                        fold_labels.append(\"1×\")\n",
        "\n",
        "                ax2.set_xticklabels(fold_labels, fontsize=tick_fontsize)\n",
        "                ax2.set_xlabel(\"Fold-Change\", fontsize=label_fontsize, fontweight='bold')\n",
        "\n",
        "            # --- FINALIZE PLOT ---\n",
        "            fig.tight_layout()\n",
        "\n",
        "            # --- SAVE FILES ---\n",
        "            print(f\"\\n💾 Saving files...\")\n",
        "\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            base_filename = f\"{filename_prefix}_{plot_model}_{timestamp}\"\n",
        "\n",
        "            saved_files = []\n",
        "\n",
        "            if save_pdf:\n",
        "                pdf_filename = f\"{base_filename}.pdf\"\n",
        "                fig.savefig(pdf_filename, bbox_inches='tight', transparent=transparent_bg)\n",
        "                saved_files.append(pdf_filename)\n",
        "                print(f\"  ✓ {pdf_filename}\")\n",
        "\n",
        "            if save_png:\n",
        "                png_filename = f\"{base_filename}.png\"\n",
        "                fig.savefig(png_filename, dpi=png_dpi, bbox_inches='tight', transparent=transparent_bg)\n",
        "                saved_files.append(png_filename)\n",
        "                print(f\"  ✓ {png_filename} (DPI: {png_dpi})\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            print(f\"\\n\" + \"=\"*70)\n",
        "            print(\"✅ FOREST PLOT COMPLETE\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"Files: {', '.join(saved_files)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ ERROR: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "# --- 4. CREATE BUTTON AND DISPLAY ---\n",
        "plot_button = widgets.Button(\n",
        "    description='📊 Generate Forest Plot',\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='450px', height='50px'),\n",
        "    style={'font_weight': 'bold', 'font_size': '14px'}\n",
        ")\n",
        "\n",
        "plot_button.on_click(generate_plot)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ FOREST PLOT INTERFACE READY\")\n",
        "print(\"=\"*70)\n",
        "print(\"👆 Customize your plot using the tabs above, then click Generate\")\n",
        "print(\"\\n📝 Tips:\")\n",
        "print(\"  • Use the 'Labels' tab to rename coded variables\")\n",
        "print(\"  • Auto-scale considers ALL data points for proper spacing\")\n",
        "print(\"  • Annotations and group labels will fit within the plot\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='color: #2E86AB;'>📊 Forest Plot Generator</h3>\"),\n",
        "    widgets.HTML(\"<p style='color: #666;'>Create publication-ready forest plots with full customization</p>\"),\n",
        "    widgets.HTML(\"<hr style='margin: 15px 0;'>\"),\n",
        "    tab,\n",
        "    widgets.HTML(\"<hr style='margin: 15px 0;'>\"),\n",
        "    plot_button,\n",
        "    plot_output\n",
        "]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-XXpDWLFt-s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "PQQCAwh5C8A6"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 🍎 Orchard Plot (ENHANCED)\n",
        "# =============================================================================\n",
        "# CELL 15: PUBLICATION-READY ORCHARD PLOT\n",
        "# Purpose: Create customizable orchard plots for meta-analysis results\n",
        "# Enhanced: Full GUI matching Dynamic Forest Plot functionality\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import datetime\n",
        "from scipy.stats import t, norm\n",
        "\n",
        "# --- 1. INITIALIZATION ---\n",
        "print(\"=\"*70)\n",
        "print(\"ORCHARD PLOT CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    if 'ANALYSIS_CONFIG' not in globals():\n",
        "        raise NameError(\"ANALYSIS_CONFIG not found. Run previous cells first.\")\n",
        "\n",
        "    # Load Raw Data\n",
        "    if 'analysis_data' in globals():\n",
        "        df_orchard = analysis_data.copy()\n",
        "    elif 'data_filtered' in globals():\n",
        "        df_orchard = data_filtered.copy()\n",
        "    else:\n",
        "        df_orchard = ANALYSIS_CONFIG.get('cleaned_data', ANALYSIS_CONFIG.get('data'))\n",
        "\n",
        "    if df_orchard is None:\n",
        "        raise ValueError(\"Analysis data not found.\")\n",
        "\n",
        "    # Load Results & Config\n",
        "    overall_res = ANALYSIS_CONFIG['overall_results']\n",
        "    es_config = ANALYSIS_CONFIG.get('es_config', {})\n",
        "\n",
        "    if 'subgroup_results' in ANALYSIS_CONFIG and 'results_df' in ANALYSIS_CONFIG['subgroup_results']:\n",
        "        subgroup_config = ANALYSIS_CONFIG['subgroup_results']\n",
        "        res_orchard = subgroup_config['results_df'].copy()\n",
        "\n",
        "        analysis_type = subgroup_config.get('analysis_type', 'single')\n",
        "        mod1_name = subgroup_config.get('moderator1', 'Subgroup')\n",
        "        mod2_name = subgroup_config.get('moderator2', None)\n",
        "        has_subgroups = True\n",
        "\n",
        "        # Sort Subgroups\n",
        "        if analysis_type == 'two_way' and mod2_name:\n",
        "            if mod1_name in res_orchard.columns and mod2_name in res_orchard.columns:\n",
        "                res_orchard = res_orchard.sort_values([mod1_name, mod2_name]).reset_index(drop=True)\n",
        "            default_title = f\"Orchard Plot: {mod1_name} × {mod2_name}\"\n",
        "            default_y_label = mod2_name\n",
        "        else:\n",
        "            res_orchard = res_orchard.sort_values('group').reset_index(drop=True)\n",
        "            default_title = f\"Orchard Plot: {mod1_name}\"\n",
        "            default_y_label = mod1_name\n",
        "    else:\n",
        "        # No subgroups\n",
        "        res_orchard = pd.DataFrame()\n",
        "        mod1_name = 'Subgroup'\n",
        "        mod2_name = None\n",
        "        has_subgroups = False\n",
        "        analysis_type = 'single'\n",
        "        default_title = \"Orchard Plot: Overall Effect\"\n",
        "        default_y_label = \"Groups\"\n",
        "\n",
        "    default_x_label = es_config.get('effect_label', \"Effect Size\")\n",
        "\n",
        "    # --- ADD OVERALL ROW ---\n",
        "    overall_row = pd.DataFrame([{\n",
        "        'group': 'Overall',\n",
        "        'pooled_effect_re': overall_res['pooled_effect_random'],\n",
        "        'pooled_se_re': overall_res.get('pooled_SE_random_reported', 0.1),\n",
        "        'tau_squared': overall_res.get('tau_squared', 0),\n",
        "        'sigma_squared': overall_res.get('sigma_squared', 0),\n",
        "        'k': overall_res['k'],\n",
        "        'n_papers': overall_res.get('k_papers', overall_res['k']),\n",
        "        mod1_name: 'Overall',\n",
        "        mod2_name: 'Overall' if mod2_name else None\n",
        "    }])\n",
        "\n",
        "    # Combine: Overall first, then subgroups\n",
        "    plot_df_final = pd.concat([overall_row, res_orchard], ignore_index=True)\n",
        "\n",
        "    # Setup Columns for Raw Data\n",
        "    effect_col = es_config.get('es_col', 'hedges_g')\n",
        "    var_col = es_config.get('var_col', 'Vg')\n",
        "    df_orchard = df_orchard.dropna(subset=[effect_col, var_col])\n",
        "\n",
        "    # --- UPDATED: Dynamic Inference (Row-by-Row) ---\n",
        "    # 1. Get Global Settings\n",
        "    if 'ANALYSIS_CONFIG' in globals():\n",
        "        gs = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "        alpha = gs.get('alpha', 0.05)\n",
        "        dist_type = gs.get('dist_type', 'norm')\n",
        "    else:\n",
        "        alpha = 0.05\n",
        "        dist_type = 'norm'\n",
        "\n",
        "    q_val = 1 - (alpha / 2)\n",
        "\n",
        "    # 2. Define helper to get critical value per subgroup\n",
        "    def get_critical_value(row):\n",
        "        if dist_type == 't':\n",
        "            # Degrees of Freedom = Number of Papers - 1 (Conservative for 3-Level)\n",
        "            # Fallback to 'k' if 'n_papers' is missing\n",
        "            n = row.get('n_papers', row.get('k', 2))\n",
        "            df = max(1, int(n) - 1)\n",
        "            return t.ppf(q_val, df)\n",
        "        else:\n",
        "            # Normal Distribution (Z)\n",
        "            return norm.ppf(q_val)\n",
        "\n",
        "    # 3. Apply to create a dynamic 'crit_val' column\n",
        "    plot_df_final['crit_val'] = plot_df_final.apply(get_critical_value, axis=1)\n",
        "\n",
        "    # 4. Calculate PIs and CIs using the dynamic critical value\n",
        "    sigma_sq = plot_df_final.get('sigma_squared', plot_df_final.get('sigma_2', 0)).fillna(0)\n",
        "    plot_df_final['PI_SD'] = np.sqrt(plot_df_final['pooled_se_re']**2 + plot_df_final['tau_squared'] + sigma_sq)\n",
        "\n",
        "    # Use the dynamic 'crit_val' instead of 1.96\n",
        "    plot_df_final['PI_Lower'] = plot_df_final['pooled_effect_re'] - plot_df_final['crit_val'] * plot_df_final['PI_SD']\n",
        "    plot_df_final['PI_Upper'] = plot_df_final['pooled_effect_re'] + plot_df_final['crit_val'] * plot_df_final['PI_SD']\n",
        "\n",
        "    plot_df_final['CI_Lower'] = plot_df_final['pooled_effect_re'] - plot_df_final['crit_val'] * plot_df_final['pooled_se_re']\n",
        "    plot_df_final['CI_Upper'] = plot_df_final['pooled_effect_re'] + plot_df_final['crit_val'] * plot_df_final['pooled_se_re']\n",
        "    # -----------------------------------------------\n",
        "    print(f\"✓ Analysis type: {analysis_type}\")\n",
        "    print(f\"✓ Has subgroups: {has_subgroups}\")\n",
        "    print(f\"✓ Total groups: {len(plot_df_final)}\")\n",
        "    print(f\"✓ Configuration loaded successfully\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Initialization Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    plot_df_final = None\n",
        "\n",
        "# --- 2. DEFINE CUSTOMIZATION WIDGETS ---\n",
        "\n",
        "# ========== TAB 1: STYLE & LAYOUT ==========\n",
        "style_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>🎨 Style & Layout</h3>\")\n",
        "\n",
        "# Dimensions Section\n",
        "width_widget = widgets.FloatSlider(\n",
        "    value=10.0, min=6.0, max=16.0, step=0.5,\n",
        "    description='Plot Width (in):',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "height_widget = widgets.FloatSlider(\n",
        "    value=max(6.0, len(plot_df_final)*0.6 if plot_df_final is not None else 6.0),\n",
        "    min=4.0, max=30.0, step=0.5,\n",
        "    description='Plot Height (in):',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Typography Section\n",
        "title_fontsize_widget = widgets.IntSlider(\n",
        "    value=14, min=8, max=24, step=1,\n",
        "    description='Title Font Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "label_fontsize_widget = widgets.IntSlider(\n",
        "    value=12, min=8, max=18, step=1,\n",
        "    description='Axis Label Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "tick_fontsize_widget = widgets.IntSlider(\n",
        "    value=10, min=6, max=16, step=1,\n",
        "    description='Tick Label Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "annot_fontsize_widget = widgets.IntSlider(\n",
        "    value=9, min=6, max=14, step=1,\n",
        "    description='Annotation Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Visual Style Section\n",
        "color_scheme_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Viridis (Default)', 'viridis'),\n",
        "        ('Plasma', 'plasma'),\n",
        "        ('Coolwarm', 'coolwarm'),\n",
        "        ('Grayscale (Publication)', 'gray'),\n",
        "        ('Color Palette (Tab10)', 'tab10'),\n",
        "        ('Color Palette (Set2)', 'Set2'),\n",
        "        ('Black & White Only', 'bw')\n",
        "    ],\n",
        "    value='viridis',\n",
        "    description='Color Scheme:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "marker_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Circle/Diamond (●/◆)', 'circle_diamond'),\n",
        "        ('Square/Diamond (■/◆)', 'square_diamond'),\n",
        "        ('Circle/Star (●/★)', 'circle_star')\n",
        "    ],\n",
        "    value='circle_diamond',\n",
        "    description='Marker Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "ci_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Solid Line', 'solid'),\n",
        "        ('Dashed Line', 'dashed'),\n",
        "        ('Solid with Caps', 'caps')\n",
        "    ],\n",
        "    value='solid',\n",
        "    description='CI Line Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Orientation Section\n",
        "orientation_widget = widgets.Dropdown(\n",
        "    options=[('Horizontal', 'h'), ('Vertical', 'v')],\n",
        "    value='h',\n",
        "    description='Orientation:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "style_tab = widgets.VBox([\n",
        "    style_header,\n",
        "    widgets.HTML(\"<b>Dimensions:</b>\"),\n",
        "    width_widget,\n",
        "    height_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Typography:</b>\"),\n",
        "    title_fontsize_widget,\n",
        "    label_fontsize_widget,\n",
        "    tick_fontsize_widget,\n",
        "    annot_fontsize_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Visual Style:</b>\"),\n",
        "    color_scheme_widget,\n",
        "    marker_style_widget,\n",
        "    ci_style_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Orientation:</b>\"),\n",
        "    orientation_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 2: TEXT & LABELS ==========\n",
        "text_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>📝 Text & Labels</h3>\")\n",
        "\n",
        "show_title_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Plot Title',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "title_widget = widgets.Text(\n",
        "    value=default_title,\n",
        "    description='Plot Title:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "xlabel_widget = widgets.Text(\n",
        "    value=default_x_label,\n",
        "    description='X-Axis Label:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "show_ylabel_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Y-Axis Label',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "ylabel_widget = widgets.Text(\n",
        "    value=default_y_label,\n",
        "    description='Y-Axis Label:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "text_tab = widgets.VBox([\n",
        "    text_header,\n",
        "    widgets.HTML(\"<b>Title:</b>\"),\n",
        "    show_title_widget,\n",
        "    title_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Axis Labels:</b>\"),\n",
        "    xlabel_widget,\n",
        "    show_ylabel_widget,\n",
        "    ylabel_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 3: ORCHARD ELEMENTS ==========\n",
        "orchard_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>🍎 Orchard Elements</h3>\")\n",
        "\n",
        "# Raw Data Points Section\n",
        "point_alpha_widget = widgets.FloatSlider(\n",
        "    value=0.5, min=0.1, max=1.0, step=0.05,\n",
        "    description='Point Alpha:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "point_size_widget = widgets.IntSlider(\n",
        "    value=50, min=10, max=200, step=5,\n",
        "    description='Point Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "jitter_widget = widgets.FloatSlider(\n",
        "    value=0.10, min=0.01, max=0.4, step=0.01,\n",
        "    description='Jitter Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "scale_points_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Scale Points by Weight',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Pooled Effects Visualization Section\n",
        "show_pi_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show PI - Prediction Interval (Trunk)',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "pi_linewidth_widget = widgets.FloatSlider(\n",
        "    value=4.0, min=1.0, max=8.0, step=0.5,\n",
        "    description='PI Line Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "pi_alpha_widget = widgets.FloatSlider(\n",
        "    value=0.3, min=0.1, max=1.0, step=0.05,\n",
        "    description='PI Alpha:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "show_ci_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show CI - Confidence Interval (Fruit)',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "ci_linewidth_widget = widgets.FloatSlider(\n",
        "    value=2.0, min=1.0, max=5.0, step=0.5,\n",
        "    description='CI Line Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Annotations Section\n",
        "show_k_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show k (observations)',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "show_papers_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Paper Count',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "show_effect_value_widget = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Show Effect Size Value',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "annot_pos_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Right of Interval', 'right'),\n",
        "        ('Above Marker', 'above'),\n",
        "        ('Below Marker', 'below')\n",
        "    ],\n",
        "    value='right',\n",
        "    description='Position:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "annot_h_offset_widget = widgets.FloatSlider(\n",
        "    value=0.0, min=-1.0, max=1.0, step=0.05,\n",
        "    description='H-Offset:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    readout_format='.2f'\n",
        ")\n",
        "\n",
        "annot_v_offset_widget = widgets.FloatSlider(\n",
        "    value=0.0, min=-1.0, max=1.0, step=0.05,\n",
        "    description='V-Offset:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    readout_format='.2f'\n",
        ")\n",
        "\n",
        "# Group Organization Section\n",
        "group_gap_widget = widgets.FloatSlider(\n",
        "    value=1.0, min=0.0, max=3.0, step=0.25,\n",
        "    description='Group Gap:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "show_inner_labels_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Inner Group Labels',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "group_label_h_offset_widget = widgets.FloatSlider(\n",
        "    value=0.0, min=-2.0, max=2.0, step=0.1,\n",
        "    description='Group H-Offset:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "group_label_v_offset_widget = widgets.FloatSlider(\n",
        "    value=0.0, min=-2.0, max=2.0, step=0.1,\n",
        "    description='Group V-Offset:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "group_label_fontsize_widget = widgets.IntSlider(\n",
        "    value=12, min=6, max=20, step=1,\n",
        "    description='Group Fontsize:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "orchard_tab = widgets.VBox([\n",
        "    orchard_header,\n",
        "    widgets.HTML(\"<b>Raw Data Points (Leaves):</b>\"),\n",
        "    point_alpha_widget,\n",
        "    point_size_widget,\n",
        "    jitter_widget,\n",
        "    scale_points_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Pooled Effects Visualization:</b>\"),\n",
        "    show_pi_widget,\n",
        "    pi_linewidth_widget,\n",
        "    pi_alpha_widget,\n",
        "    show_ci_widget,\n",
        "    ci_linewidth_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Annotations:</b>\"),\n",
        "    show_k_widget,\n",
        "    show_papers_widget,\n",
        "    show_effect_value_widget,\n",
        "    annot_pos_widget,\n",
        "    annot_h_offset_widget,\n",
        "    annot_v_offset_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Group Organization:</b>\"),\n",
        "    group_gap_widget,\n",
        "    show_inner_labels_widget,\n",
        "    group_label_h_offset_widget,\n",
        "    group_label_v_offset_widget,\n",
        "    group_label_fontsize_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 4: AXES & GRID ==========\n",
        "axes_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>📏 Axes & Grid</h3>\")\n",
        "\n",
        "# Axis Scaling Section\n",
        "auto_scale_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Auto-Scale Axis',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "axis_min_widget = widgets.FloatText(\n",
        "    value=-2.0,\n",
        "    description='Axis Min:',\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width='220px', visibility='hidden')\n",
        ")\n",
        "\n",
        "axis_max_widget = widgets.FloatText(\n",
        "    value=2.0,\n",
        "    description='Axis Max:',\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width='220px', visibility='hidden')\n",
        ")\n",
        "\n",
        "manual_scale_box = widgets.HBox([axis_min_widget, axis_max_widget])\n",
        "\n",
        "def toggle_manual_scale(change):\n",
        "    if change['new']:\n",
        "        axis_min_widget.layout.visibility = 'hidden'\n",
        "        axis_max_widget.layout.visibility = 'hidden'\n",
        "    else:\n",
        "        axis_min_widget.layout.visibility = 'visible'\n",
        "        axis_max_widget.layout.visibility = 'visible'\n",
        "\n",
        "auto_scale_widget.observe(toggle_manual_scale, names='value')\n",
        "\n",
        "# Grid & Reference Lines Section\n",
        "show_grid_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Grid',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "grid_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Dashed (Light)', 'dashed'),\n",
        "        ('Dotted (Light)', 'dotted'),\n",
        "        ('Solid (Light)', 'solid')\n",
        "    ],\n",
        "    value='dashed',\n",
        "    description='Grid Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "show_null_line_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Null Effect Line',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "null_line_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Solid', 'solid'),\n",
        "        ('Dashed', 'dashed')\n",
        "    ],\n",
        "    value='solid',\n",
        "    description='Null Line Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "null_line_width_widget = widgets.FloatSlider(\n",
        "    value=1.5, min=0.5, max=3.0, step=0.25,\n",
        "    description='Null Line Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Separator Lines Section\n",
        "show_separators_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Group Separators',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "separator_width_widget = widgets.FloatSlider(\n",
        "    value=0.8, min=0.5, max=2.0, step=0.1,\n",
        "    description='Separator Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "separator_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Solid', 'solid'),\n",
        "        ('Dashed', 'dashed')\n",
        "    ],\n",
        "    value='solid',\n",
        "    description='Separator Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "axes_tab = widgets.VBox([\n",
        "    axes_header,\n",
        "    widgets.HTML(\"<b>Axis Scaling:</b>\"),\n",
        "    auto_scale_widget,\n",
        "    manual_scale_box,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Grid & Reference Lines:</b>\"),\n",
        "    show_grid_widget,\n",
        "    grid_style_widget,\n",
        "    show_null_line_widget,\n",
        "    null_line_style_widget,\n",
        "    null_line_width_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Separator Lines:</b>\"),\n",
        "    show_separators_widget,\n",
        "    separator_width_widget,\n",
        "    separator_style_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 5: EXPORT OPTIONS ==========\n",
        "export_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>💾 Export Options</h3>\")\n",
        "\n",
        "save_pdf_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Save as PDF',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "save_png_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Save as PNG',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "png_dpi_widget = widgets.IntSlider(\n",
        "    value=300, min=150, max=600, step=50,\n",
        "    description='PNG DPI:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "filename_widget = widgets.Text(\n",
        "    value='OrchardPlot',\n",
        "    description='Filename Prefix:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "transparent_bg_widget = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Transparent Background',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "include_timestamp_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Include Timestamp',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "export_tab = widgets.VBox([\n",
        "    export_header,\n",
        "    widgets.HTML(\"<b>File Formats:</b>\"),\n",
        "    save_pdf_widget,\n",
        "    save_png_widget,\n",
        "    png_dpi_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>File Settings:</b>\"),\n",
        "    filename_widget,\n",
        "    transparent_bg_widget,\n",
        "    include_timestamp_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 6: LABEL EDITOR ==========\n",
        "label_editor_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>✏️ Label Editor</h3>\")\n",
        "label_editor_desc = widgets.HTML(\n",
        "    \"<p style='color: #666;'><i>Customize display names for all groups and subgroups in the plot</i></p>\"\n",
        ")\n",
        "\n",
        "print(f\"\\n🔍 Identifying labels for editor...\")\n",
        "\n",
        "unique_labels = set()\n",
        "label_widgets_dict = {}\n",
        "\n",
        "try:\n",
        "    if has_subgroups and plot_df_final is not None:\n",
        "        if analysis_type == 'single':\n",
        "            unique_labels.update(res_orchard['group'].astype(str).unique())\n",
        "        else:  # two_way\n",
        "            unique_labels.update(plot_df_final[mod1_name].astype(str).unique())\n",
        "            unique_labels.update(plot_df_final[mod2_name].astype(str).unique())\n",
        "\n",
        "    unique_labels.add('Overall')\n",
        "    sorted_labels = sorted(list(unique_labels))\n",
        "\n",
        "    print(f\"  ✓ Found {len(sorted_labels)} unique labels\")\n",
        "\n",
        "    label_editor_widgets = []\n",
        "    for label in sorted_labels:\n",
        "        widget_label = f\"Overall Effect:\" if label == 'Overall' else f\"{label}:\"\n",
        "        text_widget = widgets.Text(\n",
        "            value=str(label),\n",
        "            description=widget_label,\n",
        "            layout=widgets.Layout(width='500px'),\n",
        "            style={'description_width': '200px'}\n",
        "        )\n",
        "        label_editor_widgets.append(text_widget)\n",
        "        label_widgets_dict[str(label)] = text_widget\n",
        "\n",
        "    label_editor_tab = widgets.VBox([\n",
        "        label_editor_header,\n",
        "        label_editor_desc,\n",
        "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "        widgets.HTML(\n",
        "            \"<p><b>Instructions:</b> Edit the text on the right to change how labels appear in the plot. \"\n",
        "            \"The original coded names are shown on the left.</p>\"\n",
        "        ),\n",
        "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "        *label_editor_widgets\n",
        "    ])\n",
        "\n",
        "    print(f\"  ✓ Label editor created\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  ⚠️  Error creating label editor: {e}\")\n",
        "    label_editor_tab = widgets.VBox([\n",
        "        label_editor_header,\n",
        "        widgets.HTML(\"<p style='color: red;'>Error creating label editor.</p>\")\n",
        "    ])\n",
        "    label_widgets_dict = {}\n",
        "\n",
        "# ========== CREATE TAB WIDGET ==========\n",
        "tab_children = [style_tab, text_tab, orchard_tab, axes_tab, export_tab, label_editor_tab]\n",
        "tab = widgets.Tab(children=tab_children)\n",
        "tab.set_title(0, '🎨 Style')\n",
        "tab.set_title(1, '📝 Text')\n",
        "tab.set_title(2, '🍎 Orchard')\n",
        "tab.set_title(3, '📏 Axes')\n",
        "tab.set_title(4, '💾 Export')\n",
        "tab.set_title(5, '✏️ Labels')\n",
        "\n",
        "# --- 3. DEFINE PLOT GENERATION FUNCTION ---\n",
        "plot_output = widgets.Output()\n",
        "\n",
        "def generate_orchard_plot(b):\n",
        "    with plot_output:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"GENERATING ORCHARD PLOT\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        if plot_df_final is None:\n",
        "            print(\"❌ ERROR: No data available\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # --- GET WIDGET VALUES ---\n",
        "            plot_width = width_widget.value\n",
        "            plot_height = height_widget.value\n",
        "            title_fontsize = title_fontsize_widget.value\n",
        "            label_fontsize = label_fontsize_widget.value\n",
        "            tick_fontsize = tick_fontsize_widget.value\n",
        "            annot_fontsize = annot_fontsize_widget.value\n",
        "            color_scheme = color_scheme_widget.value\n",
        "            marker_style = marker_style_widget.value\n",
        "            ci_style = ci_style_widget.value\n",
        "            orientation = orientation_widget.value\n",
        "\n",
        "            show_title = show_title_widget.value\n",
        "            graph_title = title_widget.value\n",
        "            x_label = xlabel_widget.value\n",
        "            show_ylabel = show_ylabel_widget.value\n",
        "            y_label = ylabel_widget.value\n",
        "\n",
        "            point_alpha = point_alpha_widget.value\n",
        "            point_size = point_size_widget.value\n",
        "            jitter_amount = jitter_widget.value\n",
        "            scale_points = scale_points_widget.value\n",
        "\n",
        "            show_pi = show_pi_widget.value\n",
        "            pi_linewidth = pi_linewidth_widget.value\n",
        "            pi_alpha = pi_alpha_widget.value\n",
        "            show_ci = show_ci_widget.value\n",
        "            ci_linewidth = ci_linewidth_widget.value\n",
        "\n",
        "            show_k = show_k_widget.value\n",
        "            show_papers = show_papers_widget.value\n",
        "            show_effect_value = show_effect_value_widget.value\n",
        "            annot_pos = annot_pos_widget.value\n",
        "            annot_h_offset = annot_h_offset_widget.value\n",
        "            annot_v_offset = annot_v_offset_widget.value\n",
        "\n",
        "            group_gap = group_gap_widget.value\n",
        "            show_inner_labels = show_inner_labels_widget.value\n",
        "            group_label_h_offset = group_label_h_offset_widget.value\n",
        "            group_label_v_offset = group_label_v_offset_widget.value\n",
        "            group_label_fontsize = group_label_fontsize_widget.value\n",
        "\n",
        "            auto_scale = auto_scale_widget.value\n",
        "            axis_min_manual = axis_min_widget.value\n",
        "            axis_max_manual = axis_max_widget.value\n",
        "            show_grid = show_grid_widget.value\n",
        "            grid_style = grid_style_widget.value\n",
        "            show_null_line = show_null_line_widget.value\n",
        "            null_line_style = null_line_style_widget.value\n",
        "            null_line_width = null_line_width_widget.value\n",
        "\n",
        "            show_separators = show_separators_widget.value\n",
        "            separator_width = separator_width_widget.value\n",
        "            separator_style = separator_style_widget.value\n",
        "\n",
        "            save_pdf = save_pdf_widget.value\n",
        "            save_png = save_png_widget.value\n",
        "            png_dpi = png_dpi_widget.value\n",
        "            filename_prefix = filename_widget.value\n",
        "            transparent_bg = transparent_bg_widget.value\n",
        "            include_timestamp = include_timestamp_widget.value\n",
        "\n",
        "            # --- BUILD LABEL MAPPING ---\n",
        "            label_mapping = {}\n",
        "            for original_label, widget in label_widgets_dict.items():\n",
        "                custom_label = widget.value\n",
        "                label_mapping[original_label] = custom_label\n",
        "                label_mapping[str(original_label)] = custom_label\n",
        "\n",
        "            print(f\"📊 Configuration:\")\n",
        "            print(f\"  Dimensions: {plot_width}\\\" × {plot_height}\\\"\")\n",
        "            print(f\"  Orientation: {orientation}\")\n",
        "            print(f\"  Color scheme: {color_scheme}\")\n",
        "            print(f\"  Has subgroups: {has_subgroups}\")\n",
        "\n",
        "            # Show custom labels if any were changed\n",
        "            changed_labels = {k: v for k, v in label_mapping.items() if k != v}\n",
        "            if changed_labels:\n",
        "                print(f\"\\n📝 Custom labels ({len(changed_labels)} changed):\")\n",
        "                for orig, custom in list(changed_labels.items())[:5]:\n",
        "                    print(f\"  '{orig}' → '{custom}'\")\n",
        "                if len(changed_labels) > 5:\n",
        "                    print(f\"  ... and {len(changed_labels)-5} more\")\n",
        "\n",
        "            # --- PREPARE DATA ---\n",
        "            plot_df = plot_df_final.copy()\n",
        "            raw_df = df_orchard.copy()\n",
        "\n",
        "            # --- PREPARE MATCH KEYS ---\n",
        "            if has_subgroups:\n",
        "                if analysis_type == 'two_way' and mod2_name:\n",
        "                    raw_df['MatchKey'] = raw_df[mod1_name].astype(str) + \" x \" + raw_df[mod2_name].astype(str)\n",
        "                    if len(set(raw_df['MatchKey']).intersection(set(plot_df['group']))) == 0:\n",
        "                        raw_df['MatchKey'] = raw_df[mod1_name].astype(str) + \" × \" + raw_df[mod2_name].astype(str)\n",
        "                else:\n",
        "                    raw_df['MatchKey'] = raw_df[mod1_name].astype(str)\n",
        "            else:\n",
        "                raw_df['MatchKey'] = 'Overall'\n",
        "\n",
        "            # --- CALCULATE POSITIONS & GAPS ---\n",
        "            is_horiz = (orientation == 'h')\n",
        "            groups = plot_df['group'].tolist()\n",
        "\n",
        "            if is_horiz:\n",
        "                groups = groups[::-1]\n",
        "\n",
        "            y_locs = []\n",
        "            display_labels = []\n",
        "            group_centers = {}\n",
        "\n",
        "            cursor = 0\n",
        "            last_mod1 = None\n",
        "            separators = []\n",
        "            current_group_locs = []\n",
        "\n",
        "            for grp in groups:\n",
        "                is_overall = (grp == 'Overall')\n",
        "\n",
        "                # Identify Mod1 and Mod2\n",
        "                if is_overall:\n",
        "                    curr_mod1 = \"Overall\"\n",
        "                    curr_mod2 = \"Overall\"\n",
        "                elif analysis_type == 'two_way' and mod1_name in plot_df.columns:\n",
        "                    row = plot_df[plot_df['group'] == grp].iloc[0]\n",
        "                    curr_mod1 = str(row[mod1_name])\n",
        "                    curr_mod2 = str(row[mod2_name])\n",
        "                else:\n",
        "                    curr_mod1 = \"Subgroups\"\n",
        "                    curr_mod2 = grp\n",
        "\n",
        "                # Gap Logic\n",
        "                if last_mod1 is not None and curr_mod1 != last_mod1:\n",
        "                    if last_mod1 not in group_centers and current_group_locs:\n",
        "                        group_centers[last_mod1] = np.mean(current_group_locs)\n",
        "                    current_group_locs = []\n",
        "\n",
        "                    cursor += group_gap\n",
        "                    sep_pos = cursor - (group_gap/2) - 0.5\n",
        "                    separators.append(sep_pos)\n",
        "\n",
        "                y_locs.append(cursor)\n",
        "                current_group_locs.append(cursor)\n",
        "\n",
        "                # Apply label mapping\n",
        "                display_label = label_mapping.get(str(curr_mod2), curr_mod2)\n",
        "                display_labels.append(display_label)\n",
        "\n",
        "                last_mod1 = curr_mod1\n",
        "                cursor += 1\n",
        "\n",
        "            # Catch last group center\n",
        "            if last_mod1 not in group_centers and current_group_locs:\n",
        "                group_centers[last_mod1] = np.mean(current_group_locs)\n",
        "\n",
        "            print(f\"  Total groups: {len(groups)}\")\n",
        "            print(f\"  Separators: {len(separators)}\")\n",
        "\n",
        "            # --- DETERMINE COLOR SCHEME ---\n",
        "            if color_scheme == 'gray':\n",
        "                cmap = plt.cm.gray\n",
        "            elif color_scheme == 'bw':\n",
        "                cmap = plt.cm.binary\n",
        "            else:\n",
        "                cmap = plt.get_cmap(color_scheme)\n",
        "\n",
        "            # --- DETERMINE MARKER STYLES ---\n",
        "            if marker_style == 'circle_diamond':\n",
        "                subgroup_marker = 'o'\n",
        "                overall_marker = 'D'\n",
        "            elif marker_style == 'square_diamond':\n",
        "                subgroup_marker = 's'\n",
        "                overall_marker = 'D'\n",
        "            else:  # circle_star\n",
        "                subgroup_marker = 'o'\n",
        "                overall_marker = '*'\n",
        "\n",
        "            # --- CREATE FIGURE ---\n",
        "            fig, ax = plt.subplots(figsize=(plot_width, plot_height))\n",
        "\n",
        "            if transparent_bg:\n",
        "                fig.patch.set_alpha(0)\n",
        "                ax.patch.set_alpha(0)\n",
        "\n",
        "            print(f\"\\n🎨 Plotting {len(groups)} groups...\")\n",
        "\n",
        "            # --- PLOT DATA ---\n",
        "            for i, (grp, pos) in enumerate(zip(groups, y_locs)):\n",
        "                row = plot_df[plot_df['group'] == grp].iloc[0]\n",
        "                is_overall = (grp == 'Overall')\n",
        "\n",
        "                # Color\n",
        "                if is_overall:\n",
        "                    if color_scheme == 'bw' or color_scheme == 'gray':\n",
        "                        col = 'black'\n",
        "                    else:\n",
        "                        col = 'black'\n",
        "                    marker = overall_marker\n",
        "                    alpha_trunk = 1.0\n",
        "                elif analysis_type == 'two_way' and mod1_name in plot_df.columns:\n",
        "                    unique_mod1 = sorted(plot_df[plot_df['group']!='Overall'][mod1_name].unique().astype(str).tolist())\n",
        "                    if is_horiz:\n",
        "                        unique_mod1 = unique_mod1[::-1]\n",
        "                    try:\n",
        "                        c_idx = unique_mod1.index(str(row[mod1_name]))\n",
        "                    except:\n",
        "                        c_idx = 0\n",
        "                    col = cmap(c_idx / max(1, len(unique_mod1)-1))\n",
        "                    marker = subgroup_marker\n",
        "                    alpha_trunk = pi_alpha\n",
        "                else:\n",
        "                    col = cmap(i/max(1, len(groups)-1))\n",
        "                    marker = subgroup_marker\n",
        "                    alpha_trunk = pi_alpha\n",
        "\n",
        "                # Trunk (PI)\n",
        "                if show_pi:\n",
        "                    trunk_lw = pi_linewidth if not is_overall else pi_linewidth * 1.2\n",
        "                    if is_horiz:\n",
        "                        ax.plot([row['PI_Lower'], row['PI_Upper']], [pos, pos],\n",
        "                               color=col, alpha=alpha_trunk, lw=trunk_lw,\n",
        "                               solid_capstyle='round', zorder=1)\n",
        "                    else:\n",
        "                        ax.plot([pos, pos], [row['PI_Lower'], row['PI_Upper']],\n",
        "                               color=col, alpha=alpha_trunk, lw=trunk_lw,\n",
        "                               solid_capstyle='round', zorder=1)\n",
        "\n",
        "                # Leaves (Raw Data)\n",
        "                if is_overall:\n",
        "                    sub_data = raw_df\n",
        "                else:\n",
        "                    sub_data = raw_df[raw_df['MatchKey'] == grp]\n",
        "\n",
        "                if not sub_data.empty:\n",
        "                    rng = np.random.default_rng(42+i)\n",
        "                    jitter = rng.uniform(-jitter_amount, jitter_amount, size=len(sub_data))\n",
        "                    sizes = point_size\n",
        "                    if scale_points:\n",
        "                        w = 1 / sub_data[var_col]\n",
        "                        sizes = point_size * (w / w.mean())\n",
        "                        sizes = sizes.clip(10, point_size*3)\n",
        "\n",
        "                    pt_col = 'gray' if is_overall else col\n",
        "                    pt_alpha = point_alpha * 0.5 if is_overall else point_alpha\n",
        "\n",
        "                    if is_horiz:\n",
        "                        ax.scatter(sub_data[effect_col], np.full(len(sub_data), pos)+jitter,\n",
        "                                 s=sizes, color=pt_col, alpha=pt_alpha, edgecolor='none', zorder=2)\n",
        "                    else:\n",
        "                        ax.scatter(np.full(len(sub_data), pos)+jitter, sub_data[effect_col],\n",
        "                                 s=sizes, color=pt_col, alpha=pt_alpha, edgecolor='none', zorder=2)\n",
        "\n",
        "                # Fruit (CI and Mean)\n",
        "                msize = point_size * 2.5 if is_overall else point_size * 2\n",
        "\n",
        "                if show_ci:\n",
        "                    ci_lw = ci_linewidth if not is_overall else ci_linewidth * 1.2\n",
        "                    ci_ls = '-' if ci_style == 'solid' else '--'\n",
        "                    ci_capsize = 4 if ci_style == 'caps' else 0\n",
        "\n",
        "                    if is_horiz:\n",
        "                        ax.plot([row['CI_Lower'], row['CI_Upper']], [pos, pos],\n",
        "                               color='black', lw=ci_lw, linestyle=ci_ls, zorder=3)\n",
        "                        if ci_capsize > 0:\n",
        "                            ax.plot([row['CI_Lower'], row['CI_Lower']], [pos-0.1, pos+0.1],\n",
        "                                   color='black', lw=ci_lw, zorder=3)\n",
        "                            ax.plot([row['CI_Upper'], row['CI_Upper']], [pos-0.1, pos+0.1],\n",
        "                                   color='black', lw=ci_lw, zorder=3)\n",
        "                    else:\n",
        "                        ax.plot([pos, pos], [row['CI_Lower'], row['CI_Upper']],\n",
        "                               color='black', lw=ci_lw, linestyle=ci_ls, zorder=3)\n",
        "                        if ci_capsize > 0:\n",
        "                            ax.plot([pos-0.1, pos+0.1], [row['CI_Lower'], row['CI_Lower']],\n",
        "                                   color='black', lw=ci_lw, zorder=3)\n",
        "                            ax.plot([pos-0.1, pos+0.1], [row['CI_Upper'], row['CI_Upper']],\n",
        "                                   color='black', lw=ci_lw, zorder=3)\n",
        "\n",
        "                # Plot mean marker\n",
        "                if is_horiz:\n",
        "                    ax.plot(row['pooled_effect_re'], pos, marker=marker,\n",
        "                           markersize=np.sqrt(msize)*1.5, color=col,\n",
        "                           markeredgecolor='black', markeredgewidth=1.5, zorder=4)\n",
        "                else:\n",
        "                    ax.plot(pos, row['pooled_effect_re'], marker=marker,\n",
        "                           markersize=np.sqrt(msize)*1.5, color=col,\n",
        "                           markeredgecolor='black', markeredgewidth=1.5, zorder=4)\n",
        "\n",
        "                # Annotations\n",
        "                if show_k or show_papers or show_effect_value:\n",
        "                    annot_parts = []\n",
        "                    if show_k:\n",
        "                        annot_parts.append(f\"k={int(row['k'])}\")\n",
        "                    if show_papers and pd.notna(row.get('n_papers')):\n",
        "                        annot_parts.append(f\"({int(row['n_papers'])})\")\n",
        "                    if show_effect_value:\n",
        "                        annot_parts.append(f\"ES={row['pooled_effect_re']:.2f}\")\n",
        "\n",
        "                    annotation_text = \" \".join(annot_parts)\n",
        "                    font_w = 'bold' if is_overall else 'normal'\n",
        "\n",
        "                    if is_horiz:\n",
        "                        if annot_pos == 'right':\n",
        "                            label_x = (row['PI_Upper'] if show_pi else row['CI_Upper']) + annot_h_offset\n",
        "                            label_y = pos + annot_v_offset\n",
        "                            ha, va = 'left', 'center'\n",
        "                        elif annot_pos == 'above':\n",
        "                            label_x = row['pooled_effect_re'] + annot_h_offset\n",
        "                            label_y = pos - 0.2 + annot_v_offset\n",
        "                            ha, va = 'center', 'bottom'\n",
        "                        else:  # below\n",
        "                            label_x = row['pooled_effect_re'] + annot_h_offset\n",
        "                            label_y = pos + 0.2 + annot_v_offset\n",
        "                            ha, va = 'center', 'top'\n",
        "\n",
        "                        ax.text(label_x, label_y, annotation_text,\n",
        "                               fontsize=annot_fontsize, fontweight=font_w,\n",
        "                               ha=ha, va=va, clip_on=False)\n",
        "                    else:\n",
        "                        if annot_pos == 'right':\n",
        "                            label_x = pos + annot_h_offset\n",
        "                            label_y = (row['PI_Upper'] if show_pi else row['CI_Upper']) + annot_v_offset\n",
        "                            ha, va = 'center', 'bottom'\n",
        "                        else:\n",
        "                            label_x = pos + annot_h_offset\n",
        "                            label_y = row['pooled_effect_re'] + annot_v_offset\n",
        "                            ha, va = 'center', 'center'\n",
        "\n",
        "                        ax.text(label_x, label_y, annotation_text,\n",
        "                               fontsize=annot_fontsize, fontweight=font_w,\n",
        "                               ha=ha, va=va, clip_on=False)\n",
        "\n",
        "            # --- ADD SEPARATORS ---\n",
        "            if show_separators:\n",
        "                sep_ls = '-' if separator_style == 'solid' else '--'\n",
        "                for sep in separators:\n",
        "                    if is_horiz:\n",
        "                        ax.axhline(sep, color='black', linestyle=sep_ls,\n",
        "                                  lw=separator_width, alpha=1.0)\n",
        "                    else:\n",
        "                        ax.axvline(sep, color='black', linestyle=sep_ls,\n",
        "                                  lw=separator_width, alpha=1.0)\n",
        "\n",
        "            # --- ADD INNER LABELS ---\n",
        "            if show_inner_labels:\n",
        "                for mod1_label, center_pos in group_centers.items():\n",
        "                    if mod1_label == 'Overall':\n",
        "                        continue\n",
        "\n",
        "                    # Apply label mapping\n",
        "                    display_mod1_label = label_mapping.get(str(mod1_label), mod1_label)\n",
        "\n",
        "                    if is_horiz:\n",
        "                        label_x = 0.98 + group_label_h_offset * 0.01\n",
        "                        label_y = center_pos + group_label_v_offset\n",
        "                        ax.text(label_x, label_y, display_mod1_label,\n",
        "                               transform=ax.get_yaxis_transform(),\n",
        "                               ha='right', va='center', fontweight='bold',\n",
        "                               fontsize=group_label_fontsize)\n",
        "                    else:\n",
        "                        label_x = center_pos + group_label_h_offset\n",
        "                        label_y = 0.98 + group_label_v_offset * 0.01\n",
        "                        ax.text(label_x, label_y, display_mod1_label,\n",
        "                               transform=ax.get_xaxis_transform(),\n",
        "                               ha='center', va='top', fontweight='bold',\n",
        "                               fontsize=group_label_fontsize)\n",
        "\n",
        "            # --- AXIS SETTINGS ---\n",
        "            if is_horiz:\n",
        "                ax.set_yticks(y_locs)\n",
        "                # Bold the 'Overall' label\n",
        "                labels_formatted = [\n",
        "                    r\"$\\bf{\" + l.replace(' ', r'\\ ') + \"}$\" if l == label_mapping.get('Overall', 'Overall') else l\n",
        "                    for l in display_labels\n",
        "                ]\n",
        "                ax.set_yticklabels(labels_formatted, fontsize=tick_fontsize)\n",
        "                ax.set_xlabel(x_label, fontweight='bold', fontsize=label_fontsize)\n",
        "                if show_ylabel:\n",
        "                    ax.set_ylabel(y_label, fontweight='bold', fontsize=label_fontsize)\n",
        "\n",
        "                # Null line\n",
        "                if show_null_line:\n",
        "                    null_ls = '-' if null_line_style == 'solid' else '--'\n",
        "                    ax.axvline(0, color='black', lw=null_line_width, linestyle=null_ls)\n",
        "\n",
        "                # Grid\n",
        "                if show_grid:\n",
        "                    grid_ls = '--' if grid_style == 'dashed' else (':' if grid_style == 'dotted' else '-')\n",
        "                    ax.grid(axis='x', linestyle=grid_ls, alpha=0.3)\n",
        "\n",
        "                # Axis limits\n",
        "                if auto_scale:\n",
        "                    # Auto scale\n",
        "                    pass\n",
        "                else:\n",
        "                    ax.set_xlim(axis_min_manual, axis_max_manual)\n",
        "\n",
        "            else:  # Vertical\n",
        "                ax.set_xticks(y_locs)\n",
        "                ax.set_xticklabels(display_labels, fontsize=tick_fontsize, rotation=45, ha='right')\n",
        "                ax.set_ylabel(x_label, fontweight='bold', fontsize=label_fontsize)\n",
        "\n",
        "                # Null line\n",
        "                if show_null_line:\n",
        "                    null_ls = '-' if null_line_style == 'solid' else '--'\n",
        "                    ax.axhline(0, color='black', lw=null_line_width, linestyle=null_ls)\n",
        "\n",
        "                # Grid\n",
        "                if show_grid:\n",
        "                    grid_ls = '--' if grid_style == 'dashed' else (':' if grid_style == 'dotted' else '-')\n",
        "                    ax.grid(axis='y', linestyle=grid_ls, alpha=0.3)\n",
        "\n",
        "                # Axis limits\n",
        "                if auto_scale:\n",
        "                    pass\n",
        "                else:\n",
        "                    ax.set_ylim(axis_min_manual, axis_max_manual)\n",
        "\n",
        "            # Title\n",
        "            if show_title:\n",
        "                ax.set_title(graph_title, fontweight='bold', fontsize=title_fontsize, pad=15)\n",
        "\n",
        "            ax.tick_params(labelsize=tick_fontsize)\n",
        "            sns.despine(trim=True)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # --- SAVE FILES ---\n",
        "            print(f\"\\n💾 Saving files...\")\n",
        "\n",
        "            saved_files = []\n",
        "\n",
        "            if include_timestamp:\n",
        "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                base_filename = f\"{filename_prefix}_{timestamp}\"\n",
        "            else:\n",
        "                base_filename = filename_prefix\n",
        "\n",
        "            if save_pdf:\n",
        "                pdf_filename = f\"{base_filename}.pdf\"\n",
        "                fig.savefig(pdf_filename, bbox_inches='tight', transparent=transparent_bg)\n",
        "                saved_files.append(pdf_filename)\n",
        "                print(f\"  ✓ {pdf_filename}\")\n",
        "\n",
        "            if save_png:\n",
        "                png_filename = f\"{base_filename}.png\"\n",
        "                fig.savefig(png_filename, dpi=png_dpi, bbox_inches='tight', transparent=transparent_bg)\n",
        "                saved_files.append(png_filename)\n",
        "                print(f\"  ✓ {png_filename} (DPI: {png_dpi})\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            print(f\"\\n\" + \"=\"*70)\n",
        "            print(\"✅ ORCHARD PLOT COMPLETE\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"Files: {', '.join(saved_files)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ ERROR: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "# --- 4. CREATE BUTTON AND DISPLAY ---\n",
        "plot_button = widgets.Button(\n",
        "    description='🌳 Generate Orchard Plot',\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='450px', height='50px'),\n",
        "    style={'font_weight': 'bold'}\n",
        ")\n",
        "\n",
        "plot_button.on_click(generate_orchard_plot)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ ORCHARD PLOT INTERFACE READY\")\n",
        "print(\"=\"*70)\n",
        "print(\"👆 Customize your plot using the tabs above, then click Generate\")\n",
        "print(\"\\n📝 Tips:\")\n",
        "print(\"  • Use the 'Labels' tab to rename coded variables\")\n",
        "print(\"  • Adjust point size and jitter for optimal data visibility\")\n",
        "print(\"  • PI (Trunk) shows prediction interval, CI (Fruit) shows pooled effect\")\n",
        "print(\"  • Auto-scale considers all data for proper spacing\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='color: #2E86AB;'>🍎 Orchard Plot Generator (Enhanced)</h3>\"),\n",
        "    widgets.HTML(\"<p style='color: #666;'>Create publication-ready orchard plots with full customization</p>\"),\n",
        "    widgets.HTML(\"<hr style='margin: 15px 0;'>\"),\n",
        "    tab,\n",
        "    widgets.HTML(\"<hr style='margin: 15px 0;'>\"),\n",
        "    plot_button,\n",
        "    plot_output\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# COMPONENT 1: _get_gls_estimates (FULL REWRITE with VCV support)\n",
        "# =============================================================================\n",
        "\n",
        "def _get_gls_estimates(params, y_all, vcv_all, X_all, N_total, M_studies, p_params):\n",
        "    \"\"\"\n",
        "    Calculates Fixed Effects (Betas) using Generalized Least Squares (GLS).\n",
        "\n",
        "    Supports:\n",
        "      - Full VCV matrices for shared controls\n",
        "      - Diagonal matrices (independence assumption)\n",
        "      - Sherman-Morrison optimization for diagonal case\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    params : array-like\n",
        "        [tau_sq, sigma_sq] - variance components\n",
        "    y_all : list of arrays\n",
        "        Effect sizes for each study\n",
        "    vcv_all : list of 2D arrays\n",
        "        Sampling covariance matrices for each study (k_i × k_i)\n",
        "    X_all : list of 2D arrays\n",
        "        Design matrices for each study (k_i × p)\n",
        "    N_total : int\n",
        "        Total number of observations\n",
        "    M_studies : int\n",
        "        Number of studies\n",
        "    p_params : int\n",
        "        Number of fixed effect parameters (including intercept)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict with keys:\n",
        "        'betas', 'cov_beta', 'se_betas', 'log_lik_reml', 'tau_sq', 'sigma_sq'\n",
        "        Returns {'log_lik_reml': -np.inf} on failure\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tau_sq, sigma_sq = params\n",
        "\n",
        "        # Safety constraints\n",
        "        tau_sq = max(tau_sq, 1e-10)\n",
        "        sigma_sq = max(sigma_sq, 1e-10)\n",
        "\n",
        "        # Accumulators for GLS normal equations\n",
        "        # β = (X'Σ⁻¹X)⁻¹ X'Σ⁻¹y\n",
        "        sum_Xt_invS_X = np.zeros((p_params, p_params))\n",
        "        sum_Xt_invS_y = np.zeros(p_params)\n",
        "\n",
        "        # Accumulators for likelihood\n",
        "        sum_log_det = 0.0\n",
        "        sum_y_invS_y = 0.0\n",
        "\n",
        "        for i in range(M_studies):\n",
        "            y_i = np.asarray(y_all[i], dtype=np.float64)\n",
        "            X_i = np.asarray(X_all[i], dtype=np.float64)\n",
        "            V_i = np.asarray(vcv_all[i], dtype=np.float64)\n",
        "            k = len(y_i)\n",
        "\n",
        "            # Ensure V_i is 2D\n",
        "            if V_i.ndim == 1:\n",
        "                V_i = np.diag(V_i)\n",
        "\n",
        "            # Check if V_i is diagonal (use fast path)\n",
        "            is_diagonal = (k == 1) or np.allclose(V_i, np.diag(np.diag(V_i)))\n",
        "\n",
        "            if is_diagonal:\n",
        "                # =============================================================\n",
        "                # FAST PATH: Sherman-Morrison Formula\n",
        "                # Σ_i = diag(v_i + σ²) + τ²J  where J is all-ones matrix\n",
        "                # =============================================================\n",
        "                v_i = np.diag(V_i) if k > 1 else np.array([V_i[0, 0]])\n",
        "\n",
        "                # A = diag(v_i + σ²)\n",
        "                A_diag = v_i + sigma_sq\n",
        "                inv_A_diag = 1.0 / A_diag\n",
        "\n",
        "                # Sherman-Morrison: (A + τ²J)⁻¹ = A⁻¹ - (τ² A⁻¹ J A⁻¹)/(1 + τ² tr(A⁻¹))\n",
        "                sum_inv_A = np.sum(inv_A_diag)\n",
        "                denom = 1.0 + tau_sq * sum_inv_A\n",
        "\n",
        "                if denom <= 0:\n",
        "                    return {'log_lik_reml': -np.inf}\n",
        "\n",
        "                # Log determinant: log|A + τ²J| = log|A| + log(1 + τ² 1'A⁻¹1)\n",
        "                log_det = np.sum(np.log(A_diag)) + np.log(denom)\n",
        "\n",
        "                # Σ⁻¹X using Sherman-Morrison\n",
        "                # invS_X[j,:] = inv_A_diag[j] * X_i[j,:] - (τ²/denom) * inv_A_diag[j] * (sum over l of inv_A_diag[l] * X_i[l,:])\n",
        "                invA_X = X_i * inv_A_diag[:, np.newaxis]  # Element-wise: each row scaled\n",
        "                sum_invA_X = np.sum(invA_X, axis=0)  # Column sums\n",
        "                invS_X = invA_X - (tau_sq / denom) * np.outer(inv_A_diag, sum_invA_X)\n",
        "\n",
        "                # Σ⁻¹y using Sherman-Morrison\n",
        "                invA_y = y_i * inv_A_diag\n",
        "                sum_invA_y = np.sum(invA_y)\n",
        "                invS_y = invA_y - (tau_sq / denom) * inv_A_diag * sum_invA_y\n",
        "\n",
        "            else:\n",
        "                # =============================================================\n",
        "                # FULL PATH: Direct Matrix Inversion for Shared Controls\n",
        "                # Σ_i = V_i + σ²I + τ²J\n",
        "                # =============================================================\n",
        "                Sigma_i = V_i.copy()\n",
        "\n",
        "                # Add Level 2: σ² on diagonal\n",
        "                Sigma_i = Sigma_i + sigma_sq * np.eye(k)\n",
        "\n",
        "                # Add Level 3: τ² to all elements (compound symmetry)\n",
        "                Sigma_i = Sigma_i + tau_sq * np.ones((k, k))\n",
        "\n",
        "                # Inversion with numerical stability\n",
        "                try:\n",
        "                    # Check for positive definiteness\n",
        "                    eigvals = np.linalg.eigvalsh(Sigma_i)\n",
        "                    if np.min(eigvals) < 1e-10:\n",
        "                        # Add ridge for stability\n",
        "                        Sigma_i = Sigma_i + 1e-8 * np.eye(k)\n",
        "\n",
        "                    L = np.linalg.cholesky(Sigma_i)\n",
        "                    # Solve via Cholesky (more stable than explicit inverse)\n",
        "                    #inv_Sigma_i = scipy.linalg.cho_solve((L, True), np.eye(k)) // old\n",
        "                    inv_Sigma_i = np.linalg.solve(Sigma_i, np.eye(k))\n",
        "                    log_det = 2.0 * np.sum(np.log(np.diag(L)))\n",
        "\n",
        "                except np.linalg.LinAlgError:\n",
        "                    # Fallback: pseudo-inverse\n",
        "                    try:\n",
        "                        inv_Sigma_i = np.linalg.pinv(Sigma_i)\n",
        "                        sign, log_det = np.linalg.slogdet(Sigma_i)\n",
        "                        if sign <= 0:\n",
        "                            return {'log_lik_reml': -np.inf}\n",
        "                    except:\n",
        "                        return {'log_lik_reml': -np.inf}\n",
        "\n",
        "                # Compute weighted quantities\n",
        "                invS_X = inv_Sigma_i @ X_i\n",
        "                invS_y = inv_Sigma_i @ y_i\n",
        "\n",
        "            # =============================================================\n",
        "            # Accumulate across studies\n",
        "            # =============================================================\n",
        "            sum_Xt_invS_X += X_i.T @ invS_X      # X'Σ⁻¹X\n",
        "            sum_Xt_invS_y += X_i.T @ invS_y      # X'Σ⁻¹y\n",
        "            sum_y_invS_y += np.dot(y_i, invS_y)  # y'Σ⁻¹y\n",
        "            sum_log_det += log_det\n",
        "\n",
        "        # =============================================================\n",
        "        # Solve GLS Normal Equations: β = (X'Σ⁻¹X)⁻¹ X'Σ⁻¹y\n",
        "        # =============================================================\n",
        "        try:\n",
        "            # Add small ridge for numerical stability\n",
        "            ridge = 1e-10 * np.eye(p_params)\n",
        "            cov_beta = np.linalg.inv(sum_Xt_invS_X + ridge)\n",
        "            betas = cov_beta @ sum_Xt_invS_y\n",
        "        except np.linalg.LinAlgError:\n",
        "            # Heavier ridge if needed\n",
        "            try:\n",
        "                cov_beta = np.linalg.inv(sum_Xt_invS_X + 1e-6 * np.eye(p_params))\n",
        "                betas = cov_beta @ sum_Xt_invS_y\n",
        "            except:\n",
        "                return {'log_lik_reml': -np.inf}\n",
        "\n",
        "        # =============================================================\n",
        "        # REML Log-Likelihood\n",
        "        # =============================================================\n",
        "        # RSS = y'Σ⁻¹y - 2β'X'Σ⁻¹y + β'X'Σ⁻¹Xβ\n",
        "        #     = sum_y_invS_y - β'X'Σ⁻¹y  (simplified using normal equations)\n",
        "        rss = sum_y_invS_y - np.dot(betas, sum_Xt_invS_y)\n",
        "\n",
        "        # Ensure RSS is non-negative (numerical precision)\n",
        "        rss = max(rss, 0.0)\n",
        "\n",
        "        # log|X'Σ⁻¹X|\n",
        "        sign, log_det_XtSX = np.linalg.slogdet(sum_Xt_invS_X)\n",
        "        if sign <= 0:\n",
        "            log_det_XtSX = -50  # Penalize but don't crash\n",
        "\n",
        "        # REML log-likelihood (matches metafor::rma.mv)\n",
        "        # -2 * logLik_REML = (N-p)*log(2π) + log|Σ| + log|X'Σ⁻¹X| + RSS\n",
        "        log_lik_reml = -0.5 * (\n",
        "            (N_total - p_params) * np.log(2.0 * np.pi) +\n",
        "            sum_log_det +\n",
        "            log_det_XtSX +\n",
        "            rss\n",
        "        )\n",
        "\n",
        "        # =============================================================\n",
        "        # Standard Errors for Betas\n",
        "        # =============================================================\n",
        "        se_betas = np.sqrt(np.diag(cov_beta))\n",
        "\n",
        "        # Check for valid SEs\n",
        "        if np.any(np.isnan(se_betas)) or np.any(se_betas <= 0):\n",
        "            se_betas = np.full(p_params, np.nan)\n",
        "\n",
        "        return {\n",
        "            'betas': betas,\n",
        "            'cov_beta': cov_beta,\n",
        "            'se_betas': se_betas,\n",
        "            'log_lik_reml': log_lik_reml,\n",
        "            'tau_sq': tau_sq,\n",
        "            'sigma_sq': sigma_sq,\n",
        "            'rss': rss,\n",
        "            'sum_log_det': sum_log_det\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {'log_lik_reml': -np.inf}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENT 2: _neg_log_lik_reml_reg (Updated wrapper)\n",
        "# =============================================================================\n",
        "\n",
        "def _neg_log_lik_reml_reg(params, y_all, vcv_all, X_all, N_total, M_studies, p_params):\n",
        "    \"\"\"\n",
        "    Negative REML log-likelihood for optimization.\n",
        "\n",
        "    Now accepts vcv_all (list of matrices) instead of v_all (list of vectors).\n",
        "    \"\"\"\n",
        "    tau_sq = max(params[0], 1e-10)\n",
        "    sigma_sq = max(params[1], 1e-10)\n",
        "\n",
        "    est = _get_gls_estimates(\n",
        "        [tau_sq, sigma_sq], y_all, vcv_all, X_all, N_total, M_studies, p_params\n",
        "    )\n",
        "\n",
        "    ll = est.get('log_lik_reml', -np.inf)\n",
        "\n",
        "    if not np.isfinite(ll):\n",
        "        return 1e10\n",
        "\n",
        "    return -ll\n",
        "\n",
        "\n",
        "def _neg_log_lik_reml_reg_constrained(params, y_all, vcv_all, X_all, N_total, M_studies,\n",
        "                                       p_params, tau_sq_prior, penalty_weight):\n",
        "    \"\"\"\n",
        "    Constrained REML for constant-within-study moderators.\n",
        "    Adds penalty to prevent tau² from collapsing to zero.\n",
        "    \"\"\"\n",
        "    base_nll = _neg_log_lik_reml_reg(params, y_all, vcv_all, X_all, N_total, M_studies, p_params)\n",
        "\n",
        "    # Penalty: discourage tau² from going too far from the intercept-only estimate\n",
        "    tau_sq = max(params[0], 1e-10)\n",
        "    penalty = penalty_weight * (np.log(tau_sq) - np.log(tau_sq_prior)) ** 2\n",
        "\n",
        "    return base_nll + penalty\n",
        "\n",
        "#==============================================================================\n",
        "# Meta regression function\n",
        "#==============================================================================\n",
        "import scipy.linalg\n",
        "from scipy.optimize import minimize, minimize_scalar\n",
        "\n",
        "def _run_three_level_reml_regression_v2(analysis_data, moderator_col, effect_col, var_col):\n",
        "    \"\"\"\n",
        "    Robust 3-level REML meta-regression with Fallback Strategy.\n",
        "\n",
        "    Strategies:\n",
        "      - Plan A: Full 3-Level GLS with VCV Matrices\n",
        "      - Plan B: 3-Level GLS with Diagonal (Independence)\n",
        "      - Plan C: Aggregated 2-Level Regression (Fallback for Study-Level Moderators)\n",
        "    \"\"\"\n",
        "\n",
        "    # =================================================================\n",
        "    # 1. DATA PREPARATION\n",
        "    # =================================================================\n",
        "    analysis_data = analysis_data.sort_values(['id']).reset_index(drop=True)\n",
        "    vcv_dict = ANALYSIS_CONFIG.get('vcv_matrices', {})\n",
        "\n",
        "    grouped = analysis_data.groupby('id', sort=False)\n",
        "\n",
        "    y_all = []\n",
        "    vcv_all_matrix = []\n",
        "    vcv_all_diag = []\n",
        "    X_all = []\n",
        "    study_ids = []\n",
        "\n",
        "    for study_id, group in grouped:\n",
        "        k = len(group)\n",
        "        study_ids.append(study_id)\n",
        "\n",
        "        y_i = group[effect_col].values.astype(np.float64)\n",
        "        y_all.append(y_i)\n",
        "\n",
        "        vi = group[var_col].values.astype(np.float64)\n",
        "        vcv_all_diag.append(np.diag(vi))\n",
        "\n",
        "        sid_str = str(study_id)\n",
        "        if sid_str in vcv_dict:\n",
        "            V_i = np.asarray(vcv_dict[sid_str], dtype=np.float64)\n",
        "            if V_i.shape[0] != k: V_i = np.diag(vi)\n",
        "        elif study_id in vcv_dict:\n",
        "            V_i = np.asarray(vcv_dict[study_id], dtype=np.float64)\n",
        "            if V_i.shape[0] != k: V_i = np.diag(vi)\n",
        "        else:\n",
        "            V_i = np.diag(vi)\n",
        "        vcv_all_matrix.append(V_i)\n",
        "\n",
        "        mod_values = group[moderator_col].values.astype(np.float64)\n",
        "        X_i = np.column_stack([np.ones(k), mod_values])\n",
        "        X_all.append(X_i)\n",
        "\n",
        "    N_total = len(analysis_data)\n",
        "    M_studies = len(y_all)\n",
        "    p_params = 2\n",
        "\n",
        "    if M_studies < 2 or N_total < 3:\n",
        "        return None, None, None\n",
        "\n",
        "    is_constant_within = analysis_data.groupby('id')[moderator_col].nunique().max() == 1\n",
        "\n",
        "    mod_range = analysis_data[moderator_col].max() - analysis_data[moderator_col].min()\n",
        "    if mod_range < 1e-10:\n",
        "        return None, None, None\n",
        "\n",
        "    tau_sq_prior, sigma_sq_prior = _estimate_variance_from_intercept_model_vcv(\n",
        "        y_all, vcv_all_matrix, M_studies\n",
        "    )\n",
        "\n",
        "    # =================================================================\n",
        "    # 2. OPTIMIZER DEFINITION\n",
        "    # =================================================================\n",
        "\n",
        "    def run_optimizer(vcv_input, constrained=False):\n",
        "        best_res = None\n",
        "        best_fun = np.inf\n",
        "\n",
        "        if constrained:\n",
        "            starts = [\n",
        "                [tau_sq_prior, sigma_sq_prior],\n",
        "                [tau_sq_prior * 0.5, sigma_sq_prior],\n",
        "                [tau_sq_prior * 2.0, sigma_sq_prior],\n",
        "                [0.1, 0.1],\n",
        "                [0.5, 0.01],\n",
        "            ]\n",
        "            penalty = 5.0\n",
        "\n",
        "            for start in starts:\n",
        "                try:\n",
        "                    with warnings.catch_warnings():\n",
        "                        warnings.simplefilter(\"ignore\")\n",
        "                        res = minimize(\n",
        "                            _neg_log_lik_reml_reg_constrained,\n",
        "                            x0=start,\n",
        "                            args=(y_all, vcv_input, X_all, N_total, M_studies, p_params,\n",
        "                                  tau_sq_prior, penalty),\n",
        "                            method='L-BFGS-B',\n",
        "                            bounds=[(1e-8, 50.0), (1e-8, 50.0)],\n",
        "                            options={'ftol': 1e-11, 'maxiter': 2000}\n",
        "                        )\n",
        "                    if res.fun < best_fun and np.isfinite(res.fun):\n",
        "                        best_fun = res.fun\n",
        "                        best_res = res\n",
        "                except:\n",
        "                    continue\n",
        "        else:\n",
        "            starts = [\n",
        "                [tau_sq_prior, sigma_sq_prior],\n",
        "                [0.01, 0.01], [0.1, 0.1], [1.0, 0.1], [0.1, 1.0], [0.5, 0.5]\n",
        "            ]\n",
        "            for start in starts:\n",
        "                try:\n",
        "                    with warnings.catch_warnings():\n",
        "                        warnings.simplefilter(\"ignore\")\n",
        "                        res = minimize(\n",
        "                            _neg_log_lik_reml_reg,\n",
        "                            x0=start,\n",
        "                            args=(y_all, vcv_input, X_all, N_total, M_studies, p_params),\n",
        "                            method='L-BFGS-B',\n",
        "                            bounds=[(1e-8, 50.0), (1e-8, 50.0)],\n",
        "                            options={'ftol': 1e-11, 'maxiter': 2000}\n",
        "                        )\n",
        "                    if res.fun < best_fun and np.isfinite(res.fun):\n",
        "                        best_fun = res.fun\n",
        "                        best_res = res\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        return best_res\n",
        "\n",
        "    # =================================================================\n",
        "    # 3. EXECUTION: PLAN A -> B -> C\n",
        "    # =================================================================\n",
        "\n",
        "    best_res = None\n",
        "    final_vcv = None\n",
        "    model_type = \"Unknown\"\n",
        "    plan_c_result = None  # NEW: Store Plan C result separately\n",
        "\n",
        "    # --- Plan A: Full Matrix ---\n",
        "    has_off_diag = any(not np.allclose(m, np.diag(np.diag(m))) for m in vcv_all_matrix)\n",
        "\n",
        "    if has_off_diag:\n",
        "        best_res = run_optimizer(vcv_all_matrix, constrained=is_constant_within)\n",
        "        if best_res is not None:\n",
        "            final_vcv = vcv_all_matrix\n",
        "            model_type = \"3-Level VCV\" if not is_constant_within else \"3-Level VCV (Constrained)\"\n",
        "\n",
        "    # --- Plan B: Diagonal ---\n",
        "    if best_res is None:\n",
        "        best_res = run_optimizer(vcv_all_diag, constrained=is_constant_within)\n",
        "        if best_res is not None:\n",
        "            final_vcv = vcv_all_diag\n",
        "            model_type = \"3-Level Diagonal\" if not is_constant_within else \"3-Level Diagonal (Constrained)\"\n",
        "\n",
        "    # --- Plan C: Aggregated 2-Level ---\n",
        "    # CRITICAL: This returns DIRECTLY, not through _get_gls_estimates\n",
        "    if best_res is None and is_constant_within:\n",
        "        plan_c_result = _run_aggregated_2level_regression(\n",
        "            y_all, vcv_all_diag, X_all, M_studies, tau_sq_prior\n",
        "        )\n",
        "\n",
        "        if plan_c_result is not None:\n",
        "            # Add model type and return immediately\n",
        "            plan_c_result['model_type'] = \"2-Level Aggregated (Fallback)\"\n",
        "            plan_c_result['is_constant_within'] = True\n",
        "            plan_c_result['n_studies'] = M_studies\n",
        "            plan_c_result['n_obs'] = N_total\n",
        "\n",
        "            info = (N_total, M_studies, p_params)\n",
        "            fake_opt_result = type('obj', (object,), {\n",
        "                'x': np.array([plan_c_result['tau_sq'], plan_c_result['sigma_sq']]),\n",
        "                'success': True,\n",
        "                'fun': 0.0\n",
        "            })()\n",
        "\n",
        "            return plan_c_result, info, fake_opt_result\n",
        "\n",
        "    # All plans failed\n",
        "    if best_res is None:\n",
        "        return None, None, None\n",
        "\n",
        "    # =================================================================\n",
        "    # 4. FINAL CALCULATION (Plans A & B only)\n",
        "    # =================================================================\n",
        "\n",
        "    try:\n",
        "        # Polish with Nelder-Mead\n",
        "        try:\n",
        "            final_res = minimize(\n",
        "                _neg_log_lik_reml_reg,\n",
        "                x0=best_res.x,\n",
        "                args=(y_all, final_vcv, X_all, N_total, M_studies, p_params),\n",
        "                method='Nelder-Mead',\n",
        "                options={'xatol': 1e-10, 'fatol': 1e-10, 'maxiter': 1000}\n",
        "            )\n",
        "            if np.isfinite(final_res.fun) and final_res.fun < best_res.fun:\n",
        "                best_res = final_res\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Calculate final estimates\n",
        "        final_est = _get_gls_estimates(\n",
        "            best_res.x, y_all, final_vcv, X_all, N_total, M_studies, p_params\n",
        "        )\n",
        "\n",
        "        if final_est.get('log_lik_reml', -np.inf) == -np.inf:\n",
        "            return None, None, None\n",
        "\n",
        "        betas = final_est['betas']\n",
        "        se_betas = final_est['se_betas']\n",
        "\n",
        "        # Robust SEs\n",
        "        try:\n",
        "            var_betas_robust = _compute_robust_var_betas(\n",
        "                betas, y_all, final_vcv, X_all, final_est['tau_sq'], final_est['sigma_sq']\n",
        "            )\n",
        "            se_betas_robust = np.sqrt(np.diag(var_betas_robust))\n",
        "            # Use robust SEs if they're valid\n",
        "            if np.all(np.isfinite(se_betas_robust)) and np.all(se_betas_robust > 0):\n",
        "                se_betas = se_betas_robust\n",
        "            else:\n",
        "                var_betas_robust = final_est['cov_beta']\n",
        "        except:\n",
        "            var_betas_robust = final_est['cov_beta']\n",
        "\n",
        "        # Degrees of freedom\n",
        "        if is_constant_within:\n",
        "            df = max(1, M_studies - p_params)\n",
        "        else:\n",
        "            df = max(1, N_total - M_studies - p_params + 1)\n",
        "\n",
        "        # Inference\n",
        "        t_stats = betas / se_betas\n",
        "        p_values = 2 * (1 - t.cdf(np.abs(t_stats), df))\n",
        "\n",
        "        # Confidence intervals\n",
        "        alpha = ANALYSIS_CONFIG.get('global_settings', {}).get('alpha', 0.05)\n",
        "        crit_val = t.ppf(1 - alpha/2, df)\n",
        "        ci_lower = betas - crit_val * se_betas\n",
        "        ci_upper = betas + crit_val * se_betas\n",
        "\n",
        "        final_est.update({\n",
        "            'df': df,\n",
        "            't_stats': t_stats,\n",
        "            'p_values': p_values,\n",
        "            'ci_lower': ci_lower,\n",
        "            'ci_upper': ci_upper,\n",
        "            'var_betas_robust': var_betas_robust,\n",
        "            'se_betas': se_betas,\n",
        "            'model_type': model_type,\n",
        "            'is_constant_within': is_constant_within,\n",
        "            'n_studies': M_studies,\n",
        "            'n_obs': N_total,\n",
        "            'optimizer_converged': getattr(best_res, 'success', False)\n",
        "        })\n",
        "\n",
        "        return final_est, (N_total, M_studies, p_params), best_res\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# NEW HELPER: Plan C - Aggregated 2-Level Regression\n",
        "# =============================================================================\n",
        "\n",
        "def _run_aggregated_2level_regression(y_all, vcv_all, X_all, M_studies, tau_sq_prior):\n",
        "    \"\"\"\n",
        "    Runs a 2-level (study-level) meta-regression when 3-level fails.\n",
        "\n",
        "    This aggregates multiple effects per study into a single weighted mean,\n",
        "    then performs standard random-effects meta-regression.\n",
        "\n",
        "    Returns a complete estimates dictionary matching _get_gls_estimates output.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # =============================================================\n",
        "        # 1. AGGREGATE TO STUDY LEVEL\n",
        "        # =============================================================\n",
        "\n",
        "        agg_y = []      # Aggregated effect sizes\n",
        "        agg_v = []      # Aggregated variances\n",
        "        agg_mod = []    # Moderator values (constant within study)\n",
        "\n",
        "        for i in range(M_studies):\n",
        "            y_i = y_all[i]\n",
        "            V_i = vcv_all[i]\n",
        "            X_i = X_all[i]\n",
        "\n",
        "            # Get variances (diagonal of VCV)\n",
        "            if V_i.ndim == 1:\n",
        "                v_i = V_i\n",
        "            else:\n",
        "                v_i = np.diag(V_i)\n",
        "\n",
        "            # Weights = inverse variance\n",
        "            w_i = 1.0 / v_i\n",
        "            sum_w = np.sum(w_i)\n",
        "\n",
        "            # Weighted mean effect size\n",
        "            mu_agg = np.sum(y_i * w_i) / sum_w\n",
        "\n",
        "            # Variance of the weighted mean\n",
        "            # For independent effects: Var(weighted mean) = 1 / sum(weights)\n",
        "            # For correlated effects, this is an approximation\n",
        "            v_agg = 1.0 / sum_w\n",
        "\n",
        "            # Moderator (take first value - it's constant within study)\n",
        "            mod_val = X_i[0, 1]\n",
        "\n",
        "            agg_y.append(mu_agg)\n",
        "            agg_v.append(v_agg)\n",
        "            agg_mod.append(mod_val)\n",
        "\n",
        "        agg_y = np.array(agg_y)\n",
        "        agg_v = np.array(agg_v)\n",
        "        agg_mod = np.array(agg_mod)\n",
        "\n",
        "        # =============================================================\n",
        "        # 2. OPTIMIZE TAU² (Between-Study Variance)\n",
        "        # =============================================================\n",
        "\n",
        "        def neg_reml_2level(tau_sq):\n",
        "            \"\"\"Negative REML log-likelihood for 2-level model.\"\"\"\n",
        "            tau_sq = max(tau_sq, 1e-10)\n",
        "\n",
        "            # Total variance = sampling variance + tau²\n",
        "            total_v = agg_v + tau_sq\n",
        "            weights = 1.0 / total_v\n",
        "\n",
        "            # Weighted least squares\n",
        "            X = np.column_stack([np.ones(M_studies), agg_mod])\n",
        "            W = np.diag(weights)\n",
        "\n",
        "            try:\n",
        "                XtWX = X.T @ W @ X\n",
        "                XtWy = X.T @ W @ agg_y\n",
        "\n",
        "                # Check for singularity\n",
        "                if np.linalg.cond(XtWX) > 1e10:\n",
        "                    return 1e10\n",
        "\n",
        "                betas = np.linalg.solve(XtWX, XtWy)\n",
        "\n",
        "                # Residuals\n",
        "                resid = agg_y - X @ betas\n",
        "\n",
        "                # REML log-likelihood components\n",
        "                log_det_V = np.sum(np.log(total_v))\n",
        "                sign, log_det_XtWX = np.linalg.slogdet(XtWX)\n",
        "                rss = resid.T @ W @ resid\n",
        "\n",
        "                # REML: -2 * logLik = log|V| + log|X'V⁻¹X| + RSS\n",
        "                neg_2_loglik = log_det_V + log_det_XtWX + rss\n",
        "\n",
        "                return 0.5 * neg_2_loglik\n",
        "\n",
        "            except:\n",
        "                return 1e10\n",
        "\n",
        "        # Multi-start optimization\n",
        "        starts = [tau_sq_prior, 0.01, 0.1, 0.5, 1.0, 2.0]\n",
        "        best_tau_sq = tau_sq_prior\n",
        "        best_nll = np.inf\n",
        "\n",
        "        for start in starts:\n",
        "            try:\n",
        "                res = minimize_scalar(\n",
        "                    neg_reml_2level,\n",
        "                    bounds=(1e-10, 50.0),\n",
        "                    method='bounded',\n",
        "                    options={'xatol': 1e-10}\n",
        "                )\n",
        "                if res.fun < best_nll:\n",
        "                    best_nll = res.fun\n",
        "                    best_tau_sq = res.x\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        tau_sq = max(best_tau_sq, 1e-10)\n",
        "\n",
        "        # =============================================================\n",
        "        # 3. COMPUTE FINAL ESTIMATES\n",
        "        # =============================================================\n",
        "\n",
        "        total_v = agg_v + tau_sq\n",
        "        weights = 1.0 / total_v\n",
        "\n",
        "        X = np.column_stack([np.ones(M_studies), agg_mod])\n",
        "        W = np.diag(weights)\n",
        "\n",
        "        XtWX = X.T @ W @ X\n",
        "        XtWy = X.T @ W @ agg_y\n",
        "\n",
        "        # Coefficients\n",
        "        cov_beta = np.linalg.inv(XtWX)\n",
        "        betas = cov_beta @ XtWy\n",
        "        se_betas = np.sqrt(np.diag(cov_beta))\n",
        "\n",
        "        # Residuals and RSS\n",
        "        resid = agg_y - X @ betas\n",
        "        rss = resid.T @ W @ resid\n",
        "\n",
        "        # Log-likelihood (for reporting)\n",
        "        log_det_V = np.sum(np.log(total_v))\n",
        "        sign, log_det_XtWX = np.linalg.slogdet(XtWX)\n",
        "        log_lik_reml = -0.5 * (\n",
        "            (M_studies - 2) * np.log(2 * np.pi) +\n",
        "            log_det_V +\n",
        "            log_det_XtWX +\n",
        "            rss\n",
        "        )\n",
        "\n",
        "        # Degrees of freedom\n",
        "        df = max(1, M_studies - 2)\n",
        "\n",
        "        # Inference\n",
        "        t_stats = betas / se_betas\n",
        "        p_values = 2 * (1 - t.cdf(np.abs(t_stats), df))\n",
        "\n",
        "        # Confidence intervals\n",
        "        alpha = ANALYSIS_CONFIG.get('global_settings', {}).get('alpha', 0.05)\n",
        "        crit_val = t.ppf(1 - alpha/2, df)\n",
        "        ci_lower = betas - crit_val * se_betas\n",
        "        ci_upper = betas + crit_val * se_betas\n",
        "\n",
        "        return {\n",
        "            'betas': betas,\n",
        "            'cov_beta': cov_beta,\n",
        "            'se_betas': se_betas,\n",
        "            'var_betas_robust': cov_beta,  # No robust SEs for 2-level\n",
        "            'tau_sq': tau_sq,\n",
        "            'sigma_sq': 0.0,  # No within-study variance in 2-level\n",
        "            'log_lik_reml': log_lik_reml,\n",
        "            'rss': rss,\n",
        "            'df': df,\n",
        "            't_stats': t_stats,\n",
        "            'p_values': p_values,\n",
        "            'ci_lower': ci_lower,\n",
        "            'ci_upper': ci_upper,\n",
        "            'optimizer_converged': True\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "# =============================================================================\n",
        "# COMPONENT 4: Helper Functions\n",
        "# =============================================================================\n",
        "\n",
        "def _estimate_variance_from_intercept_model_vcv(y_all, vcv_all, M_studies):\n",
        "    \"\"\"\n",
        "    Get variance component starting values from intercept-only model.\n",
        "    Uses method-of-moments (similar to DerSimonian-Laird).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Simple weighted mean for starting point\n",
        "        weights = []\n",
        "        effects = []\n",
        "\n",
        "        for i in range(M_studies):\n",
        "            y_i = y_all[i]\n",
        "            V_i = vcv_all[i]\n",
        "\n",
        "            # Use inverse of total variance as weight\n",
        "            if V_i.ndim == 1:\n",
        "                w_i = 1.0 / np.mean(V_i)\n",
        "            else:\n",
        "                w_i = 1.0 / np.mean(np.diag(V_i))\n",
        "\n",
        "            weights.append(w_i * len(y_i))\n",
        "            effects.extend(y_i)\n",
        "\n",
        "        weights = np.array(weights)\n",
        "        effects = np.array(effects)\n",
        "\n",
        "        # Weighted mean\n",
        "        overall_mean = np.average([np.mean(y) for y in y_all], weights=weights)\n",
        "\n",
        "        # Between-study variance (tau²) - variance of study means\n",
        "        study_means = np.array([np.mean(y) for y in y_all])\n",
        "        tau_sq = np.var(study_means, ddof=1)\n",
        "        tau_sq = max(tau_sq, 0.01)\n",
        "\n",
        "        # Within-study variance (sigma²) - average within-study variance\n",
        "        within_vars = []\n",
        "        for y_i in y_all:\n",
        "            if len(y_i) > 1:\n",
        "                within_vars.append(np.var(y_i, ddof=1))\n",
        "\n",
        "        if within_vars:\n",
        "            sigma_sq = np.mean(within_vars)\n",
        "        else:\n",
        "            sigma_sq = 0.01\n",
        "\n",
        "        sigma_sq = max(sigma_sq, 0.01)\n",
        "\n",
        "        return tau_sq, sigma_sq\n",
        "\n",
        "    except Exception:\n",
        "        return 0.1, 0.1\n",
        "\n",
        "\n",
        "def _compute_robust_var_betas(betas, y_all, vcv_all, X_all, tau_sq, sigma_sq):\n",
        "    \"\"\"\n",
        "    Compute robust (sandwich) variance estimator for beta coefficients.\n",
        "\n",
        "    Var_robust(β) = (X'Σ⁻¹X)⁻¹ (X'Σ⁻¹ e e' Σ⁻¹X) (X'Σ⁻¹X)⁻¹\n",
        "\n",
        "    where e = y - Xβ are the residuals.\n",
        "    \"\"\"\n",
        "    M_studies = len(y_all)\n",
        "    p_params = len(betas)\n",
        "\n",
        "    # Recompute (X'Σ⁻¹X)⁻¹\n",
        "    sum_Xt_invS_X = np.zeros((p_params, p_params))\n",
        "\n",
        "    # Middle term: sum over studies of X_i'Σ_i⁻¹ e_i e_i' Σ_i⁻¹ X_i\n",
        "    meat = np.zeros((p_params, p_params))\n",
        "\n",
        "    for i in range(M_studies):\n",
        "        y_i = np.asarray(y_all[i], dtype=np.float64)\n",
        "        X_i = np.asarray(X_all[i], dtype=np.float64)\n",
        "        V_i = np.asarray(vcv_all[i], dtype=np.float64)\n",
        "        k = len(y_i)\n",
        "\n",
        "        if V_i.ndim == 1:\n",
        "            V_i = np.diag(V_i)\n",
        "\n",
        "        # Build Σ_i\n",
        "        Sigma_i = V_i + sigma_sq * np.eye(k) + tau_sq * np.ones((k, k))\n",
        "\n",
        "        try:\n",
        "            inv_Sigma_i = np.linalg.inv(Sigma_i)\n",
        "        except:\n",
        "            inv_Sigma_i = np.linalg.pinv(Sigma_i)\n",
        "\n",
        "        # Residuals for this study\n",
        "        e_i = y_i - X_i @ betas\n",
        "\n",
        "        # Accumulate bread\n",
        "        invS_X = inv_Sigma_i @ X_i\n",
        "        sum_Xt_invS_X += X_i.T @ invS_X\n",
        "\n",
        "        # Accumulate meat: X_i' Σ⁻¹ e_i e_i' Σ⁻¹ X_i\n",
        "        invS_e = inv_Sigma_i @ e_i\n",
        "        meat += np.outer(X_i.T @ invS_e, X_i.T @ invS_e)\n",
        "\n",
        "    # Sandwich: bread⁻¹ × meat × bread⁻¹\n",
        "    try:\n",
        "        bread_inv = np.linalg.inv(sum_Xt_invS_X)\n",
        "        var_robust = bread_inv @ meat @ bread_inv\n",
        "    except:\n",
        "        var_robust = np.eye(p_params) * 0.01\n",
        "\n",
        "    return var_robust\n"
      ],
      "metadata": {
        "id": "UuNjq-M41Dxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wrDhm1mjeOl",
        "cellView": "form"
      },
      "source": [
        "#@title 📈 Meta-Regression\n",
        "\n",
        "# =============================================================================\n",
        "# CELL: META-REGRESSION WITH DASHBOARD\n",
        "# Purpose: Run meta-regression with organized, publication-ready output.\n",
        "# Enhancement: Tabbed interface for results, diagnostics, details, and publication text.\n",
        "# Note: Use the dedicated plot cell for visualization (allows full customization).\n",
        "# =============================================================================\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from scipy.stats import t, norm, chi2\n",
        "import scipy.stats as stats\n",
        "from scipy.optimize import minimize_scalar, minimize\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import t as t_dist\n",
        "\n",
        "\n",
        "print(\"✅ High-Precision Regression Engine Ready (Robust Mode).\")\n",
        "\n",
        "# --- 1. LAYOUT & WIDGETS ---\n",
        "tab_results = widgets.Output()\n",
        "tab_diagnostics = widgets.Output()\n",
        "tab_details = widgets.Output()\n",
        "tab_publication = widgets.Output()\n",
        "\n",
        "tab_export = widgets.Output()\n",
        "tabs = widgets.Tab(children=[tab_results, tab_diagnostics, tab_details, tab_publication, tab_export])\n",
        "tabs.set_title(0, '📊 Results')\n",
        "tabs.set_title(1, '🔍 Diagnostics')\n",
        "tabs.set_title(2, '⚙️ Model Details')\n",
        "tabs.set_title(3, '📝 Publication Text')\n",
        "tabs.set_title(4, '💾 Export')\n",
        "\n",
        "# --- 2. HELPER FUNCTIONS ---\n",
        "\n",
        "def _run_aggregated_re_regression(agg_df, moderator_col, effect_col, var_col):\n",
        "    \"\"\"\n",
        "    Runs a standard Random-Effects Meta-Regression (2-Level).\n",
        "    Used when the moderator is constant within studies.\n",
        "    \"\"\"\n",
        "    y = agg_df[effect_col].values\n",
        "    v = agg_df[var_col].values\n",
        "    X = sm.add_constant(agg_df[moderator_col].values)\n",
        "\n",
        "    def re_nll(tau2):\n",
        "        if tau2 < 0: tau2 = 0\n",
        "        weights = 1.0 / (v + tau2)\n",
        "        try:\n",
        "            wls = sm.WLS(y, X, weights=weights).fit()\n",
        "            betas = wls.params\n",
        "            resid = y - wls.fittedvalues\n",
        "            ll = -0.5 * (np.sum(np.log(v + tau2)) +\n",
        "                         np.log(np.linalg.det(X.T @ np.diag(weights) @ X)) +\n",
        "                         np.sum((resid**2) * weights))\n",
        "            return -ll\n",
        "        except:\n",
        "            return np.inf\n",
        "\n",
        "    res = minimize_scalar(re_nll, bounds=(0, 100), method='bounded')\n",
        "    tau2_est = res.x\n",
        "\n",
        "    weights_final = 1.0 / (v + tau2_est)\n",
        "    final_model = sm.WLS(y, X, weights=weights_final).fit()\n",
        "\n",
        "    return {\n",
        "        'betas': final_model.params,\n",
        "        'se_betas': final_model.bse,\n",
        "        'p_values': final_model.pvalues,\n",
        "        'tau_sq': tau2_est,\n",
        "        'model_type': 'Aggregated Random-Effects (2-Level)',\n",
        "        'n_obs': len(agg_df),\n",
        "        'resid_df': final_model.df_resid,\n",
        "        'fitted': final_model.fittedvalues,\n",
        "        'resid': final_model.resid_pearson,\n",
        "        'model': final_model\n",
        "    }\n",
        "\n",
        "def get_potential_moderators(df):\n",
        "    valid_mods = []\n",
        "    exclude = ['id', 'w_fixed', 'w_random']\n",
        "    if 'ANALYSIS_CONFIG' in globals():\n",
        "        exclude.extend([\n",
        "            ANALYSIS_CONFIG.get('effect_col'),\n",
        "            ANALYSIS_CONFIG.get('var_col'),\n",
        "            ANALYSIS_CONFIG.get('se_col')\n",
        "        ])\n",
        "\n",
        "    for col in df.columns:\n",
        "        if col in exclude or col is None: continue\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            if df[col].nunique() > 1: valid_mods.append(col)\n",
        "        elif df[col].dtype == 'object':\n",
        "            try:\n",
        "                nums = pd.to_numeric(df[col], errors='coerce')\n",
        "                if nums.notna().sum() >= 3 and nums.nunique() > 1:\n",
        "                    valid_mods.append(col)\n",
        "            except: pass\n",
        "    return sorted(list(set(valid_mods)))\n",
        "\n",
        "def get_analysis_data():\n",
        "    if 'analysis_data' in globals(): return analysis_data\n",
        "    elif 'data_filtered' in globals(): return data_filtered\n",
        "    else: return None\n",
        "\n",
        "def calculate_r_squared(tau2_null, tau2_model):\n",
        "    \"\"\"Calculate pseudo R-squared for variance explained\"\"\"\n",
        "    if tau2_null <= 0:\n",
        "        return 0.0\n",
        "    r2 = max(0, (tau2_null - tau2_model) / tau2_null * 100)\n",
        "    return r2\n",
        "\n",
        "def calculate_influence_diagnostics(model_results, reg_df, moderator_col, effect_col, var_col):\n",
        "    \"\"\"Calculate Cook's distance and other influence metrics\"\"\"\n",
        "    try:\n",
        "        n = len(reg_df)\n",
        "        # For aggregated models, use statsmodels diagnostics\n",
        "        if 'model' in model_results:\n",
        "            from statsmodels.stats.outliers_influence import OLSInfluence\n",
        "            influence = OLSInfluence(model_results['model'])\n",
        "            cooks_d = influence.cooks_distance[0]\n",
        "            return {\n",
        "                'cooks_d': cooks_d,\n",
        "                'has_influential': np.any(cooks_d > 4/n)\n",
        "            }\n",
        "        else:\n",
        "            # For 3-level, calculate manually\n",
        "            return {'cooks_d': np.zeros(n), 'has_influential': False}\n",
        "    except:\n",
        "        return {'cooks_d': np.zeros(len(reg_df)), 'has_influential': False}\n",
        "\n",
        "def generate_publication_text(mod_col, beta0, beta1, se0, se1, p0, p1, ci0, ci1,\n",
        "                               tau2, sigma2, k_studies, n_obs, model_type,\n",
        "                               r2, resid_het, df_resid, ci_level=95):\n",
        "    \"\"\"Generate publication-ready text for meta-regression\"\"\"\n",
        "\n",
        "    sig_text = \"significantly predicted\" if p1 < 0.05 else \"did not significantly predict\"\n",
        "    p_format = f\"< 0.001\" if p1 < 0.001 else f\"= {p1:.3f}\"\n",
        "    direction = \"increased\" if beta1 > 0 else \"decreased\"\n",
        "\n",
        "    # Model type description\n",
        "    if \"2-Level\" in model_type or \"Aggregated\" in model_type:\n",
        "        model_desc = \"two-level aggregated random-effects meta-regression\"\n",
        "        variance_note = f\"Between-study variance (τ²) was estimated at {tau2:.4f}.\"\n",
        "        cluster_note = \"\"\n",
        "    else:\n",
        "        model_desc = \"three-level random-effects meta-regression with cluster-robust variance estimation\"\n",
        "        variance_note = f\"Between-study variance (τ²) was {tau2:.4f} and within-study variance (σ²) was {sigma2:.4f}.\"\n",
        "        cluster_note = \" Cluster-robust standard errors were computed to account for the nested structure of effect sizes within studies.\"\n",
        "\n",
        "    # Residual heterogeneity interpretation\n",
        "    if resid_het < 25:\n",
        "        het_text = \"low\"\n",
        "        het_interp = \"suggesting that the moderator successfully captured most of the systematic variation in effect sizes\"\n",
        "    elif resid_het < 50:\n",
        "        het_text = \"moderate\"\n",
        "        het_interp = \"suggesting that additional unmeasured factors may contribute to the variation in effect sizes\"\n",
        "    else:\n",
        "        het_text = \"substantial\"\n",
        "        het_interp = \"indicating that additional moderators not examined in this analysis likely contribute to variation in effect sizes\"\n",
        "\n",
        "    text = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff;'>\n",
        "\n",
        "<h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px;'>Meta-Regression Results</h3>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "We conducted a meta-regression to examine whether <b>{mod_col}</b> moderated the effect sizes. A {model_desc} was employed to account for dependencies in the data.{cluster_note} The analysis included <b>k = {k_studies}</b> studies with <b>n = {n_obs}</b> effect sizes.\n",
        "</p>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "The moderator <b>{sig_text}</b> effect sizes (β₁ = <b>{beta1:.3f}</b>, SE = {se1:.3f}, <i>t</i>({df_resid}) = {beta1/se1:.2f}, <i>p</i> {p_format}, {ci_level:.0f}% CI [{ci1[0]:.3f}, {ci1[1]:.3f}]). \"\"\"\n",
        "\n",
        "    if p1 < 0.05:\n",
        "        text += f\"\"\"For every one-unit increase in {mod_col}, the effect size {direction} by <b>{abs(beta1):.3f}</b> units on average.\n",
        "</p>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "This finding suggests that <b>{mod_col}</b> is an important moderator of the outcome. \"\"\"\n",
        "        if r2 > 0:\n",
        "            text += f\"\"\"The moderator explained <b>{r2:.1f}%</b> of the between-study heterogeneity. \"\"\"\n",
        "        text += f\"\"\"[<i>Add domain-specific interpretation: Why might {mod_col} influence the effect? Link to theory or prior research.</i>]\n",
        "</p>\n",
        "\"\"\"\n",
        "    else:\n",
        "        text += f\"\"\"The relationship between {mod_col} and effect sizes was not statistically significant.\n",
        "</p>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "No significant linear relationship was detected between <b>{mod_col}</b> and effect sizes, suggesting that {mod_col} may not be a primary source of heterogeneity in this meta-analysis. \"\"\"\n",
        "        if k_studies < 10:\n",
        "            text += f\"\"\"However, the small number of studies (k = {k_studies}) may have limited statistical power to detect a relationship. \"\"\"\n",
        "        text += f\"\"\"[<i>Discuss alternative explanations: Could the relationship be non-linear? Are there confounding factors? Should subgroup analysis be considered?</i>]\n",
        "</p>\n",
        "\"\"\"\n",
        "\n",
        "    text += f\"\"\"\n",
        "<p style='text-align: justify;'>\n",
        "{variance_note} Residual heterogeneity remained <b>{het_text}</b> (τ²<sub>residual</sub> = {tau2:.4f}\"\"\"\n",
        "\n",
        "    if resid_het > 0:\n",
        "        text += f\"\"\", <i>I</i>²<sub>residual</sub> = {resid_het:.1f}%\"\"\"\n",
        "\n",
        "    text += f\"\"\"), {het_interp}.\n",
        "</p>\n",
        "\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Statistical Methods</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "All meta-regression analyses were conducted using {model_desc}. Restricted maximum likelihood (REML) was used to estimate variance components. The intercept (β₀ = {beta0:.3f}, {ci_level:.0f}% CI [{ci0[0]:.3f}, {ci0[1]:.3f}]) represents the expected effect size when {mod_col} equals zero. Confidence intervals and <i>p</i>-values were based on a <i>t</i>-distribution with {df_resid} degrees of freedom.\n",
        "</p>\n",
        "\n",
        "<hr style='margin: 30px 0; border: none; border-top: 1px solid #bdc3c7;'>\n",
        "\n",
        "<div style='background-color: #ecf0f1; padding: 20px; border-left: 4px solid #3498db; margin-top: 25px;'>\n",
        "<h4 style='margin-top: 0; color: #2c3e50;'>📊 Table 1. Meta-Regression Coefficients</h4>\n",
        "<table style='width: 100%; border-collapse: collapse; margin-top: 15px; background-color: white;'>\n",
        "<thead style='background-color: #34495e; color: white;'>\n",
        "<tr>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: left;'>Predictor</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>β</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>SE</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'><i>t</i></th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>df</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'><i>p</i>-value</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>{ci_level:.0f}% CI</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr style='background-color: #f8f9fa;'>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>Intercept</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{beta0:.3f}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{se0:.3f}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{beta0/se0:.2f}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{df_resid}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{\"<0.001\" if p0 < 0.001 else f\"{p0:.3f}\"}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>[{ci0[0]:.3f}, {ci0[1]:.3f}]</td>\n",
        "</tr>\n",
        "<tr style='background-color: white;'>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'><b>{mod_col}</b></td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center; {\"font-weight: bold;\" if p1 < 0.05 else \"\"}'>{beta1:.3f}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{se1:.3f}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{beta1/se1:.2f}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{df_resid}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center; {\"font-weight: bold;\" if p1 < 0.05 else \"\"}'>{\"<0.001\" if p1 < 0.001 else f\"{p1:.3f}\"}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>[{ci1[0]:.3f}, {ci1[1]:.3f}]</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "<p style='margin-top: 10px; font-size: 0.9em; color: #6c757d;'><i>Note:</i> Results from {model_desc}. k = number of studies; n = number of effect sizes; τ² = between-study variance\"\"\"\n",
        "\n",
        "    if sigma2 > 0:\n",
        "        text += f\"\"\"; σ² = within-study variance\"\"\"\n",
        "\n",
        "    text += f\"\"\".</p>\n",
        "</div>\n",
        "\n",
        "<hr style='margin: 30px 0; border: none; border-top: 1px solid #bdc3c7;'>\n",
        "\n",
        "<div style='background-color: #ecf0f1; padding: 15px; border-left: 4px solid #3498db; margin-top: 20px;'>\n",
        "<h4 style='margin-top: 0; color: #2c3e50;'>Interpretation Guidance:</h4>\n",
        "<ul style='margin-bottom: 0;'>\n",
        "<li>Customize the interpretation based on your specific research domain and theoretical framework</li>\n",
        "<li>Add context about why {mod_col} might theoretically influence the effect sizes</li>\n",
        "<li>Discuss the practical significance of the slope magnitude (not just statistical significance)</li>\n",
        "<li>Consider whether the relationship might be non-linear (quadratic, threshold effects, etc.)</li>\n",
        "<li>Link findings to prior research or meta-analyses in your field</li>\n",
        "<li>If non-significant, discuss statistical power and whether a larger sample might detect an effect</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "<div style='background-color: #fff3cd; padding: 10px; border-left: 4px solid #ffc107; margin-top: 15px;'>\n",
        "<p style='margin: 0;'><b>💡 Tip:</b> Select all text (Ctrl+A / Cmd+A), copy (Ctrl+C / Cmd+C), and paste into your word processor. Edit the [<i>bracketed notes</i>] to add your domain-specific interpretations. Delete sections not relevant to your journal's requirements.</p>\n",
        "</div>\n",
        "\n",
        "</div>\"\"\"\n",
        "\n",
        "    return text\n",
        "\n",
        "def generate_methods_text_regression(es_config, reg_results):\n",
        "    \"\"\"\n",
        "    Generates a 'Materials and Methods' section for Meta-Regression\n",
        "    with dynamic citations based on the model type (RVE vs Standard).\n",
        "    \"\"\"\n",
        "    import sys\n",
        "\n",
        "    # 1. Extract Settings\n",
        "    mod_col = reg_results.get('moderator_col_name', 'the moderator')\n",
        "    model_type = reg_results.get('model_type', '3-Level')\n",
        "\n",
        "    # 2. Define Citation Database\n",
        "    db = {\n",
        "        'thompson': \"Thompson, S. G., & Higgins, J. P. (2002). How should meta-regression analyses be undertaken and interpreted? <i>Statistics in Medicine</i>, 21(11), 1559-1573.\",\n",
        "        'rve': \"Hedges, L. V., Tipton, E., & Johnson, M. C. (2010). Robust variance estimation in meta-regression with dependent effect size estimates. <i>Research Synthesis Methods</i>, 1(1), 39-65.\",\n",
        "        '3level': \"Van den Noortgate, W., López-López, J. A., Marín-Martínez, F., & Sánchez-Meca, J. (2013). Three-level meta-analysis of dependent effect sizes. <i>Behavior Research Methods</i>, 45(2), 576-594.\",\n",
        "        'knapp': \"Knapp, G., & Hartung, J. (2003). Improved tests for a random effects meta-regression with a single covariate. <i>Statistics in Medicine</i>, 22(17), 2693-2710.\",\n",
        "        'tool': \"<b>[CITATION FOR THIS TOOL]:</b> Author, A. A. (202X). <i>Co-Meta: An Interactive Pipeline for Meta-Analysis</i>. [Repository/DOI].\"\n",
        "    }\n",
        "\n",
        "    # 3. Determine Citations based on Model Type\n",
        "    active_refs = []\n",
        "    model_desc = \"\"\n",
        "\n",
        "    if \"Cluster-Robust\" in model_type or \"3-Level\" in model_type:\n",
        "        model_desc = \"To account for the nested structure of the data (multiple effect sizes per study), we employed a multi-level meta-regression model [1].\"\n",
        "        active_refs.append(db['3level'])\n",
        "\n",
        "        model_desc += \" Standard errors and hypothesis tests were computed using Cluster-Robust Variance Estimation (RVE) to handle dependencies [2].\"\n",
        "        active_refs.append(db['rve'])\n",
        "    else:\n",
        "        model_desc = \"We examined the relationship between effect size and the moderator using a mixed-effects meta-regression model [1].\"\n",
        "        active_refs.append(db['thompson'])\n",
        "\n",
        "        model_desc += \" The Knapp-Hartung adjustment was used for hypothesis testing to improve the accuracy of significance levels [2].\"\n",
        "        active_refs.append(db['knapp'])\n",
        "\n",
        "    # Add Tool Citation\n",
        "    active_refs.append(db['tool'])\n",
        "    ref_tool = len(active_refs)\n",
        "\n",
        "    # 4. Build HTML\n",
        "    html = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff; border: 1px solid #eee; margin-bottom: 20px;'>\n",
        "\n",
        "    <h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-top: 0;'>Materials and Methods</h3>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Meta-Regression.</b> We conducted a meta-regression analysis to investigate whether the variation in effect sizes could be explained by the continuous moderator <b>{mod_col}</b>.\n",
        "    {model_desc}\n",
        "    </p>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Model Specification.</b> The model estimated the intercept (β₀, expected effect when {mod_col} is zero) and the slope (β₁, the change in effect size per unit increase in {mod_col}).\n",
        "    Between-study heterogeneity was quantified using the residual <i>I</i>² statistic, representing the proportion of variance not explained by the moderator.\n",
        "    All analyses were performed using the Co-Meta toolkit [{ref_tool}].\n",
        "    </p>\n",
        "\n",
        "    <h4 style='color: #34495e; margin-top: 20px; font-size: 11pt;'>References</h4>\n",
        "    <ol style='font-size: 10pt; color: #555;'>\n",
        "    \"\"\"\n",
        "\n",
        "    for ref in active_refs:\n",
        "        html += f\"<li>{ref}</li>\"\n",
        "\n",
        "    html += \"</ol></div>\"\n",
        "\n",
        "    return html\n",
        "# --- 3. WIDGET SETUP ---\n",
        "df_reg = get_analysis_data()\n",
        "reg_options = get_potential_moderators(df_reg) if df_reg is not None else ['Data not loaded']\n",
        "if not reg_options: reg_options = ['No numeric moderators found']\n",
        "\n",
        "moderator_widget = widgets.Dropdown(\n",
        "    options=reg_options, description='Moderator:',\n",
        "    style={'description_width': 'initial'}, layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "run_reg_btn = widgets.Button(description=\"▶ Run Meta-Regression\", button_style='success', icon='play')\n",
        "\n",
        "# --- 4. MAIN ANALYSIS FUNCTION (FIXED) ---\n",
        "def run_regression(b):\n",
        "    global ANALYSIS_CONFIG\n",
        "    # Clear all tabs\n",
        "    for tab in [tab_results, tab_diagnostics, tab_details, tab_publication]:\n",
        "        tab.clear_output()\n",
        "\n",
        "    # --- Load Global Settings ---\n",
        "    global_settings = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "    alpha = global_settings.get('alpha', 0.05)\n",
        "    dist_type = global_settings.get('dist_type', 't')\n",
        "    ci_level = (1 - alpha) * 100\n",
        "\n",
        "    mod_col = moderator_widget.value\n",
        "    df_working = get_analysis_data()\n",
        "\n",
        "    if df_working is None:\n",
        "        with tab_results:\n",
        "            display(HTML(\"<div style='color: red;'>❌ Error: Data not found. Run Step 1 first.</div>\"))\n",
        "        return\n",
        "\n",
        "    if mod_col in ['No numeric moderators found', 'Data not loaded']:\n",
        "        with tab_results:\n",
        "            display(HTML(\"<div style='color: red;'>❌ Error: No valid moderator selected.</div>\"))\n",
        "        return\n",
        "\n",
        "    if 'ANALYSIS_CONFIG' in globals():\n",
        "        effect_col = ANALYSIS_CONFIG.get('effect_col', 'hedges_g')\n",
        "        var_col = ANALYSIS_CONFIG.get('var_col', 'Vg')\n",
        "    else:\n",
        "        effect_col = 'hedges_g'\n",
        "        var_col = 'Vg'\n",
        "\n",
        "    # Data Prep\n",
        "    reg_df = df_working.copy()\n",
        "    reg_df[mod_col] = pd.to_numeric(reg_df[mod_col], errors='coerce')\n",
        "    reg_df = reg_df.dropna(subset=[mod_col, effect_col, var_col]).copy()\n",
        "    reg_df = reg_df[reg_df[var_col] > 0]\n",
        "\n",
        "    if len(reg_df) < 3:\n",
        "        with tab_results:\n",
        "            display(HTML(f\"<div style='color: red;'>❌ Error: Insufficient data (n={len(reg_df)}). Need at least 3 observations.</div>\"))\n",
        "        return\n",
        "\n",
        "    # Check for constant moderator\n",
        "    studies_with_variation = reg_df.groupby('id')[mod_col].nunique()\n",
        "    varying_studies = (studies_with_variation > 1).sum()\n",
        "    k_studies = reg_df['id'].nunique()\n",
        "    n_obs = len(reg_df)\n",
        "\n",
        "    # Determine if we have a true multilevel structure\n",
        "    has_multiple_effects_per_study = (reg_df.groupby('id').size() > 1).any()\n",
        "\n",
        "    # =================================================================\n",
        "    # DECISION LOGIC\n",
        "    # =================================================================\n",
        "\n",
        "    if not has_multiple_effects_per_study:\n",
        "        # Only 1 effect per study → true 2-level structure\n",
        "        reg_df['wi'] = 1 / reg_df[var_col]\n",
        "\n",
        "        def agg_func(x):\n",
        "            return pd.Series({\n",
        "                effect_col: np.average(x[effect_col], weights=x['wi']),\n",
        "                var_col: 1 / np.sum(x['wi']),\n",
        "                mod_col: x[mod_col].iloc[0]\n",
        "            })\n",
        "\n",
        "        try:\n",
        "            agg_df = reg_df.groupby('id').apply(agg_func, include_groups=False).reset_index()\n",
        "        except TypeError:\n",
        "            agg_df = reg_df.groupby('id').apply(agg_func).reset_index()\n",
        "\n",
        "        # Run 2-level regression\n",
        "        res = _run_aggregated_re_regression(agg_df, mod_col, effect_col, var_col)\n",
        "\n",
        "        beta0, beta1 = res['betas']\n",
        "        se0, se1 = res['se_betas']\n",
        "        p0, p1 = res['p_values']\n",
        "        tau_sq = res['tau_sq']\n",
        "        sigma_sq = 0.0\n",
        "        df_resid = res['resid_df']\n",
        "        model_type = res['model_type']\n",
        "        fitted = res['fitted']\n",
        "        resid = res['resid']\n",
        "        var_betas_robust = np.array([[se0**2, 0], [0, se1**2]])\n",
        "        reg_df_for_plot = agg_df\n",
        "\n",
        "    else:\n",
        "        # Multiple effects per study → USE 3-LEVEL MODEL\n",
        "\n",
        "        if '_run_three_level_reml_regression_v2' not in globals():\n",
        "            with tab_results:\n",
        "                display(HTML(\"<div style='color: red;'>❌ Error: Run Cell 9.5 (High-Precision Regression Engine) first.</div>\"))\n",
        "            return\n",
        "\n",
        "        est, info, opt_result = _run_three_level_reml_regression_v2(\n",
        "            reg_df, mod_col, effect_col, var_col\n",
        "        )\n",
        "\n",
        "        if est is None:\n",
        "            with tab_results:\n",
        "                display(HTML(\"<div style='color: red;'>❌ Optimization Failed. Try a different moderator or check your data.</div>\"))\n",
        "            return\n",
        "\n",
        "        # Safety check: ensure we got 2 coefficients\n",
        "        if 'betas' not in est or len(est['betas']) != 2:\n",
        "            with tab_results:\n",
        "                n_betas = len(est.get('betas', []))\n",
        "                display(HTML(f\"<div style='color: red;'>❌ Error: Expected 2 coefficients, got {n_betas}.</div>\"))\n",
        "            return\n",
        "\n",
        "        beta0, beta1 = est['betas']\n",
        "        se0, se1 = est['se_betas']\n",
        "        tau_sq = est['tau_sq']\n",
        "        sigma_sq = est['sigma_sq']\n",
        "        var_betas_robust = est.get('var_betas_robust', est.get('cov_beta'))\n",
        "\n",
        "        # Use df and p-values from estimation\n",
        "        if 'df' in est and 'p_values' in est:\n",
        "            df_resid = est['df']\n",
        "            p0, p1 = est['p_values']\n",
        "        else:\n",
        "            df_resid = max(1, k_studies - 2)\n",
        "            t_stat0 = beta0 / se0\n",
        "            t_stat1 = beta1 / se1\n",
        "            p0 = 2 * (1 - t.cdf(abs(t_stat0), df_resid))\n",
        "            p1 = 2 * (1 - t.cdf(abs(t_stat1), df_resid))\n",
        "\n",
        "        # ============================================================\n",
        "        # UPDATED: Get model_type from estimation result\n",
        "        # ============================================================\n",
        "        model_type = est.get('model_type', '3-Level REML')\n",
        "\n",
        "        # Add fallback indicator if Plan C was used\n",
        "        if 'Aggregated' in model_type or 'Fallback' in model_type:\n",
        "            # For aggregated models, we should use aggregated data for plotting\n",
        "            # But for simplicity, we'll still use reg_df and note the model type\n",
        "            pass\n",
        "\n",
        "        reg_df_for_plot = reg_df\n",
        "\n",
        "        # Calculate fitted and residuals\n",
        "        X_mod = reg_df[mod_col].values\n",
        "        fitted = beta0 + beta1 * X_mod\n",
        "        resid = reg_df[effect_col].values - fitted\n",
        "\n",
        "    # --- Dynamic Inference ---\n",
        "    q_val = 1 - (alpha / 2)\n",
        "\n",
        "    if dist_type == 't':\n",
        "        crit_val = t.ppf(q_val, df_resid)\n",
        "    else:\n",
        "        crit_val = norm.ppf(q_val)\n",
        "        p0 = 2 * (1 - norm.cdf(abs(beta0 / se0)))\n",
        "        p1 = 2 * (1 - norm.cdf(abs(beta1 / se1)))\n",
        "\n",
        "    ci0 = [beta0 - crit_val * se0, beta0 + crit_val * se0]\n",
        "    ci1 = [beta1 - crit_val * se1, beta1 + crit_val * se1]\n",
        "\n",
        "    # Calculate R-squared\n",
        "    if 'overall_results' in ANALYSIS_CONFIG:\n",
        "        tau2_null = ANALYSIS_CONFIG['overall_results'].get('tau_squared', tau_sq)\n",
        "    else:\n",
        "        tau2_null = tau_sq\n",
        "    r2 = calculate_r_squared(tau2_null, tau_sq)\n",
        "\n",
        "    # Residual heterogeneity\n",
        "    if tau_sq > 0:\n",
        "        mean_v = np.mean(reg_df_for_plot[var_col])\n",
        "        total_var = tau_sq + sigma_sq + mean_v\n",
        "        resid_i2 = ((tau_sq + sigma_sq) / total_var) * 100 if total_var > 0 else 0\n",
        "    else:\n",
        "        resid_i2 = 0\n",
        "\n",
        "    # Calculate influence diagnostics\n",
        "    influence_metrics = calculate_influence_diagnostics(\n",
        "        {} if has_multiple_effects_per_study else res,\n",
        "        reg_df_for_plot, mod_col, effect_col, var_col\n",
        "    )\n",
        "    # --- TAB 1: RESULTS ---\n",
        "    with tab_results:\n",
        "        sig = \"***\" if p1 < 0.001 else \"**\" if p1 < 0.01 else \"*\" if p1 < 0.05 else \"ns\"\n",
        "        color = \"#28a745\" if p1 < 0.05 else \"#6c757d\"\n",
        "\n",
        "        html = f\"\"\"\n",
        "        <div style='padding: 20px;'>\n",
        "        <h2 style='color: #2c3e50; margin-bottom: 20px;'>Meta-Regression: {mod_col}</h2>\n",
        "\n",
        "        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white; margin-bottom: 20px;'>\n",
        "            <div style='text-align: center;'>\n",
        "                <div style='font-size: 0.9em; margin-bottom: 10px;'>SLOPE COEFFICIENT (β₁)</div>\n",
        "                <h1 style='margin: 0; font-size: 3em;'>{beta1:.4f}</h1>\n",
        "                <p style='margin: 10px 0 0 0; font-size: 1.2em;'>{sig}</p>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div style='display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-bottom: 20px;'>\n",
        "            <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; border-left: 4px solid #007bff;'>\n",
        "                <div style='color: #6c757d; font-size: 0.9em;'>{ci_level:.0f}% Confidence Interval</div>\n",
        "                <div style='font-size: 1.3em; font-weight: bold;'>[{ci1[0]:.4f}, {ci1[1]:.4f}]</div>\n",
        "            </div>\n",
        "            <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; border-left: 4px solid {color};'>\n",
        "                <div style='color: #6c757d; font-size: 0.9em;'>P-value</div>\n",
        "                <div style='font-size: 1.3em; font-weight: bold; color: {color};'>{p1:.4g}</div>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div style='background-color: #e7f3ff; padding: 20px; border-radius: 5px; margin-bottom: 20px;'>\n",
        "            <h4 style='margin-top: 0; color: #2c3e50;'>Interpretation</h4>\n",
        "            <p style='margin: 0; font-size: 1.05em;'>\n",
        "                For every 1-unit increase in <b>{mod_col}</b>, the effect size {'<b>increases</b>' if beta1 > 0 else '<b>decreases</b>'} by <b>{abs(beta1):.4f}</b> units.\n",
        "                {'This relationship is <b style=\"color: #28a745;\">statistically significant</b>.' if p1 < 0.05 else 'This relationship is <b>not statistically significant</b>.'}\n",
        "            </p>\n",
        "        </div>\n",
        "\n",
        "        <h3 style='color: #2c3e50; border-bottom: 2px solid #dee2e6; padding-bottom: 10px;'>Coefficient Table</h3>\n",
        "        <table style='width: 100%; border-collapse: collapse; margin: 20px 0;'>\n",
        "            <thead style='background-color: #2c3e50; color: white;'>\n",
        "                <tr>\n",
        "                    <th style='padding: 12px; text-align: left; border: 1px solid #dee2e6;'>Term</th>\n",
        "                    <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>Estimate (β)</th>\n",
        "                    <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>SE</th>\n",
        "                    <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>t-value</th>\n",
        "                    <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>p-value</th>\n",
        "                    <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>95% CI</th>\n",
        "                </tr>\n",
        "            </thead>\n",
        "            <tbody>\n",
        "                <tr style='background-color: #f8f9fa;'>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'><b>Intercept</b></td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{beta0:.4f}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{se0:.4f}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{beta0/se0:.2f}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{p0:.4g}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>[{ci0[0]:.4f}, {ci0[1]:.4f}]</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'><b>{mod_col}</b></td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6; {\"font-weight: bold; color: #28a745;\" if p1 < 0.05 else \"\"}'>{beta1:.4f}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{se1:.4f}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{beta1/se1:.2f}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6; {\"font-weight: bold; color: #28a745;\" if p1 < 0.05 else \"\"}'>{p1:.4g}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>[{ci1[0]:.4f}, {ci1[1]:.4f}]</td>\n",
        "                </tr>\n",
        "            </tbody>\n",
        "        </table>\n",
        "\n",
        "        <h3 style='color: #2c3e50; border-bottom: 2px solid #dee2e6; padding-bottom: 10px;'>Model Summary</h3>\n",
        "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px;'>\n",
        "            <p style='margin: 5px 0;'><b>Model Type:</b> {model_type}</p>\n",
        "            <p style='margin: 5px 0;'><b>Studies (k):</b> {k_studies}</p>\n",
        "            <p style='margin: 5px 0;'><b>Observations (n):</b> {n_obs}</p>\n",
        "            <p style='margin: 5px 0;'><b>Degrees of Freedom:</b> {df_resid}</p>\n",
        "            <p style='margin: 5px 0;'><b>Between-Study Variance (τ²):</b> {tau_sq:.4f}</p>\n",
        "            \"\"\"\n",
        "\n",
        "        if sigma_sq > 0:\n",
        "            html += f\"<p style='margin: 5px 0;'><b>Within-Study Variance (σ²):</b> {sigma_sq:.4f}</p>\"\n",
        "\n",
        "        if r2 > 0:\n",
        "            html += f\"<p style='margin: 5px 0;'><b>Variance Explained (R²):</b> {r2:.1f}%</p>\"\n",
        "\n",
        "        html += f\"\"\"\n",
        "            <p style='margin: 5px 0;'><b>Residual I²:</b> {resid_i2:.1f}%</p>\n",
        "        </div>\n",
        "\n",
        "        <div style='background-color: #fff3cd; padding: 15px; border-radius: 5px; border-left: 4px solid #ffc107; margin-top: 20px;'>\n",
        "            <p style='margin: 0;'><b>📊 Next Step:</b> Use the dedicated plot cell to visualize this regression relationship with full customization options.</p>\n",
        "        </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        display(HTML(html))\n",
        "\n",
        "    # --- TAB 2: DIAGNOSTICS ---\n",
        "    with tab_diagnostics:\n",
        "        display(HTML(\"<h3 style='color: #2c3e50;'>🔍 Model Diagnostics</h3>\"))\n",
        "\n",
        "        # Residuals summary\n",
        "        resid_std = resid / np.sqrt(np.var(resid))\n",
        "\n",
        "        diag_html = f\"\"\"\n",
        "        <div style='padding: 20px;'>\n",
        "\n",
        "        <h4 style='color: #34495e;'>Residual Analysis</h4>\n",
        "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px;'>\n",
        "            <p style='margin: 5px 0;'><b>Residual Range:</b> [{np.min(resid):.4f}, {np.max(resid):.4f}]</p>\n",
        "            <p style='margin: 5px 0;'><b>Mean Residual:</b> {np.mean(resid):.4f} (should be ≈ 0)</p>\n",
        "            <p style='margin: 5px 0;'><b>SD of Residuals:</b> {np.std(resid):.4f}</p>\n",
        "        </div>\n",
        "\n",
        "        <h4 style='color: #34495e;'>Influence Diagnostics</h4>\n",
        "        \"\"\"\n",
        "\n",
        "        if influence_metrics['has_influential']:\n",
        "            influential_indices = np.where(influence_metrics['cooks_d'] > 4/n_obs)[0]\n",
        "            diag_html += f\"\"\"\n",
        "            <div style='background-color: #fff3cd; padding: 15px; border-radius: 5px; border-left: 4px solid #ffc107; margin-bottom: 20px;'>\n",
        "                <p style='margin: 0;'><b>⚠️ Warning:</b> {len(influential_indices)} potentially influential observation(s) detected (Cook's D > {4/n_obs:.4f}).</p>\n",
        "                <p style='margin: 10px 0 0 0;'>Influential points: {', '.join(map(str, influential_indices))}</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        else:\n",
        "            diag_html += \"\"\"\n",
        "            <div style='background-color: #d4edda; padding: 15px; border-radius: 5px; border-left: 4px solid #28a745; margin-bottom: 20px;'>\n",
        "                <p style='margin: 0;'><b>✓ Good:</b> No highly influential observations detected.</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        diag_html += f\"\"\"\n",
        "        <h4 style='color: #34495e;'>Heterogeneity Assessment</h4>\n",
        "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px;'>\n",
        "            <p style='margin: 5px 0;'><b>Residual Heterogeneity (I²):</b> {resid_i2:.1f}%</p>\n",
        "            \"\"\"\n",
        "\n",
        "        if r2 > 0:\n",
        "            diag_html += f\"<p style='margin: 5px 0;'><b>Heterogeneity Explained (R²):</b> {r2:.1f}%</p>\"\n",
        "\n",
        "        diag_html += f\"\"\"\n",
        "            <p style='margin: 10px 0 0 0;'><i>Lower residual heterogeneity suggests the moderator explains variation well.</i></p>\n",
        "        </div>\n",
        "\n",
        "        <h4 style='color: #34495e;'>Model Assumptions</h4>\n",
        "        <table style='width: 100%; border-collapse: collapse;'>\n",
        "            <thead style='background-color: #f8f9fa;'>\n",
        "                <tr>\n",
        "                    <th style='padding: 10px; text-align: left; border: 1px solid #dee2e6;'>Assumption</th>\n",
        "                    <th style='padding: 10px; text-align: left; border: 1px solid #dee2e6;'>Assessment</th>\n",
        "                    <th style='padding: 10px; text-align: left; border: 1px solid #dee2e6;'>Status</th>\n",
        "                </tr>\n",
        "            </thead>\n",
        "            <tbody>\n",
        "                <tr>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'><b>Linearity</b></td>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'>Check scatter plot in dedicated plot cell</td>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'>⚠️ Visual check needed</td>\n",
        "                </tr>\n",
        "                <tr style='background-color: #f8f9fa;'>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'><b>Independence</b></td>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'>{'Cluster-robust SE used' if sigma_sq > 0 else 'Aggregated to study level'}</td>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'>✓ Accounted for</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'><b>Normality</b></td>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'>Residuals approximately normal (t-distribution)</td>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'>✓ Assumed</td>\n",
        "                </tr>\n",
        "                <tr style='background-color: #f8f9fa;'>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'><b>Homoscedasticity</b></td>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'>Weighted by inverse variance</td>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'>✓ Weighted regression</td>\n",
        "                </tr>\n",
        "            </tbody>\n",
        "        </table>\n",
        "\n",
        "        <div style='background-color: #e7f3ff; padding: 15px; border-radius: 5px; margin-top: 20px;'>\n",
        "            <p style='margin: 0;'><b>💡 Recommendation:</b> Use the dedicated plot cell to create residual plots and visually assess linearity and homoscedasticity assumptions.</p>\n",
        "        </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        display(HTML(diag_html))\n",
        "\n",
        "    # --- TAB 3: MODEL DETAILS ---\n",
        "    with tab_details:\n",
        "        display(HTML(\"<h3 style='color: #2c3e50;'>⚙️ Model Details & Specifications</h3>\"))\n",
        "\n",
        "        details_html = f\"\"\"\n",
        "        <div style='padding: 20px;'>\n",
        "\n",
        "        <h4 style='color: #34495e;'>Model Specification</h4>\n",
        "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px; font-family: monospace;'>\n",
        "            \"\"\"\n",
        "\n",
        "        if sigma_sq > 0:\n",
        "            details_html += f\"\"\"\n",
        "            <p style='margin: 5px 0;'><b>Three-Level Model:</b></p>\n",
        "            <p style='margin: 5px 0; padding-left: 20px;'>y<sub>ij</sub> = β₀ + β₁X<sub>i</sub> + u<sub>i</sub> + e<sub>ij</sub></p>\n",
        "            <p style='margin: 5px 0; padding-left: 20px;'>where:</p>\n",
        "            <p style='margin: 5px 0; padding-left: 40px;'>• y<sub>ij</sub> = effect size j in study i</p>\n",
        "            <p style='margin: 5px 0; padding-left: 40px;'>• X<sub>i</sub> = moderator value for study i</p>\n",
        "            <p style='margin: 5px 0; padding-left: 40px;'>• u<sub>i</sub> ~ N(0, τ²) = between-study random effect</p>\n",
        "            <p style='margin: 5px 0; padding-left: 40px;'>• e<sub>ij</sub> ~ N(0, σ²) = within-study random effect</p>\n",
        "            \"\"\"\n",
        "        else:\n",
        "            details_html += f\"\"\"\n",
        "            <p style='margin: 5px 0;'><b>Two-Level Aggregated Model:</b></p>\n",
        "            <p style='margin: 5px 0; padding-left: 20px;'>y<sub>i</sub> = β₀ + β₁X<sub>i</sub> + u<sub>i</sub></p>\n",
        "            <p style='margin: 5px 0; padding-left: 20px;'>where:</p>\n",
        "            <p style='margin: 5px 0; padding-left: 40px;'>• y<sub>i</sub> = aggregated effect size for study i</p>\n",
        "            <p style='margin: 5px 0; padding-left: 40px;'>• X<sub>i</sub> = moderator value for study i</p>\n",
        "            <p style='margin: 5px 0; padding-left: 40px;'>• u<sub>i</sub> ~ N(0, τ²) = between-study random effect</p>\n",
        "            <p style='margin: 10px 0 0 0;'><i>Note: Data aggregated to study level because moderator was constant within studies.</i></p>\n",
        "            \"\"\"\n",
        "\n",
        "        details_html += f\"\"\"\n",
        "        </div>\n",
        "\n",
        "        <h4 style='color: #34495e;'>Variance-Covariance Matrix</h4>\n",
        "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px;'>\n",
        "            <table style='margin: 10px auto; border-collapse: collapse;'>\n",
        "                <tr>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6; text-align: center;'>{var_betas_robust[0,0]:.6f}</td>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6; text-align: center;'>{var_betas_robust[0,1]:.6f}</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6; text-align: center;'>{var_betas_robust[1,0]:.6f}</td>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6; text-align: center;'>{var_betas_robust[1,1]:.6f}</td>\n",
        "                </tr>\n",
        "            </table>\n",
        "            <p style='margin: 10px 0 0 0; text-align: center; font-size: 0.9em;'><i>Var(β₀) and Var(β₁) on diagonal; Cov(β₀,β₁) on off-diagonal</i></p>\n",
        "        </div>\n",
        "\n",
        "        <h4 style='color: #34495e;'>Variance Components</h4>\n",
        "        <table style='width: 100%; border-collapse: collapse; margin-bottom: 20px;'>\n",
        "            <thead style='background-color: #f8f9fa;'>\n",
        "                <tr>\n",
        "                    <th style='padding: 10px; text-align: left; border: 1px solid #dee2e6;'>Component</th>\n",
        "                    <th style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>Value</th>\n",
        "                    <th style='padding: 10px; text-align: left; border: 1px solid #dee2e6;'>Description</th>\n",
        "                </tr>\n",
        "            </thead>\n",
        "            <tbody>\n",
        "                <tr>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'><b>τ²</b></td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{tau_sq:.4f}</td>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'>Between-study variance (residual)</td>\n",
        "                </tr>\n",
        "                \"\"\"\n",
        "\n",
        "        if sigma_sq > 0:\n",
        "            details_html += f\"\"\"\n",
        "                <tr style='background-color: #f8f9fa;'>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'><b>σ²</b></td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{sigma_sq:.4f}</td>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'>Within-study variance</td>\n",
        "                </tr>\n",
        "            \"\"\"\n",
        "\n",
        "        details_html += f\"\"\"\n",
        "            </tbody>\n",
        "        </table>\n",
        "\n",
        "        <h4 style='color: #34495e;'>Degrees of Freedom</h4>\n",
        "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px;'>\n",
        "            <p style='margin: 5px 0;'><b>df:</b> {df_resid}</p>\n",
        "            <p style='margin: 5px 0;'><b>Calculation:</b> \"\"\"\n",
        "\n",
        "        if sigma_sq > 0:\n",
        "            details_html += f\"Number of studies (k = {k_studies}) - Number of parameters (2) = {df_resid}\"\n",
        "        else:\n",
        "            details_html += f\"Number of observations (n = {n_obs}) - Number of parameters (2) = {df_resid}\"\n",
        "\n",
        "        details_html += f\"\"\"</p>\n",
        "            <p style='margin: 10px 0 0 0;'><i>Used for t-distribution in hypothesis testing and confidence intervals</i></p>\n",
        "        </div>\n",
        "\n",
        "        <h4 style='color: #34495e;'>Standard Error Details</h4>\n",
        "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px;'>\n",
        "            \"\"\"\n",
        "\n",
        "        if sigma_sq > 0:\n",
        "            details_html += \"\"\"\n",
        "            <p style='margin: 5px 0;'><b>Cluster-Robust Standard Errors</b></p>\n",
        "            <p style='margin: 5px 0;'>Standard errors account for clustering of effect sizes within studies, providing more conservative estimates when multiple effect sizes come from the same study.</p>\n",
        "            \"\"\"\n",
        "        else:\n",
        "            details_html += \"\"\"\n",
        "            <p style='margin: 5px 0;'><b>Standard Random-Effects Standard Errors</b></p>\n",
        "            <p style='margin: 5px 0;'>Standard errors from aggregated two-level model. Data was aggregated to study level because the moderator was constant within each study.</p>\n",
        "            \"\"\"\n",
        "\n",
        "        details_html += f\"\"\"\n",
        "        </div>\n",
        "\n",
        "        <h4 style='color: #34495e;'>Data Summary</h4>\n",
        "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px;'>\n",
        "            <p style='margin: 5px 0;'><b>Original data:</b> {len(df_working)} observations</p>\n",
        "            <p style='margin: 5px 0;'><b>After cleaning:</b> {len(reg_df)} observations</p>\n",
        "            <p style='margin: 5px 0;'><b>Used in analysis:</b> {n_obs} {'studies' if varying_studies == 0 else 'observations'}</p>\n",
        "            <p style='margin: 5px 0;'><b>Moderator range:</b> [{reg_df_for_plot[mod_col].min():.3f}, {reg_df_for_plot[mod_col].max():.3f}]</p>\n",
        "            <p style='margin: 5px 0;'><b>Effect size range:</b> [{reg_df_for_plot[effect_col].min():.3f}, {reg_df_for_plot[effect_col].max():.3f}]</p>\n",
        "        </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        display(HTML(details_html))\n",
        "\n",
        "# --- SAVE RESULTS (MOVED BEFORE PUBLICATION TEXT) ---\n",
        "    if 'ANALYSIS_CONFIG' not in globals():\n",
        "        ANALYSIS_CONFIG = {}\n",
        "\n",
        "    ANALYSIS_CONFIG['meta_regression_RVE_results'] = {\n",
        "        'reg_df': reg_df_for_plot,\n",
        "        'moderator_col_name': mod_col,\n",
        "        'effect_col': effect_col,\n",
        "        'betas': [beta0, beta1],\n",
        "        'var_betas_robust': var_betas_robust,\n",
        "        'std_errors_robust': [se0, se1],\n",
        "        'p_slope': p1,\n",
        "        'p_intercept': p0,  # NEW: also save intercept p-value\n",
        "        'R_squared_adj': r2,\n",
        "        'df_robust': df_resid,\n",
        "        'fitted': fitted,\n",
        "        'resid': resid,\n",
        "        # NEW: Save variance components for validation\n",
        "        'tau_sq': tau_sq,\n",
        "        'sigma_sq': sigma_sq,\n",
        "        'model_type': model_type,\n",
        "        'k_studies': k_studies,\n",
        "        'n_obs': n_obs,\n",
        "    }\n",
        "    ANALYSIS_CONFIG['var_col'] = var_col\n",
        "\n",
        "    # --- TAB 4: PUBLICATION TEXT ---\n",
        "    with tab_publication:\n",
        "        display(HTML(\"<h3 style='color: #2c3e50;'>📝 Publication-Ready Results Text</h3>\"))\n",
        "        display(HTML(\"<p style='color: #6c757d;'>Copy and paste this formatted text into your manuscript:</p>\"))\n",
        "\n",
        "        # A. Generate METHODS Text (New)\n",
        "        methods_html = generate_methods_text_regression(\n",
        "            ANALYSIS_CONFIG.get('es_config', {}),\n",
        "            ANALYSIS_CONFIG['meta_regression_RVE_results']\n",
        "        )\n",
        "\n",
        "        # B. Generate RESULTS Text (Existing)\n",
        "        results_html = generate_publication_text(\n",
        "            mod_col, beta0, beta1, se0, se1, p0, p1, ci0, ci1,\n",
        "            tau_sq, sigma_sq, k_studies, n_obs, model_type,\n",
        "            r2, resid_i2, df_resid\n",
        "        )\n",
        "\n",
        "        # C. Display Both\n",
        "        display(HTML(methods_html))\n",
        "        display(HTML(results_html))\n",
        "\n",
        "        # D. Save Combined Text for Audit Report\n",
        "        ANALYSIS_CONFIG['regression_text'] = methods_html + \"<br><hr><br>\" + results_html\n",
        "\n",
        "\n",
        "run_reg_btn.on_click(run_regression)\n",
        "\n",
        "# --- 5. DISPLAY UI ---\n",
        "display(HTML(\"<h3>📊 Meta-Regression Analysis (V2)</h3>\"))\n",
        "display(HTML(\"<p style='color: #6c757d;'>Select a moderator variable and run the analysis. Results will appear in organized tabs below.</p>\"))\n",
        "display(widgets.VBox([moderator_widget, run_reg_btn]))\n",
        "display(tabs)\n",
        "\n",
        "with tab_export:\n",
        "    display(HTML(\"<h3 style='color: #2E86AB;'>💾 Download Audit Report</h3>\"))\n",
        "    display(HTML(\"<p>Generate a full Excel audit trail including regression coefficients, diagnostics, and model settings.</p>\"))\n",
        "\n",
        "    btn_reg_export = widgets.Button(\n",
        "        description=\"📥 Download Regression Report\",\n",
        "        button_style='info', icon='file-excel', layout=widgets.Layout(width='300px', height='40px')\n",
        "    )\n",
        "\n",
        "    def on_reg_export_click(b):\n",
        "        export_analysis_report(report_type='regression', filename_prefix='Meta_Regression_Audit')\n",
        "\n",
        "    btn_reg_export.on_click(on_reg_export_click)\n",
        "    display(btn_reg_export)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 📈 META-REGRESSION PLOT\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 11 (REPLACEMENT): META-REGRESSION PLOT\n",
        "# Purpose: Visualize the meta-regression results from Cell 10\n",
        "# Method: Creates a bubble plot with cluster-robust confidence bands\n",
        "# Dependencies: Cell 10 (meta_regression_RVE_results)\n",
        "# Outputs: Publication-ready plot (PDF/PNG)\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import t\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import sys\n",
        "import traceback\n",
        "import warnings\n",
        "\n",
        "# --- 1. WIDGET DEFINITIONS ---\n",
        "# Initialize lists\n",
        "available_color_moderators = ['None']\n",
        "analysis_data_init = None\n",
        "default_x_label = \"Moderator\"\n",
        "default_y_label = \"Effect Size\"\n",
        "default_title = \"Meta-Regression Plot\"\n",
        "label_widgets_dict = {} # Dictionary to store label widgets\n",
        "\n",
        "try:\n",
        "    if 'ANALYSIS_CONFIG' not in globals():\n",
        "        raise NameError(\"ANALYSIS_CONFIG not found\")\n",
        "\n",
        "    if 'analysis_data' in globals():\n",
        "        analysis_data_init = analysis_data.copy()\n",
        "    elif 'data_filtered' in globals():\n",
        "        analysis_data_init = data_filtered.copy()\n",
        "    else:\n",
        "        raise ValueError(\"No data found\")\n",
        "\n",
        "    if 'meta_regression_RVE_results' in ANALYSIS_CONFIG:\n",
        "        reg_results = ANALYSIS_CONFIG['meta_regression_RVE_results']\n",
        "        es_config = ANALYSIS_CONFIG['es_config']\n",
        "        default_x_label = reg_results['moderator_col_name']\n",
        "        default_y_label = es_config['effect_label']\n",
        "        default_title = f\"Meta-Regression: {default_y_label} vs. {default_x_label}\"\n",
        "\n",
        "    # Find categorical moderators for color AND labels\n",
        "    excluded_cols = [\n",
        "        ANALYSIS_CONFIG.get('effect_col'), ANALYSIS_CONFIG.get('var_col'),\n",
        "        ANALYSIS_CONFIG.get('se_col'), 'w_fixed', 'w_random', 'id',\n",
        "        'xe', 'sde', 'ne', 'xc', 'sdc', 'nc',\n",
        "        ANALYSIS_CONFIG.get('ci_lower_col'), ANALYSIS_CONFIG.get('ci_upper_col')\n",
        "    ]\n",
        "    excluded_cols = [col for col in excluded_cols if col is not None]\n",
        "\n",
        "    categorical_cols = analysis_data_init.select_dtypes(include=['object', 'category']).columns\n",
        "    available_color_moderators.extend([\n",
        "        col for col in categorical_cols\n",
        "        if col not in excluded_cols and analysis_data_init[col].nunique() <= 10\n",
        "    ])\n",
        "\n",
        "    # *** NEW: Find all unique labels for the Label Editor ***\n",
        "    all_categorical_labels = set()\n",
        "    for col in available_color_moderators:\n",
        "        if col != 'None' and col in analysis_data_init.columns:\n",
        "            # Add the column name itself (e.g., \"Crop\")\n",
        "            all_categorical_labels.add(col)\n",
        "            # Add all unique values in that column (e.g., \"B\", \"C\", \"R\", \"W\")\n",
        "            all_categorical_labels.update(analysis_data_init[col].astype(str).str.strip().unique())\n",
        "\n",
        "    # Remove any empty strings\n",
        "    all_categorical_labels.discard('')\n",
        "    all_categorical_labels.discard('nan')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠️  Initialization Error: {e}. Please run previous cells.\")\n",
        "\n",
        "\n",
        "# --- Widget Interface ---\n",
        "header = widgets.HTML(\n",
        "    \"<h3 style='color: #2E86AB;'>Meta-Regression Plot Setup</h3>\"\n",
        "    \"<p style='color: #666;'><i>Visualize the relationship between moderator and effect size</i></p>\"\n",
        ")\n",
        "\n",
        "# ========== TAB 1: PLOT STYLE ==========\n",
        "show_title_widget = widgets.Checkbox(value=True, description='Show Plot Title', indent=False)\n",
        "title_widget = widgets.Text(value=default_title, description='Plot Title:',\n",
        "                            layout=widgets.Layout(width='450px'), style={'description_width': '120px'})\n",
        "xlabel_widget = widgets.Text(value=default_x_label, description='X-Axis Label:',\n",
        "                             layout=widgets.Layout(width='450px'), style={'description_width': '120px'})\n",
        "ylabel_widget = widgets.Text(value=default_y_label, description='Y-Axis Label:',\n",
        "                             layout=widgets.Layout(width='450px'), style={'description_width': '120px'})\n",
        "width_widget = widgets.FloatSlider(value=8.0, min=5.0, max=14.0, step=0.5, description='Plot Width (in):',\n",
        "                                   continuous_update=False, style={'description_width': '120px'},\n",
        "                                   layout=widgets.Layout(width='450px'))\n",
        "height_widget = widgets.FloatSlider(value=6.0, min=4.0, max=12.0, step=0.5, description='Plot Height (in):',\n",
        "                                    continuous_update=False, style={'description_width': '120px'},\n",
        "                                    layout=widgets.Layout(width='450px'))\n",
        "\n",
        "style_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Labels & Size</h4>\"),\n",
        "    show_title_widget, title_widget, xlabel_widget, ylabel_widget, width_widget, height_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 2: DATA POINTS ==========\n",
        "color_mod_widget = widgets.Dropdown(options=available_color_moderators, value='None', description='Color By:',\n",
        "                                    style={'description_width': '120px'}, layout=widgets.Layout(width='450px'))\n",
        "point_color_widget = widgets.Dropdown(options=['gray', 'blue', 'red', 'green', 'purple', 'orange'], value='gray',\n",
        "                                      description='Point Color:', style={'description_width': '120px'},\n",
        "                                      layout=widgets.Layout(width='450px'))\n",
        "bubble_base_widget = widgets.IntSlider(value=20, min=0, max=200, step=10, description='Min Bubble Size:',\n",
        "                                       continuous_update=False, style={'description_width': '120px'},\n",
        "                                       layout=widgets.Layout(width='450px'))\n",
        "bubble_range_widget = widgets.IntSlider(value=800, min=100, max=2000, step=100, description='Max Bubble Size:',\n",
        "                                        continuous_update=False, style={'description_width': '120px'},\n",
        "                                        layout=widgets.Layout(width='450px'))\n",
        "bubble_alpha_widget = widgets.FloatSlider(value=0.6, min=0.1, max=1.0, step=0.1, description='Transparency:',\n",
        "                                          continuous_update=False, style={'description_width': '120px'},\n",
        "                                          layout=widgets.Layout(width='450px'))\n",
        "\n",
        "points_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Data Points</h4>\"),\n",
        "    color_mod_widget, point_color_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Bubble Size (by precision):</b>\"),\n",
        "    bubble_base_widget, bubble_range_widget, bubble_alpha_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 3: REGRESSION LINE ==========\n",
        "show_ci_widget = widgets.Checkbox(value=True, description='Show 95% Confidence Band', indent=False)\n",
        "line_color_widget = widgets.Dropdown(options=['red', 'blue', 'black', 'green', 'purple'], value='red',\n",
        "                                     description='Line Color:', style={'description_width': '120px'},\n",
        "                                     layout=widgets.Layout(width='450px'))\n",
        "line_width_widget = widgets.FloatSlider(value=2.0, min=0.5, max=5.0, step=0.5, description='Line Width:',\n",
        "                                        continuous_update=False, style={'description_width': '120px'},\n",
        "                                        layout=widgets.Layout(width='450px'))\n",
        "ci_alpha_widget = widgets.FloatSlider(value=0.3, min=0.1, max=0.8, step=0.1, description='CI Transparency:',\n",
        "                                      continuous_update=False, style={'description_width': '120px'},\n",
        "                                      layout=widgets.Layout(width='450px'))\n",
        "show_equation_widget = widgets.Checkbox(value=True, description='Show Regression Equation & P-value', indent=False)\n",
        "show_r2_widget = widgets.Checkbox(value=True, description='Show R² Value', indent=False)\n",
        "\n",
        "regline_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Regression Line</h4>\"),\n",
        "    line_color_widget, line_width_widget, show_ci_widget, ci_alpha_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    show_equation_widget, show_r2_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 4: LAYOUT & EXPORT ==========\n",
        "show_grid_widget = widgets.Checkbox(value=True, description='Show Grid', indent=False)\n",
        "show_null_line_widget = widgets.Checkbox(value=True, description='Show Null Effect Line (y=0)', indent=False)\n",
        "legend_loc_widget = widgets.Dropdown(options=['best', 'upper right', 'upper left', 'lower left', 'lower right'],\n",
        "                                     value='best', description='Legend Position:',\n",
        "                                     style={'description_width': '120px'}, layout=widgets.Layout(width='450px'))\n",
        "legend_fontsize_widget = widgets.IntSlider(value=10, min=6, max=14, step=1, description='Legend Font:',\n",
        "                                           continuous_update=False, style={'description_width': '120px'},\n",
        "                                           layout=widgets.Layout(width='450px'))\n",
        "save_pdf_widget = widgets.Checkbox(value=True, description='Save as PDF', indent=False)\n",
        "save_png_widget = widgets.Checkbox(value=True, description='Save as PNG', indent=False)\n",
        "png_dpi_widget = widgets.IntSlider(value=300, min=150, max=600, step=50, description='PNG DPI:',\n",
        "                                   continuous_update=False, style={'description_width': '120px'},\n",
        "                                   layout=widgets.Layout(width='450px'))\n",
        "filename_prefix_widget = widgets.Text(value='MetaRegression_Plot', description='Filename Prefix:',\n",
        "                                      layout=widgets.Layout(width='450px'), style={'description_width': '120px'})\n",
        "transparent_bg_widget = widgets.Checkbox(value=False, description='Transparent Background', indent=False)\n",
        "\n",
        "layout_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Layout & Legend</h4>\"),\n",
        "    show_grid_widget, show_null_line_widget, legend_loc_widget, legend_fontsize_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Export</h4>\"),\n",
        "    save_pdf_widget, save_png_widget, png_dpi_widget, filename_prefix_widget, transparent_bg_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 5: LABELS (NEW) ==========\n",
        "label_editor_widgets = []\n",
        "for label in sorted(list(all_categorical_labels)):\n",
        "    text_widget = widgets.Text(\n",
        "        value=str(label),\n",
        "        description=f\"{label}:\",\n",
        "        layout=widgets.Layout(width='500px'),\n",
        "        style={'description_width': '200px'}\n",
        "    )\n",
        "    label_editor_widgets.append(text_widget)\n",
        "    label_widgets_dict[str(label)] = text_widget # Store widget by its original name\n",
        "\n",
        "label_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Edit Plot Labels</h4>\"),\n",
        "    widgets.HTML(\"<p style='color: #666;'><i>Rename raw data values (e.g., 'W') to publication-ready labels (e.g., 'Wheat').</i></p>\"),\n",
        "    *label_editor_widgets\n",
        "])\n",
        "\n",
        "\n",
        "# --- Assemble Tabs ---\n",
        "tab = widgets.Tab(children=[style_tab, points_tab, regline_tab, layout_tab, label_tab])\n",
        "tab.set_title(0, '🎨 Style'); tab.set_title(1, '⚫ Points'); tab.set_title(2, '📈 Regression')\n",
        "tab.set_title(3, '💾 Layout/Export'); tab.set_title(4, '✏️ Labels')\n",
        "\n",
        "run_plot_button = widgets.Button(\n",
        "    description='📊 Generate Regression Plot',\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='450px', height='50px'),\n",
        "    style={'font_weight': 'bold'}\n",
        ")\n",
        "plot_output = widgets.Output()\n",
        "\n",
        "# --- 2. PLOTTING FUNCTION ---\n",
        "@run_plot_button.on_click\n",
        "def generate_regression_plot(b):\n",
        "    \"\"\"Generate meta-regression scatter plot with regression line\"\"\"\n",
        "    with plot_output:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"GENERATING CLUSTER-ROBUST META-REGRESSION PLOT\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        try:\n",
        "            # --- Get Global Settings ---\n",
        "            gs = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "            alpha = gs.get('alpha', 0.05)\n",
        "            dist_type = gs.get('dist_type', 't')\n",
        "            ci_pct = (1 - alpha) * 100\n",
        "\n",
        "            # --- 1. Load Data & Config ---\n",
        "            print(\"STEP 1: LOADING RESULTS FROM CELL 10\")\n",
        "            print(\"---------------------------------\")\n",
        "            if 'meta_regression_RVE_results' not in ANALYSIS_CONFIG:\n",
        "                raise ValueError(\"No meta-regression results found. Please re-run Cell 10.\")\n",
        "\n",
        "            reg_results = ANALYSIS_CONFIG['meta_regression_RVE_results']\n",
        "            es_config = ANALYSIS_CONFIG['es_config']\n",
        "\n",
        "            plot_data = reg_results['reg_df'].copy()\n",
        "            moderator_col = reg_results['moderator_col_name']\n",
        "            effect_col = reg_results['effect_col']\n",
        "            var_col = ANALYSIS_CONFIG['var_col']\n",
        "\n",
        "            b0, b1 = reg_results['betas']\n",
        "            var_betas_robust = reg_results['var_betas_robust']\n",
        "            R_sq = reg_results['R_squared_adj']\n",
        "            p_slope = reg_results['p_slope']\n",
        "            df_robust = reg_results['df_robust']\n",
        "\n",
        "            print(f\"  ✓ Loaded results for moderator: {moderator_col}\")\n",
        "            print(f\"  ✓ Found {len(plot_data)} data points to plot.\")\n",
        "\n",
        "            # --- 2. Get Widget Values (*** FIX: ADDED .value TO ALL ***) ---\n",
        "            show_title = show_title_widget.value\n",
        "            graph_title = title_widget.value\n",
        "            x_label = xlabel_widget.value\n",
        "            y_label = ylabel_widget.value\n",
        "            plot_width = width_widget.value\n",
        "            plot_height = height_widget.value\n",
        "\n",
        "            color_mod_name = color_mod_widget.value\n",
        "            point_color = point_color_widget.value\n",
        "            bubble_base = bubble_base_widget.value\n",
        "            bubble_range = bubble_range_widget.value\n",
        "            bubble_alpha = bubble_alpha_widget.value\n",
        "\n",
        "            show_ci = show_ci_widget.value\n",
        "            line_color = line_color_widget.value\n",
        "            line_width = line_width_widget.value\n",
        "            ci_alpha = ci_alpha_widget.value\n",
        "            show_equation = show_equation_widget.value\n",
        "            show_r2 = show_r2_widget.value\n",
        "\n",
        "            show_grid = show_grid_widget.value\n",
        "            show_null_line = show_null_line_widget.value\n",
        "            legend_loc = legend_loc_widget.value\n",
        "            legend_fontsize = legend_fontsize_widget.value\n",
        "\n",
        "            save_pdf = save_pdf_widget.value\n",
        "            save_png = save_png_widget.value\n",
        "            png_dpi = png_dpi_widget.value\n",
        "            filename_prefix = filename_prefix_widget.value\n",
        "            transparent_bg = transparent_bg_widget.value\n",
        "            # *** END FIX ***\n",
        "\n",
        "            print(f\"\\n📊 Configuration:\")\n",
        "            print(f\"  Plot size: {plot_width}\\\\\\\" × {plot_height}\\\\\\\"\")\n",
        "            print(f\"  Color by: {color_mod_name}\")\n",
        "\n",
        "            # --- 2b. Build Label Mapping ---\n",
        "            label_mapping = {orig: w.value for orig, w in label_widgets_dict.items()}\n",
        "\n",
        "            # --- 3. Prepare Data for Plotting ---\n",
        "            print(\"\\nSTEP 2: PREPARING PLOT DATA\")\n",
        "            print(\"---------------------------------\")\n",
        "\n",
        "            if 'weights' not in plot_data.columns:\n",
        "                tau_sq_overall = ANALYSIS_CONFIG['overall_results']['tau_squared']\n",
        "                plot_data['weights'] = 1 / (plot_data[var_col] + tau_sq_overall)\n",
        "\n",
        "            min_w = plot_data['weights'].min()\n",
        "            max_w = plot_data['weights'].max()\n",
        "\n",
        "            if max_w > min_w:\n",
        "                plot_data['BubbleSize'] = bubble_base + (\n",
        "                    ((plot_data['weights'] - min_w) / (max_w - min_w)) * bubble_range\n",
        "                )\n",
        "            else:\n",
        "                plot_data['BubbleSize'] = bubble_base + bubble_range / 2\n",
        "\n",
        "            print(f\"  ✓ Bubble sizes calculated (Range: {plot_data['BubbleSize'].min():.0f} to {plot_data['BubbleSize'].max():.0f})\")\n",
        "\n",
        "            # --- Handle Color Coding (*** FIX: Corrected logic ***) ---\n",
        "            c_values = point_color\n",
        "            cmap = None\n",
        "            norm = None\n",
        "            unique_cats = []\n",
        "\n",
        "            if color_mod_name != 'None':\n",
        "                if color_mod_name in analysis_data_init.columns:\n",
        "                    # Merge color data from the original dataframe based on index\n",
        "                    color_data = analysis_data_init[[color_mod_name]].copy()\n",
        "                    plot_data = plot_data.merge(color_data, left_index=True, right_index=True, how='left',\n",
        "                                                suffixes=('', '_color'))\n",
        "\n",
        "                    # Use the merged column\n",
        "                    color_col_merged = f\"{color_mod_name}\"\n",
        "                    plot_data[color_col_merged] = plot_data[color_col_merged].fillna('N/A').astype(str).str.strip()\n",
        "                    plot_data['color_codes'], unique_cats = pd.factorize(plot_data[color_col_merged])\n",
        "                    c_values = plot_data['color_codes']\n",
        "                    cmap = 'tab10' # A good categorical colormap\n",
        "                    norm = plt.Normalize(vmin=0, vmax=len(unique_cats)-1)\n",
        "                    print(f\"  ✓ Applying color based on '{color_mod_name}' ({len(unique_cats)} categories)\")\n",
        "                else:\n",
        "                    print(f\"  ⚠️  Color moderator '{color_mod_name}' not found, using default.\")\n",
        "                    color_mod_name = 'None'\n",
        "            # *** END COLOR FIX ***\n",
        "\n",
        "            # --- 4. Create Figure ---\n",
        "            print(\"\\nSTEP 3: GENERATING PLOT\")\n",
        "            print(\"---------------------------------\")\n",
        "            fig, ax = plt.subplots(figsize=(plot_width, plot_height))\n",
        "            if transparent_bg:\n",
        "                fig.patch.set_alpha(0)\n",
        "                ax.patch.set_alpha(0)\n",
        "\n",
        "            # --- Plot Data Points ---\n",
        "            ax.scatter(\n",
        "                x=plot_data[moderator_col],\n",
        "                y=plot_data[effect_col],\n",
        "                s=plot_data['BubbleSize'],\n",
        "                c=c_values,\n",
        "                cmap=cmap,\n",
        "                norm=norm,\n",
        "                alpha=bubble_alpha,\n",
        "                edgecolors='black',\n",
        "                linewidths=0.5,\n",
        "                zorder=3\n",
        "            )\n",
        "\n",
        "            # --- Plot Regression Line & Confidence Band ---\n",
        "            x_min = plot_data[moderator_col].min()\n",
        "            x_max = plot_data[moderator_col].max()\n",
        "            x_range_val = x_max - x_min\n",
        "            x_padding = x_range_val * 0.05 if x_range_val > 0 else 1\n",
        "\n",
        "            x_line = np.linspace(x_min - x_padding, x_max + x_padding, 100)\n",
        "            y_line = b0 + b1 * x_line\n",
        "\n",
        "            ax.plot(x_line, y_line, color=line_color, linewidth=line_width, zorder=2, label=\"Regression Line\")\n",
        "\n",
        "            if show_ci:\n",
        "                X_line_pred = sm.add_constant(x_line, prepend=True)\n",
        "                se_line = np.array([\n",
        "                    np.sqrt(np.array([1, x]) @ var_betas_robust @ np.array([1, x]).T)\n",
        "                    for x in x_line\n",
        "                ])\n",
        "                # --- Dynamic Critical Value ---\n",
        "                q_val = 1 - (alpha / 2)\n",
        "                if dist_type == 't':\n",
        "                    crit_val = t.ppf(q_val, df=df_robust)\n",
        "                else:\n",
        "                    crit_val = norm.ppf(q_val)\n",
        "                # ---------------------------------------\n",
        "\n",
        "                y_ci_upper = y_line + crit_val * se_line\n",
        "                y_ci_lower = y_line - crit_val * se_line\n",
        "                ax.fill_between(x_line, y_ci_lower, y_ci_upper,\n",
        "                                color=line_color, alpha=ci_alpha, zorder=1, label=f\"95% CI (Robust, df={df_robust})\")\n",
        "                print(\"  ✓ Plotted regression line and robust confidence band.\")\n",
        "\n",
        "            # --- Customize Axes ---\n",
        "            if show_null_line:\n",
        "                ax.axhline(es_config.get('null_value', 0), color='gray', linestyle='--', linewidth=1.0, zorder=0)\n",
        "\n",
        "            ax.set_xlabel(x_label, fontsize=12, fontweight='bold')\n",
        "            ax.set_ylabel(y_label, fontsize=12, fontweight='bold')\n",
        "            if show_title:\n",
        "                ax.set_title(graph_title, fontsize=14, fontweight='bold', pad=15)\n",
        "            if show_grid:\n",
        "                ax.grid(True, linestyle=':', alpha=0.4, zorder=0)\n",
        "\n",
        "            # --- Add Equation and R² ---\n",
        "            if show_equation or show_r2:\n",
        "                text_lines = []\n",
        "                if show_equation:\n",
        "                    sign = \"+\" if b1 >= 0 else \"\"\n",
        "                    sig_marker = \"***\" if p_slope < 0.001 else \"**\" if p_slope < 0.01 else \"*\" if p_slope < 0.05 else \"ns\"\n",
        "                    eq_text = f\"y = {b0:.3f} {sign} {b1:.3f}x\"\n",
        "                    p_text = f\"p (slope) = {p_slope:.3g} {sig_marker}\"\n",
        "                    text_lines.append(eq_text)\n",
        "                    text_lines.append(p_text)\n",
        "                if show_r2:\n",
        "                    r2_text = f\"R² (adj) ≈ {R_sq:.1f}%\"\n",
        "                    text_lines.append(r2_text)\n",
        "\n",
        "                ax.text(\n",
        "                    0.05, 0.95, \"\\n\".join(text_lines),\n",
        "                    transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
        "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='gray'),\n",
        "                    zorder=10\n",
        "                )\n",
        "\n",
        "            # --- Create Legend ---\n",
        "            handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "            # *** FIX: Use Label Mapping ***\n",
        "            if color_mod_name != 'None':\n",
        "                for i, cat in enumerate(unique_cats):\n",
        "                    display_label = label_mapping.get(cat, cat) # Get new label\n",
        "                    color_val = plt.get_cmap(cmap)(norm(i))\n",
        "                    handles.append(mpatches.Patch(color=color_val, label=display_label, alpha=bubble_alpha, ec='black', lw=0.5))\n",
        "                    labels.append(display_label)\n",
        "\n",
        "            handles.append(plt.scatter([], [], s=bubble_base + bubble_range/2, c='gray' if color_mod_name == 'None' else 'lightgray',\n",
        "                                       alpha=bubble_alpha, ec='black', lw=0.5))\n",
        "            labels.append(\"Weight (1 / (vᵢ + τ²))\")\n",
        "\n",
        "            display_legend_title = label_mapping.get(color_mod_name, color_mod_name)\n",
        "\n",
        "            ax.legend(handles=handles, labels=labels, loc=legend_loc,\n",
        "                      fontsize=legend_fontsize, framealpha=0.9,\n",
        "                      title=display_legend_title if color_mod_name != 'None' else None)\n",
        "            # *** END FIX ***\n",
        "\n",
        "            fig.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # --- 5. Save Files ---\n",
        "            print(f\"\\nSTEP 4: SAVING FILES\")\n",
        "            print(\"---------------------------------\")\n",
        "\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            base_filename = f\"{filename_prefix}_{moderator_col.replace(' ','_')}_{timestamp}\"\n",
        "\n",
        "            saved_files = []\n",
        "            if save_pdf:\n",
        "                pdf_filename = f\"{base_filename}.pdf\"\n",
        "                fig.savefig(pdf_filename, bbox_inches='tight', transparent=transparent_bg)\n",
        "                saved_files.append(pdf_filename)\n",
        "                print(f\"  ✓ {pdf_filename}\")\n",
        "            if save_png:\n",
        "                png_filename = f\"{base_filename}.png\"\n",
        "                fig.savefig(png_filename, dpi=png_dpi, bbox_inches='tight', transparent=transparent_bg)\n",
        "                saved_files.append(png_filename)\n",
        "                print(f\"  ✓ {png_filename} (DPI: {png_dpi})\")\n",
        "\n",
        "            print(f\"\\n\" + \"=\"*70)\n",
        "            print(\"✅ PLOT GENERATION COMPLETE\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ AN ERROR OCCURRED:\\n\")\n",
        "            print(f\"  Type: {type(e).__name__}\")\n",
        "            print(f\"  Message: {e}\")\n",
        "            print(\"\\n  Traceback:\")\n",
        "            traceback.print_exc(file=sys.stdout)\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"ANALYSIS FAILED. See error message above.\")\n",
        "            print(\"Please check your data and configuration.\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "\n",
        "# --- 6. DISPLAY WIDGETS ---\n",
        "try:\n",
        "    if 'ANALYSIS_CONFIG' not in globals() or 'meta_regression_RVE_results' not in ANALYSIS_CONFIG:\n",
        "        print(\"=\"*70)\n",
        "        print(\"⚠️  PREREQUISITE NOT MET\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"Please run Cell 10 (Meta-Regression) successfully before running this cell.\")\n",
        "    else:\n",
        "        print(\"=\"*70)\n",
        "        print(\"✅ ROBUST META-REGRESSION PLOTTER READY\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"  ✓ Results from Cell 10 are loaded.\")\n",
        "        print(\"  ✓ Customize your plot using the tabs below and click 'Generate'.\")\n",
        "\n",
        "        # Hook up widget events\n",
        "        def on_color_mod_change(change):\n",
        "            point_color_widget.layout.display = 'none' if change['new'] != 'None' else 'flex'\n",
        "        color_mod_widget.observe(on_color_mod_change, names='value')\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            header,\n",
        "            widgets.HTML(\"<hr style='margin: 15px 0;'>\"),\n",
        "            widgets.HTML(\"<b>Plot Options:</b>\"),\n",
        "            tab,\n",
        "            widgets.HTML(\"<hr style='margin: 15px 0;'>\"),\n",
        "            run_plot_button,\n",
        "            plot_output\n",
        "        ]))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ An error occurred during initialization: {e}\")\n",
        "    print(\"Please ensure the notebook has been run in order.\")"
      ],
      "metadata": {
        "id": "3QSIWqR7P5an",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _run_robust_spline_analysis(df, moderator_col, effect_col, var_col, df_spline=3):\n",
        "    \"\"\"\n",
        "    Robust 3-Level Spline Analysis.\n",
        "    Uses VCV Matrices and fixed variance components from linear meta-regression.\n",
        "    \"\"\"\n",
        "    # 1. Run Robust Linear Meta-Regression to get stable Tau^2 and Sigma^2\n",
        "    #print(\"   Step 1: Estimating variance components via linear meta-regression...\")\n",
        "    lin_est, lin_info, _ = _run_three_level_reml_regression_v2(\n",
        "        df, moderator_col, effect_col, var_col\n",
        "    )\n",
        "\n",
        "    if lin_est is None:\n",
        "        return None, \"Linear model failed to converge\"\n",
        "\n",
        "    fixed_tau2 = lin_est['tau_sq']\n",
        "    fixed_sigma2 = lin_est['sigma_sq']\n",
        "\n",
        "    # 2. Prepare Spline Design Matrix\n",
        "    # We need to sort df exactly like _run_three_level... does to match matrices\n",
        "    df_sorted = df.sort_values('id').reset_index(drop=True)\n",
        "\n",
        "    # Generate Basis\n",
        "    # Standardize moderator for numerical stability\n",
        "    mod_vals = df_sorted[moderator_col].values\n",
        "    mod_mean = np.mean(mod_vals)\n",
        "    mod_std = np.std(mod_vals)\n",
        "    z_vals = (mod_vals - mod_mean) / mod_std\n",
        "\n",
        "    try:\n",
        "        # Create basis matrix (Natural Cubic Spline)\n",
        "        import patsy\n",
        "        formula = f\"cr(x, df={df_spline}) - 1\" # -1 removes intercept (we add it manually)\n",
        "        basis_matrix = patsy.dmatrix(formula, {\"x\": z_vals}, return_type='matrix')\n",
        "        basis_matrix = np.asarray(basis_matrix)\n",
        "    except Exception as e:\n",
        "        return None, f\"Patsy error: {e}\"\n",
        "\n",
        "    # 3. Prepare Inputs for GLS\n",
        "    # We need to group X, y, and VCV by study, just like the regression driver\n",
        "    grouped = df_sorted.groupby('id', sort=False)\n",
        "    vcv_dict = ANALYSIS_CONFIG.get('vcv_matrices', {})\n",
        "\n",
        "    y_all = []\n",
        "    vcv_all = []\n",
        "    X_all = []\n",
        "\n",
        "    global_idx = 0\n",
        "    for study_id, group in grouped:\n",
        "        k = len(group)\n",
        "\n",
        "        # y\n",
        "        y_all.append(group[effect_col].values)\n",
        "\n",
        "        # VCV (Logic matches regression driver)\n",
        "        sid_str = str(study_id)\n",
        "        if sid_str in vcv_dict:\n",
        "            V_i = np.asarray(vcv_dict[sid_str])\n",
        "            if V_i.shape[0] != k: V_i = np.diag(group[var_col].values)\n",
        "        elif study_id in vcv_dict:\n",
        "            V_i = np.asarray(vcv_dict[study_id])\n",
        "            if V_i.shape[0] != k: V_i = np.diag(group[var_col].values)\n",
        "        else:\n",
        "            V_i = np.diag(group[var_col].values)\n",
        "        vcv_all.append(V_i)\n",
        "\n",
        "        # X (Spline Basis + Intercept)\n",
        "        # Slice the global basis matrix for this study\n",
        "        basis_i = basis_matrix[global_idx : global_idx + k, :]\n",
        "        X_i = np.column_stack([np.ones(k), basis_i])\n",
        "        X_all.append(X_i)\n",
        "\n",
        "        global_idx += k\n",
        "\n",
        "    N_total = len(df_sorted)\n",
        "    M_studies = len(y_all)\n",
        "    p_params = X_all[0].shape[1]\n",
        "\n",
        "    # 4. Run GLS (Fixed Variance)\n",
        "    # We pass the FIXED tau/sigma to the estimator\n",
        "    #print(\"   Step 2: Fitting spline coefficients using GLS...\")\n",
        "    spline_est = _get_gls_estimates(\n",
        "        [fixed_tau2, fixed_sigma2], y_all, vcv_all, X_all, N_total, M_studies, p_params\n",
        "    )\n",
        "\n",
        "    # 5. Omnibus Test (Wald Test)\n",
        "    # Test if all spline coefs (excluding intercept) are 0\n",
        "    betas = spline_est['betas']\n",
        "    cov_beta = spline_est['cov_beta']\n",
        "\n",
        "    # Parameters to test: indices 1 to end (0 is intercept)\n",
        "    b_test = betas[1:]\n",
        "    cov_test = cov_beta[1:, 1:]\n",
        "\n",
        "    try:\n",
        "        chi2_stat = b_test.T @ np.linalg.inv(cov_test) @ b_test\n",
        "        df_test = len(b_test)\n",
        "        p_val = 1 - chi2.cdf(chi2_stat, df_test)\n",
        "    except:\n",
        "        chi2_stat, df_test, p_val = 0, 0, 1.0\n",
        "\n",
        "    # Add metadata\n",
        "    spline_est.update({\n",
        "        'mod_mean': mod_mean,\n",
        "        'mod_std': mod_std,\n",
        "        'formula': formula,\n",
        "        'reg_df': df_sorted, # Important for plotting\n",
        "        'moderator_col': moderator_col,\n",
        "        'omnibus_chi2': chi2_stat,\n",
        "        'omnibus_df': df_test,\n",
        "        'omnibus_p': p_val,\n",
        "        'df_spline': df_spline,\n",
        "        'model_type': \"3-Level Spline (Robust Matrix)\"\n",
        "    })\n",
        "\n",
        "    return spline_est, None"
      ],
      "metadata": {
        "id": "uTWzZSuLUeAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EOEgmWzcjeOn"
      },
      "source": [
        "#@title 🌊 3-Level Spline Analysis\n",
        "\n",
        "# =============================================================================\n",
        "# CELL: SPLINE ANALYSIS WITH DASHBOARD\n",
        "# Purpose: Non-linear meta-regression using natural cubic splines.\n",
        "# Enhancement: Tabbed interface for results, diagnostics, details, and publication text.\n",
        "# Method: Uses plug-in τ² from linear model to prevent overfitting.\n",
        "# Note: Use dedicated plot cell for visualization.\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import t, chi2, norm\n",
        "from scipy.optimize import minimize_scalar\n",
        "import statsmodels.api as sm\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import warnings\n",
        "\n",
        "# Check for patsy\n",
        "try:\n",
        "    import patsy\n",
        "    PATSY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PATSY_AVAILABLE = False\n",
        "\n",
        "# --- 1. LAYOUT & WIDGETS ---\n",
        "tab_results = widgets.Output()\n",
        "tab_diagnostics = widgets.Output()\n",
        "tab_details = widgets.Output()\n",
        "tab_publication = widgets.Output()\n",
        "\n",
        "tab_export = widgets.Output()\n",
        "tabs = widgets.Tab(children=[tab_results, tab_diagnostics, tab_details, tab_publication, tab_export])\n",
        "\n",
        "tabs.set_title(0, '📊 Results')\n",
        "tabs.set_title(1, '🔍 Diagnostics')\n",
        "tabs.set_title(2, '⚙️ Model Details')\n",
        "tabs.set_title(3, '📝 Publication Text')\n",
        "tabs.set_title(4, '💾 Export')\n",
        "\n",
        "# --- 2. HELPER FUNCTIONS ---\n",
        "\n",
        "def _run_aggregated_spline_re(agg_df, moderator_col, effect_col, var_col, df_spline, mod_mean, mod_std, fixed_tau2):\n",
        "    \"\"\"\n",
        "    Runs a Random-Effects Spline Model using a FIXED Tau^2.\n",
        "    This prevents the optimizer from crashing on flat likelihood surfaces.\n",
        "    \"\"\"\n",
        "    agg_df = agg_df.reset_index(drop=True)\n",
        "    mod_z = (agg_df[moderator_col] - mod_mean) / mod_std\n",
        "    formula = f\"cr(x, df={df_spline}) - 1\"\n",
        "\n",
        "    try:\n",
        "        basis_matrix = patsy.dmatrix(formula, {\"x\": mod_z}, return_type='dataframe')\n",
        "    except Exception as e:\n",
        "        return None, f\"Basis Error: {e}\"\n",
        "\n",
        "    y = agg_df[effect_col].values\n",
        "    v = agg_df[var_col].values\n",
        "    basis_matrix.index = agg_df.index\n",
        "    X = sm.add_constant(basis_matrix)\n",
        "    weights = 1.0 / (v + fixed_tau2 + 1e-8)\n",
        "\n",
        "    try:\n",
        "        final_model = sm.WLS(y, X, weights=weights).fit()\n",
        "        resid = y - final_model.fittedvalues\n",
        "\n",
        "        XTWX = X.T @ np.diag(weights) @ X\n",
        "        sign, logdet = np.linalg.slogdet(XTWX)\n",
        "        if sign <= 0: logdet = 0\n",
        "\n",
        "        ll = -0.5 * (np.sum(np.log(v + fixed_tau2 + 1e-8)) +\n",
        "                     logdet +\n",
        "                     np.sum((resid**2) * weights))\n",
        "\n",
        "        return {\n",
        "            'betas': final_model.params.values,\n",
        "            'var_betas': final_model.cov_params().values,\n",
        "            'tau_sq': fixed_tau2,\n",
        "            'sigma_sq': 0.0,\n",
        "            'log_lik_reml': ll,\n",
        "            'mod_mean': mod_mean,\n",
        "            'mod_std': mod_std,\n",
        "            'formula': formula,\n",
        "            'model_type': 'Spline Model (Plug-in Tau²)',\n",
        "            'X_design': X,\n",
        "            'fitted': final_model.fittedvalues,\n",
        "            'resid': resid,\n",
        "            'model': final_model\n",
        "        }, None\n",
        "    except Exception as e:\n",
        "        return None, f\"Final Fit Error: {e}\"\n",
        "\n",
        "def estimate_linear_tau2(agg_df, moderator_col, effect_col, var_col):\n",
        "    \"\"\"Estimate tau² from linear model for plug-in approach\"\"\"\n",
        "    X_lin = sm.add_constant(agg_df[moderator_col])\n",
        "    y_agg = agg_df[effect_col].values\n",
        "    v_agg = agg_df[var_col].values\n",
        "\n",
        "    def lin_nll(t2):\n",
        "        if t2 < 0: t2 = 0\n",
        "        w = 1/(v_agg + t2)\n",
        "        try:\n",
        "            res = sm.WLS(y_agg, X_lin, weights=w).fit()\n",
        "            ll = -0.5*(np.sum(np.log(v_agg+t2)) +\n",
        "                      np.log(np.linalg.det(X_lin.T@np.diag(w)@X_lin)) +\n",
        "                      np.sum(res.resid**2 * w))\n",
        "            return -ll\n",
        "        except:\n",
        "            return np.inf\n",
        "\n",
        "    opt_lin = minimize_scalar(lin_nll, bounds=(0, 100), method='bounded')\n",
        "\n",
        "    # Also fit linear model for comparison\n",
        "    w_opt = 1/(v_agg + opt_lin.x)\n",
        "    lin_model = sm.WLS(y_agg, X_lin, weights=w_opt).fit()\n",
        "    lin_ll = -lin_nll(opt_lin.x)\n",
        "\n",
        "    return opt_lin.x, lin_ll, lin_model\n",
        "\n",
        "def get_numeric_mods_robust(df):\n",
        "    if df is None: return []\n",
        "    valid_mods = []\n",
        "    technical_cols = ['id', 'xe', 'xc', 'ne', 'nc', 'sde', 'sdc', 'w_fixed', 'w_random',\n",
        "                     'df', 'sp', 'sp_squared', 'hedges_j', 'weights', 'wi']\n",
        "    if 'ANALYSIS_CONFIG' in globals():\n",
        "        technical_cols.extend([ANALYSIS_CONFIG.get('effect_col'),\n",
        "                              ANALYSIS_CONFIG.get('var_col'),\n",
        "                              ANALYSIS_CONFIG.get('se_col')])\n",
        "    for col in df.columns:\n",
        "        if col in technical_cols or col is None: continue\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            valid_mods.append(col)\n",
        "        elif df[col].dtype == 'object':\n",
        "            try:\n",
        "                if pd.to_numeric(df[col], errors='coerce').notna().sum() >= 3:\n",
        "                    valid_mods.append(col)\n",
        "            except:\n",
        "                pass\n",
        "    return sorted(list(set(valid_mods)))\n",
        "\n",
        "def get_analysis_data():\n",
        "    if 'analysis_data' in globals(): return analysis_data\n",
        "    elif 'data_filtered' in globals(): return data_filtered\n",
        "    else: return None\n",
        "\n",
        "def generate_spline_publication_text(mod_col, df_spline, chi2_stat, df_test, p_omnibus,\n",
        "                                     tau2, k_studies, ll_spline, ll_linear,\n",
        "                                     model_type, n_coefs):\n",
        "    \"\"\"Generate publication-ready text for spline analysis\"\"\"\n",
        "\n",
        "    sig_text = \"significant\" if p_omnibus < 0.05 else \"non-significant\"\n",
        "    p_format = f\"< 0.001\" if p_omnibus < 0.001 else f\"= {p_omnibus:.3f}\"\n",
        "\n",
        "    # Calculate AIC/BIC for model comparison\n",
        "    aic_linear = -2*ll_linear + 2*2  # 2 parameters (intercept + slope)\n",
        "    aic_spline = -2*ll_spline + 2*n_coefs\n",
        "    bic_linear = -2*ll_linear + np.log(k_studies)*2\n",
        "    bic_spline = -2*ll_spline + np.log(k_studies)*n_coefs\n",
        "\n",
        "    better_model = \"spline\" if aic_spline < aic_linear else \"linear\"\n",
        "\n",
        "    text = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff;'>\n",
        "\n",
        "<h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px;'>Spline Meta-Regression Results</h3>\n",
        "\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Statistical Methods</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "To examine potential non-linear relationships between <b>{mod_col}</b> and effect sizes, we conducted a spline meta-regression analysis using natural cubic splines with <b>{df_spline} degrees of freedom</b>. The spline model allows the relationship to vary smoothly across the range of the moderator, capturing potential non-linearities that a simple linear model might miss.\n",
        "</p>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "We employed a random-effects framework with heterogeneity variance (τ²) fixed at the value estimated from the linear meta-regression model (τ² = <b>{tau2:.4f}</b>). This \"plug-in\" approach prevents overfitting and ensures stable variance estimates, as spline models can sometimes produce unrealistic variance estimates when both regression coefficients and variance components are estimated simultaneously. The analysis included <b>k = {k_studies}</b> studies.\n",
        "</p>\n",
        "\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Non-Linearity Test</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "An omnibus test for non-linearity was conducted by testing whether the spline basis coefficients (excluding the intercept) were jointly different from zero. This test evaluates whether the data support a non-linear relationship beyond what a simple linear model would predict.\n",
        "</p>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "The omnibus test for non-linearity was <b>{sig_text}</b> (χ²({df_test}) = <b>{chi2_stat:.2f}</b>, <i>p</i> {p_format}). \"\"\"\n",
        "\n",
        "    if p_omnibus < 0.05:\n",
        "        text += f\"\"\"This indicates that the relationship between {mod_col} and effect sizes exhibits <b>significant non-linear patterns</b>. The spline model provides a better fit to the data compared to a simple linear model.\n",
        "</p>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "[<i>Describe the nature of the non-linearity based on visual inspection of the spline plot: Does the effect increase then plateau? Is there a threshold effect? Are there diminishing returns? Quadratic pattern? Provide domain-specific interpretation.</i>]\n",
        "</p>\n",
        "\"\"\"\n",
        "    else:\n",
        "        text += f\"\"\"This suggests that a simple linear relationship may adequately describe the association between {mod_col} and effect sizes. While we fitted a flexible spline model, the data do not provide strong evidence for non-linear patterns.\n",
        "</p>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "The lack of significant non-linearity could indicate that: (1) the relationship is genuinely linear across the observed range of {mod_col}, (2) sample size may be insufficient to detect subtle non-linear patterns, or (3) the range of {mod_col} values may be too narrow to reveal non-linear trends. [<i>Add domain-specific interpretation based on your theoretical expectations.</i>]\n",
        "</p>\n",
        "\"\"\"\n",
        "\n",
        "    # Model comparison section\n",
        "    text += f\"\"\"\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Model Comparison</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "We compared the spline model to a simpler linear meta-regression using information criteria. The Akaike Information Criterion (AIC) penalizes model complexity while rewarding fit, with lower values indicating better models.\n",
        "</p>\n",
        "\n",
        "<div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 15px 0;'>\n",
        "<p style='margin: 5px 0;'><b>Linear Model:</b> AIC = {aic_linear:.2f}, BIC = {bic_linear:.2f}</p>\n",
        "<p style='margin: 5px 0;'><b>Spline Model:</b> AIC = {aic_spline:.2f}, BIC = {bic_spline:.2f}</p>\n",
        "<p style='margin: 5px 0;'><b>Preferred Model:</b> {better_model.capitalize()} (lower AIC)</p>\n",
        "</div>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "{'The spline model provides a better fit (lower AIC), supporting the presence of non-linear patterns.' if better_model == 'spline' else 'The linear model is preferred (lower AIC), suggesting that added complexity of the spline does not substantially improve model fit.'}\n",
        "</p>\n",
        "\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Practical Implications</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "[<i>Discuss what these findings mean for your research domain. For example:</i>]\n",
        "</p>\n",
        "\n",
        "<ul style='line-height: 2.0;'>\n",
        "<li><i>If non-linear:</i> \"The non-linear relationship suggests that the effect of {mod_col} varies across its range. [Describe pattern: e.g., 'Effects are strongest at moderate levels', 'There appears to be a threshold at X value', 'Diminishing returns are observed at higher levels']\"</li>\n",
        "<li><i>If linear:</i> \"The linear relationship suggests a consistent, proportional association between {mod_col} and outcomes across its observed range.\"</li>\n",
        "<li><i>Implications for practice:</i> \"These findings suggest that [practical recommendations based on the shape of the relationship]\"</li>\n",
        "</ul>\n",
        "\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Model Specification</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "The spline model was fitted as: y<sub>i</sub> = β₀ + f({mod_col}<sub>i</sub>) + u<sub>i</sub>, where f(·) represents the natural cubic spline function with {df_spline} degrees of freedom, and u<sub>i</sub> ~ N(0, τ²) represents between-study random effects. The moderator was standardized (mean-centered and scaled) before spline basis generation to improve numerical stability.\n",
        "</p>\n",
        "\n",
        "<hr style='margin: 30px 0; border: none; border-top: 1px solid #bdc3c7;'>\n",
        "\n",
        "<div style='background-color: #ecf0f1; padding: 20px; border-left: 4px solid #3498db; margin-top: 25px;'>\n",
        "<h4 style='margin-top: 0; color: #2c3e50;'>📊 Table 1. Spline Model Summary</h4>\n",
        "<table style='width: 100%; border-collapse: collapse; margin-top: 15px; background-color: white;'>\n",
        "<thead style='background-color: #34495e; color: white;'>\n",
        "<tr>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: left;'>Statistic</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>Value</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr style='background-color: #f8f9fa;'>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>Number of studies (k)</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{k_studies}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>Spline degrees of freedom</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{df_spline}</td>\n",
        "</tr>\n",
        "<tr style='background-color: #f8f9fa;'>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>Between-study variance (τ²)</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{tau2:.4f}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>Omnibus test for non-linearity</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>χ²({df_test}) = {chi2_stat:.2f}, <i>p</i> {p_format}</td>\n",
        "</tr>\n",
        "<tr style='background-color: #f8f9fa;'>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>Model comparison (vs. linear)</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{better_model.capitalize()} model preferred</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "</div>\n",
        "\n",
        "<hr style='margin: 30px 0; border: none; border-top: 1px solid #bdc3c7;'>\n",
        "\n",
        "<div style='background-color: #ecf0f1; padding: 15px; border-left: 4px solid #3498db; margin-top: 20px;'>\n",
        "<h4 style='margin-top: 0; color: #2c3e50;'>Interpretation Guidance:</h4>\n",
        "<ul style='margin-bottom: 0;'>\n",
        "<li>Examine the spline plot (dedicated plot cell) to understand the shape of the non-linear relationship</li>\n",
        "<li>Identify key features: thresholds, plateaus, peaks, or inflection points</li>\n",
        "<li>Compare spline predictions to linear predictions to see where they diverge</li>\n",
        "<li>Consider whether non-linearity has practical significance beyond statistical significance</li>\n",
        "<li>Link the pattern to theoretical mechanisms in your field</li>\n",
        "<li>Discuss whether the observed range of {mod_col} is sufficient to reveal the full non-linear pattern</li>\n",
        "<li>Consider sensitivity to df choice (try df=3, 4, 5 to see if pattern is robust)</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "<div style='background-color: #fff3cd; padding: 10px; border-left: 4px solid #ffc107; margin-top: 15px;'>\n",
        "<p style='margin: 0;'><b>💡 Tip:</b> Select all text (Ctrl+A / Cmd+A), copy (Ctrl+C / Cmd+C), and paste into your word processor. Edit the [<i>bracketed notes</i>] to add your specific interpretations of the non-linear pattern.</p>\n",
        "</div>\n",
        "\n",
        "</div>\"\"\"\n",
        "\n",
        "    return text\n",
        "\n",
        "def generate_methods_text_spline(es_config, spline_results):\n",
        "    \"\"\"\n",
        "    Generates a 'Materials and Methods' section for Spline Analysis.\n",
        "    \"\"\"\n",
        "    import sys\n",
        "\n",
        "    # 1. Extract Settings\n",
        "    mod_col = spline_results.get('moderator_col', 'the moderator')\n",
        "    df_spline = spline_results.get('df_spline', 3)\n",
        "\n",
        "    # 2. Define Citation Database\n",
        "    db = {\n",
        "        'splines': \"Durrleman, S., & Simon, R. (1989). Flexible regression models with cubic splines. <i>Statistics in Medicine</i>, 8(5), 551-561.\",\n",
        "        'harrell': \"Harrell, F. E. (2001). <i>Regression Modeling Strategies</i>. New York: Springer.\",\n",
        "        'aic': \"Burnham, K. P., & Anderson, D. R. (2002). <i>Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach</i> (2nd ed.). New York: Springer-Verlag.\",\n",
        "        'plugin': \"Jackson, D., White, I. R., & Thompson, S. G. (2010). Extending DerSimonian and Laird's methodology to perform multivariate random effects meta-analyses. <i>Statistics in Medicine</i>, 29(12), 1282-1297.\",\n",
        "        'tool': \"<b>[CITATION FOR THIS TOOL]:</b> Author, A. A. (202X). <i>Co-Meta: An Interactive Pipeline for Meta-Analysis</i>. [Repository/DOI].\"\n",
        "    }\n",
        "\n",
        "    # 3. Build HTML\n",
        "    html = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff; border: 1px solid #eee; margin-bottom: 20px;'>\n",
        "\n",
        "    <h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-top: 0;'>Materials and Methods</h3>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Spline Meta-Regression.</b> To detect potential non-linear relationships between the effect size and <b>{mod_col}</b>, we fitted a Restricted Cubic Spline model [1].\n",
        "    Restricted (natural) cubic splines are linear beyond the boundary knots, preventing unrealistic behavior at the tails of the distribution [2].\n",
        "    The model was specified with {df_spline} degrees of freedom.\n",
        "    </p>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Variance Estimation.</b> To ensure numerical stability and prevent overfitting in the flexible spline model, we employed a \\\"plug-in\\\" approach for the between-study variance ($\\tau^2$).\n",
        "    The $\\tau^2$ value was estimated from the linear random-effects model and held fixed during the spline optimization [3].\n",
        "    </p>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Hypothesis Testing.</b> Non-linearity was assessed using an omnibus Wald-type $\\chi^2$ test, evaluating whether the non-linear spline coefficients were jointly significantly different from zero.\n",
        "    Model fit was compared between the linear and spline models using the Akaike Information Criterion (AIC) [4].\n",
        "    All analyses were performed using the Co-Meta toolkit [5].\n",
        "    </p>\n",
        "\n",
        "    <h4 style='color: #34495e; margin-top: 20px; font-size: 11pt;'>References</h4>\n",
        "    <ol style='font-size: 10pt; color: #555;'>\n",
        "        <li>{db['splines']}</li>\n",
        "        <li>{db['harrell']}</li>\n",
        "        <li>{db['plugin']}</li>\n",
        "        <li>{db['aic']}</li>\n",
        "        <li>{db['tool']}</li>\n",
        "    </ol>\n",
        "    </div>\"\"\"\n",
        "\n",
        "    return html\n",
        "\n",
        "# --- 3. UI WIDGETS ---\n",
        "df_spline_in = get_analysis_data()\n",
        "opts = get_numeric_mods_robust(df_spline_in) if df_spline_in is not None else ['Data not loaded']\n",
        "\n",
        "mod_widget = widgets.Dropdown(\n",
        "    options=opts,\n",
        "    description='Moderator:',\n",
        "    layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "df_widget = widgets.IntSlider(\n",
        "    value=3,\n",
        "    min=3,\n",
        "    max=6,\n",
        "    description='Spline df:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "run_spline_btn = widgets.Button(\n",
        "    description='▶ Run Spline Analysis',\n",
        "    button_style='success',\n",
        "    icon='play'\n",
        ")\n",
        "\n",
        "# --- 4. MAIN ANALYSIS FUNCTION ---\n",
        "def run_spline(b):\n",
        "    global ANALYSIS_CONFIG\n",
        "\n",
        "    # Clear all tabs\n",
        "    for tab in [tab_results, tab_diagnostics, tab_details, tab_publication]:\n",
        "        tab.clear_output()\n",
        "\n",
        "    if not PATSY_AVAILABLE:\n",
        "        with tab_results:\n",
        "            display(HTML(\"<div style='color: red;'>❌ Error: 'patsy' package not installed. Install with: pip install patsy</div>\"))\n",
        "        return\n",
        "\n",
        "    mod_col = mod_widget.value\n",
        "    df_k = df_widget.value\n",
        "\n",
        "    df_working = get_analysis_data()\n",
        "    if df_working is None:\n",
        "        with tab_results:\n",
        "            display(HTML(\"<div style='color: red;'>❌ Error: Data not found. Run Step 1 first.</div>\"))\n",
        "        return\n",
        "\n",
        "    if 'ANALYSIS_CONFIG' not in globals():\n",
        "        with tab_results:\n",
        "            display(HTML(\"<div style='color: red;'>❌ Error: Config not found. Run Step 1 first.</div>\"))\n",
        "        return\n",
        "\n",
        "    eff_col = ANALYSIS_CONFIG.get('effect_col', 'hedges_g')\n",
        "    var_col = ANALYSIS_CONFIG.get('var_col', 'Vg')\n",
        "\n",
        "    # Prep data\n",
        "    df = df_working.copy()\n",
        "    df[mod_col] = pd.to_numeric(df[mod_col], errors='coerce')\n",
        "    df = df.dropna(subset=[mod_col, eff_col, var_col])\n",
        "    df = df[df[var_col] > 0]\n",
        "\n",
        "    if len(df) < 3:\n",
        "        with tab_results:\n",
        "            display(HTML(f\"<div style='color: red;'>❌ Error: Insufficient data (n={len(df)}). Need at least 3 observations.</div>\"))\n",
        "        return\n",
        "\n",
        "    # --- 1. RUN ROBUST SPLINE ANALYSIS ---\n",
        "    with tab_results:\n",
        "        print(\"⏳ Running Robust Spline Analysis...\")\n",
        "\n",
        "    est, err = _run_robust_spline_analysis(\n",
        "        df, mod_col, eff_col, var_col, df_spline=df_k\n",
        "    )\n",
        "\n",
        "    if err:\n",
        "        with tab_results:\n",
        "            clear_output()\n",
        "            display(HTML(f\"<div style='color: red;'>❌ {err}</div>\"))\n",
        "        return\n",
        "\n",
        "    # --- 2. GET LINEAR MODEL STATS (FOR COMPARISON) ---\n",
        "    # We re-run the linear regression driver to get its AIC/LogLik\n",
        "    lin_est, _, _ = _run_three_level_reml_regression_v2(\n",
        "        df, mod_col, eff_col, var_col\n",
        "    )\n",
        "\n",
        "    if lin_est:\n",
        "        ll_linear = lin_est['log_lik_reml']\n",
        "        # AIC = 2k - 2ln(L). k=2 for linear (intercept + slope)\n",
        "        aic_linear = 2*2 - 2*ll_linear\n",
        "    else:\n",
        "        ll_linear = np.nan\n",
        "        aic_linear = np.nan\n",
        "\n",
        "    # --- 3. EXTRACT RESULTS ---\n",
        "    betas = est['betas']\n",
        "    ll_spline = est['log_lik_reml']\n",
        "\n",
        "    # Calculate Spline AIC. k = number of betas (df + 1 intercept)\n",
        "    aic_spline = 2*len(betas) - 2*ll_spline\n",
        "\n",
        "    chi2_stat = est['omnibus_chi2']\n",
        "    df_test = est['omnibus_df']\n",
        "    p_omnibus = est['omnibus_p']\n",
        "    fixed_tau2 = est['tau_sq']\n",
        "\n",
        "    # --- 4. SAVE RESULTS (CORRECTED) ---\n",
        "    if 'ANALYSIS_CONFIG' not in globals():\n",
        "        ANALYSIS_CONFIG = {}\n",
        "\n",
        "    # FIX: Use est['cov_beta'] and est['reg_df'] instead of undefined variables\n",
        "    ANALYSIS_CONFIG['spline_model_results'] = est\n",
        "    # Add linear stats for the text generator\n",
        "    ANALYSIS_CONFIG['spline_model_results']['ll_linear'] = ll_linear\n",
        "\n",
        "    # --- 5. RENDER RESULTS TAB ---\n",
        "    with tab_results:\n",
        "        clear_output()\n",
        "        sig = \"***\" if p_omnibus < 0.001 else \"**\" if p_omnibus < 0.01 else \"*\" if p_omnibus < 0.05 else \"ns\"\n",
        "        color = \"#28a745\" if p_omnibus < 0.05 else \"#6c757d\"\n",
        "\n",
        "        # Determine preferred model\n",
        "        preferred = \"Spline\" if (not np.isnan(aic_linear) and aic_spline < aic_linear) else \"Linear\"\n",
        "\n",
        "        html = f\"\"\"\n",
        "        <div style='padding: 20px;'>\n",
        "        <h2 style='color: #2c3e50; margin-bottom: 20px;'>Spline Meta-Regression: {mod_col}</h2>\n",
        "\n",
        "        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white; margin-bottom: 20px;'>\n",
        "            <div style='text-align: center;'>\n",
        "                <div style='font-size: 0.9em; margin-bottom: 10px;'>OMNIBUS TEST FOR NON-LINEARITY</div>\n",
        "                <h1 style='margin: 0; font-size: 2.5em;'>{\"Significant\" if p_omnibus < 0.05 else \"Not Significant\"}</h1>\n",
        "                <p style='margin: 10px 0 0 0; font-size: 1.2em;'>p {p_omnibus:.4g} {sig}</p>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div style='display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-bottom: 20px;'>\n",
        "            <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; border-left: 4px solid #007bff;'>\n",
        "                <div style='color: #6c757d; font-size: 0.9em;'>Chi-Square Statistic</div>\n",
        "                <div style='font-size: 1.5em; font-weight: bold;'>χ²({df_test}) = {chi2_stat:.2f}</div>\n",
        "            </div>\n",
        "            <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; border-left: 4px solid {color};'>\n",
        "                <div style='color: #6c757d; font-size: 0.9em;'>P-value</div>\n",
        "                <div style='font-size: 1.5em; font-weight: bold; color: {color};'>{p_omnibus:.4g}</div>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div style='background-color: #e7f3ff; padding: 20px; border-radius: 5px; margin-bottom: 20px;'>\n",
        "            <h4 style='margin-top: 0; color: #2c3e50;'>Interpretation</h4>\n",
        "            <p style='margin: 0; font-size: 1.05em;'>\n",
        "        \"\"\"\n",
        "\n",
        "        if p_omnibus < 0.05:\n",
        "            html += f\"\"\"The relationship between <b>{mod_col}</b> and effect sizes exhibits <b style='color: #28a745;'>significant non-linear patterns</b>.\n",
        "                A flexible spline model fits the data better than a simple linear relationship.\n",
        "                Examine the spline plot to understand the nature of this non-linearity.\"\"\"\n",
        "        else:\n",
        "            html += f\"\"\"No significant evidence of non-linearity was detected.\n",
        "                A simple linear relationship may adequately describe the association between <b>{mod_col}</b> and effect sizes.\n",
        "                The added complexity of the spline model does not significantly improve fit.\"\"\"\n",
        "\n",
        "        html += f\"\"\"\n",
        "            </p>\n",
        "        </div>\n",
        "\n",
        "        <h3 style='color: #2c3e50; border-bottom: 2px solid #dee2e6; padding-bottom: 10px;'>Model Comparison</h3>\n",
        "        <table style='width: 100%; border-collapse: collapse; margin: 20px 0;'>\n",
        "            <thead style='background-color: #2c3e50; color: white;'>\n",
        "                <tr>\n",
        "                    <th style='padding: 12px; text-align: left; border: 1px solid #dee2e6;'>Model</th>\n",
        "                    <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>Parameters</th>\n",
        "                    <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>Log-Likelihood</th>\n",
        "                    <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>AIC</th>\n",
        "                    <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>Preferred</th>\n",
        "                </tr>\n",
        "            </thead>\n",
        "            <tbody>\n",
        "                <tr style='background-color: #f8f9fa;'>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'><b>Linear</b></td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>2</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{ll_linear:.2f}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{aic_linear:.2f}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{\"✓\" if preferred == \"Linear\" else \"\"}</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td style='padding: 10px; border: 1px solid #dee2e6;'><b>Spline (df={df_k})</b></td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{len(betas)}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{ll_spline:.2f}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{aic_spline:.2f}</td>\n",
        "                    <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{\"✓\" if preferred == \"Spline\" else \"\"}</td>\n",
        "                </tr>\n",
        "            </tbody>\n",
        "        </table>\n",
        "\n",
        "        <h3 style='color: #2c3e50; border-bottom: 2px solid #dee2e6; padding-bottom: 10px;'>Model Summary</h3>\n",
        "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px;'>\n",
        "            <p style='margin: 5px 0;'><b>Model Type:</b> {est.get('model_type', 'Spline')}</p>\n",
        "            <p style='margin: 5px 0;'><b>Studies (k):</b> {est.get('reg_df', pd.DataFrame()).shape[0]}</p>\n",
        "            <p style='margin: 5px 0;'><b>Spline Degrees of Freedom:</b> {df_k}</p>\n",
        "            <p style='margin: 5px 0;'><b>Number of Coefficients:</b> {len(betas)} (1 intercept + {len(betas)-1} spline terms)</p>\n",
        "            <p style='margin: 5px 0;'><b>Between-Study Variance (τ²):</b> {fixed_tau2:.4f} (fixed from linear model)</p>\n",
        "        </div>\n",
        "\n",
        "        <div style='background-color: #fff3cd; padding: 15px; border-radius: 5px; border-left: 4px solid #ffc107; margin-top: 20px;'>\n",
        "            <p style='margin: 0;'><b>📊 Next Step:</b> Use the dedicated spline plot cell to visualize the non-linear relationship and identify key features (thresholds, plateaus, etc.).</p>\n",
        "        </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        display(HTML(html))\n",
        "\n",
        "    # --- TAB 2: DIAGNOSTICS ---\n",
        "    with tab_diagnostics:\n",
        "        clear_output()\n",
        "        # Simple diagnostics display\n",
        "        resid = est['resid']\n",
        "        diag_html = f\"\"\"\n",
        "        <div style='padding: 20px;'>\n",
        "        <h4 style='color: #34495e;'>Residual Analysis</h4>\n",
        "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px;'>\n",
        "            <p><b>Mean Residual:</b> {np.mean(resid):.4f}</p>\n",
        "            <p><b>SD Residual:</b> {np.std(resid):.4f}</p>\n",
        "            <p><b>Range:</b> [{np.min(resid):.3f}, {np.max(resid):.3f}]</p>\n",
        "        </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(diag_html))\n",
        "\n",
        "    # --- TAB 3: DETAILS ---\n",
        "    with tab_details:\n",
        "        clear_output()\n",
        "        details_html = f\"\"\"\n",
        "        <div style='padding: 20px;'>\n",
        "        <h4 style='color: #34495e;'>Coefficient Estimates</h4>\n",
        "        <p><i>See Results tab for Omnibus Test</i></p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(details_html))\n",
        "\n",
        "    # --- TAB 4: PUBLICATION TEXT ---\n",
        "    with tab_publication:\n",
        "        clear_output()\n",
        "        display(HTML(\"<h3 style='color: #2c3e50;'>📝 Publication-Ready Results Text</h3>\"))\n",
        "        display(HTML(\"<p style='color: #6c757d;'>Copy and paste this formatted text into your manuscript:</p>\"))\n",
        "\n",
        "        # A. Generate METHODS Text\n",
        "        methods_html = generate_methods_text_spline(\n",
        "            ANALYSIS_CONFIG.get('es_config', {}),\n",
        "            est\n",
        "        )\n",
        "\n",
        "        # B. Generate RESULTS Text\n",
        "        results_html = generate_spline_publication_text(\n",
        "            mod_col, df_k, chi2_stat, df_test, p_omnibus,\n",
        "            fixed_tau2, est.get('reg_df', pd.DataFrame()).shape[0],\n",
        "            ll_spline, ll_linear,\n",
        "            est.get('model_type', 'Spline'), len(betas)\n",
        "        )\n",
        "\n",
        "        display(HTML(methods_html))\n",
        "        display(HTML(results_html))\n",
        "\n",
        "        ANALYSIS_CONFIG['spline_text'] = methods_html + \"<br><hr><br>\" + results_html\n",
        "run_spline_btn.on_click(run_spline)\n",
        "\n",
        "# --- 5. DISPLAY UI ---\n",
        "display(HTML(\"<h3>🌊 Spline Meta-Regression Analysis (V2)</h3>\"))\n",
        "display(HTML(\"<p style='color: #6c757d;'>Test for non-linear relationships using natural cubic splines. Results appear in organized tabs below.</p>\"))\n",
        "display(widgets.VBox([mod_widget, df_widget, run_spline_btn]))\n",
        "display(tabs)\n",
        "\n",
        "# --- ADD THIS CODE TO THE END OF THE CELL ---\n",
        "with tab_export:\n",
        "    display(HTML(\"<h3 style='color: #2E86AB;'>💾 Download Audit Report</h3>\"))\n",
        "    display(HTML(\"<p>Download the Spline analysis results, including the non-linearity test and model comparison statistics.</p>\"))\n",
        "\n",
        "    btn_spline_export = widgets.Button(\n",
        "        description=\"📥 Download Spline Report\",\n",
        "        button_style='info',\n",
        "        icon='file-excel',\n",
        "        layout=widgets.Layout(width='300px', height='40px')\n",
        "    )\n",
        "\n",
        "    def on_spline_export_click(b):\n",
        "        if 'pub_text' in locals():\n",
        "            ANALYSIS_CONFIG['latest_text'] = pub_text\n",
        "\n",
        "        export_analysis_report(report_type='spline', filename_prefix='Spline_Analysis_Audit')\n",
        "\n",
        "    btn_spline_export.on_click(on_spline_export_click)\n",
        "    display(btn_spline_export)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 📊 Spline Plot\n",
        "# =============================================================================\n",
        "# CELL 11b: ADVANCED SPLINE PLOTTER\n",
        "# Purpose: Visualize results from Cell 11 with full customization.\n",
        "# Features: Tabs for Style, Points, Curve, Layout, and Label Editing.\n",
        "# Compatibility: Works with the new Robust/Aggregated Spline results.\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import t\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import traceback\n",
        "import patsy\n",
        "\n",
        "# --- 1. INITIALIZATION & CONFIG LOADING ---\n",
        "available_color_moderators = ['None']\n",
        "analysis_data_init = None\n",
        "default_x_label = \"Moderator\"\n",
        "default_y_label = \"Effect Size\"\n",
        "default_title = \"Natural Cubic Spline Analysis\"\n",
        "label_widgets_dict = {}\n",
        "\n",
        "try:\n",
        "    if 'ANALYSIS_CONFIG' not in globals():\n",
        "        raise NameError(\"ANALYSIS_CONFIG not found\")\n",
        "\n",
        "    # Get data for dropdowns\n",
        "    if 'analysis_data' in globals():\n",
        "        analysis_data_init = analysis_data.copy()\n",
        "    elif 'data_filtered' in globals():\n",
        "        analysis_data_init = data_filtered.copy()\n",
        "    else:\n",
        "        # Fallback to reg_df if main data missing\n",
        "        if 'spline_model_results' in ANALYSIS_CONFIG:\n",
        "            analysis_data_init = ANALYSIS_CONFIG['spline_model_results']['reg_df'].copy()\n",
        "\n",
        "    # Load Defaults from Results\n",
        "    if 'spline_model_results' in ANALYSIS_CONFIG:\n",
        "        spline_results = ANALYSIS_CONFIG['spline_model_results']\n",
        "        es_config = ANALYSIS_CONFIG.get('es_config', {})\n",
        "        default_x_label = spline_results.get('moderator_col', 'Moderator')\n",
        "        default_y_label = es_config.get('effect_label', 'Effect Size')\n",
        "        default_title = f\"Spline Regression: {default_y_label} vs. {default_x_label}\"\n",
        "\n",
        "    # Identify Categorical Moderators for Coloring\n",
        "    if analysis_data_init is not None:\n",
        "        excluded_cols = [\n",
        "            ANALYSIS_CONFIG.get('effect_col'), ANALYSIS_CONFIG.get('var_col'),\n",
        "            ANALYSIS_CONFIG.get('se_col'), 'w_fixed', 'w_random', 'id',\n",
        "            'xe', 'sde', 'ne', 'xc', 'sdc', 'nc'\n",
        "        ]\n",
        "\n",
        "        for col in analysis_data_init.columns:\n",
        "            if col in excluded_cols or col is None: continue\n",
        "            # Check if categorical (object or category) and reasonable size\n",
        "            if analysis_data_init[col].dtype == 'object' or isinstance(analysis_data_init[col].dtype, pd.CategoricalDtype):\n",
        "                if analysis_data_init[col].nunique() <= 15: # Limit to reasonable number of colors\n",
        "                    available_color_moderators.append(col)\n",
        "\n",
        "    # Find unique labels for Editor\n",
        "    all_categorical_labels = set()\n",
        "    for col in available_color_moderators:\n",
        "        if col != 'None' and col in analysis_data_init.columns:\n",
        "            all_categorical_labels.add(col)\n",
        "            unique_vals = analysis_data_init[col].astype(str).str.strip().unique()\n",
        "            all_categorical_labels.update(unique_vals)\n",
        "\n",
        "    all_categorical_labels.discard('')\n",
        "    all_categorical_labels.discard('nan')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠️  Initialization Warning: {e}\")\n",
        "\n",
        "# --- 2. WIDGET DEFINITIONS ---\n",
        "\n",
        "# === TAB 1: STYLE ===\n",
        "show_title_widget = widgets.Checkbox(value=True, description='Show Plot Title', indent=False)\n",
        "title_widget = widgets.Text(value=default_title, description='Title:', layout=widgets.Layout(width='450px'))\n",
        "xlabel_widget = widgets.Text(value=default_x_label, description='X Label:', layout=widgets.Layout(width='450px'))\n",
        "ylabel_widget = widgets.Text(value=default_y_label, description='Y Label:', layout=widgets.Layout(width='450px'))\n",
        "width_widget = widgets.FloatSlider(value=10.0, min=5.0, max=16.0, step=0.5, description='Width (in):', continuous_update=False)\n",
        "height_widget = widgets.FloatSlider(value=6.0, min=4.0, max=12.0, step=0.5, description='Height (in):', continuous_update=False)\n",
        "\n",
        "style_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Labels & Dimensions</h4>\"),\n",
        "    show_title_widget, title_widget,\n",
        "    xlabel_widget, ylabel_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 5px 0;'>\"),\n",
        "    preset_widget,\n",
        "    width_widget, height_widget\n",
        "])\n",
        "\n",
        "# === TAB 2: POINTS ===\n",
        "show_points_widget = widgets.Checkbox(value=True, description='Show Data Points', indent=False)\n",
        "color_mod_widget = widgets.Dropdown(options=available_color_moderators, value='None', description='Color By:', layout=widgets.Layout(width='400px'))\n",
        "point_color_widget = widgets.Dropdown(options=['gray', 'steelblue', 'black', 'red', 'green', 'purple'], value='gray', description='Color:')\n",
        "point_size_widget = widgets.IntSlider(value=40, min=10, max=150, step=5, description='Size:')\n",
        "point_alpha_widget = widgets.FloatSlider(value=0.5, min=0.1, max=1.0, step=0.1, description='Opacity:')\n",
        "\n",
        "points_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Data Points</h4>\"),\n",
        "    show_points_widget,\n",
        "    color_mod_widget,\n",
        "    point_color_widget,\n",
        "    point_size_widget,\n",
        "    point_alpha_widget\n",
        "])\n",
        "\n",
        "# === TAB 3: CURVE ===\n",
        "curve_color_widget = widgets.Dropdown(options=['blue', 'red', 'black', 'green', 'purple'], value='blue', description='Line Color:')\n",
        "curve_width_widget = widgets.FloatSlider(value=2.5, min=0.5, max=6.0, step=0.5, description='Line Width:')\n",
        "show_ci_widget = widgets.Checkbox(value=True, description='Show 95% Confidence Band', indent=False)\n",
        "ci_alpha_widget = widgets.FloatSlider(value=0.15, min=0.05, max=0.5, step=0.05, description='CI Opacity:')\n",
        "show_stats_widget = widgets.Checkbox(value=True, description='Show Stats (P-value/R²)', indent=False)\n",
        "\n",
        "curve_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Spline Curve</h4>\"),\n",
        "    curve_color_widget, curve_width_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 5px 0;'>\"),\n",
        "    show_ci_widget, ci_alpha_widget,\n",
        "    show_stats_widget\n",
        "])\n",
        "\n",
        "# === TAB 4: LAYOUT ===\n",
        "show_grid_widget = widgets.Checkbox(value=True, description='Show Grid', indent=False)\n",
        "show_null_line_widget = widgets.Checkbox(value=True, description='Show Null Line (y=0)', indent=False)\n",
        "legend_loc_widget = widgets.Dropdown(options=['best', 'upper right', 'upper left', 'lower right', 'lower left', 'none'], value='best', description='Legend:')\n",
        "save_pdf_widget = widgets.Checkbox(value=True, description='Save as PDF', indent=False)\n",
        "save_png_widget = widgets.Checkbox(value=True, description='Save as PNG', indent=False)\n",
        "filename_prefix_widget = widgets.Text(value='Spline_Plot', description='Filename:', layout=widgets.Layout(width='300px'))\n",
        "\n",
        "layout_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Layout & Export</h4>\"),\n",
        "    show_grid_widget, show_null_line_widget, legend_loc_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 5px 0;'>\"),\n",
        "    save_pdf_widget, save_png_widget, filename_prefix_widget\n",
        "])\n",
        "\n",
        "# === TAB 5: LABELS (Dynamic) ===\n",
        "label_editor_widgets = []\n",
        "label_widgets_dict = {}\n",
        "\n",
        "if all_categorical_labels:\n",
        "    for label in sorted(list(all_categorical_labels)):\n",
        "        w = widgets.Text(value=str(label), description=f\"{label}:\", layout=widgets.Layout(width='400px'))\n",
        "        label_editor_widgets.append(w)\n",
        "        label_widgets_dict[str(label)] = w\n",
        "else:\n",
        "    label_editor_widgets.append(widgets.Label(\"No categorical labels found to edit.\"))\n",
        "\n",
        "labels_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Label Editor</h4>\"),\n",
        "    widgets.HTML(\"<i>Rename data categories for the legend:</i>\"),\n",
        "    *label_editor_widgets\n",
        "])\n",
        "\n",
        "# Assemble Tabs\n",
        "tabs = widgets.Tab(children=[style_tab, points_tab, curve_tab, layout_tab, labels_tab])\n",
        "tabs.set_title(0, '🎨 Style')\n",
        "tabs.set_title(1, '⚫ Points')\n",
        "tabs.set_title(2, '🌊 Curve')\n",
        "tabs.set_title(3, '💾 Layout')\n",
        "tabs.set_title(4, '✏️ Labels')\n",
        "\n",
        "run_plot_btn = widgets.Button(\n",
        "    description='📊 Generate Spline Plot',\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='450px', height='50px'),\n",
        "    style={'font_weight': 'bold'}\n",
        ")\n",
        "plot_output = widgets.Output()\n",
        "\n",
        "# --- 3. PLOTTING LOGIC ---\n",
        "def generate_spline_plot(b):\n",
        "    with plot_output:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        try:\n",
        "            # --- Get Global Settings ---\n",
        "            gs = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "            alpha = gs.get('alpha', 0.05)\n",
        "            dist_type = gs.get('dist_type', 'norm')\n",
        "            ci_pct = (1 - alpha) * 100\n",
        "\n",
        "            # 1. Load Results\n",
        "            if 'ANALYSIS_CONFIG' not in globals() or 'spline_model_results' not in ANALYSIS_CONFIG:\n",
        "                print(\"❌ Error: Please run the Spline Analysis (Cell 11) first.\")\n",
        "                return\n",
        "\n",
        "            res = ANALYSIS_CONFIG['spline_model_results']\n",
        "            df = res['reg_df'].copy() # Dataframe used in model\n",
        "\n",
        "            # Extract Model info\n",
        "            betas = res['betas']\n",
        "            cov = res['var_betas']\n",
        "            formula = res['formula']\n",
        "            mod_mean = res['mod_mean']\n",
        "            mod_std = res['mod_std']\n",
        "            mod_col = res['moderator_col']\n",
        "            eff_col = ANALYSIS_CONFIG['effect_col']\n",
        "\n",
        "            # 2. Re-calculate Curve (High Resolution)\n",
        "            x_min, x_max = df[mod_col].min(), df[mod_col].max()\n",
        "            padding = (x_max - x_min) * 0.05\n",
        "            x_grid = np.linspace(x_min - padding, x_max + padding, 200)\n",
        "            x_grid_z = (x_grid - mod_mean) / mod_std\n",
        "\n",
        "            # Generate Basis for Grid\n",
        "            try:\n",
        "                # We need to match the column structure of the model\n",
        "                # The model might have dropped columns (collinearity), so we need to be careful.\n",
        "                pred_matrix = patsy.dmatrix(formula, {\"x\": x_grid_z}, return_type='dataframe')\n",
        "                X_pred_full = sm.add_constant(pred_matrix)\n",
        "\n",
        "                # Filter columns to match what the model used\n",
        "                # If 'kept_cols' or similar isn't saved, we assume simple match by length or name if possible\n",
        "                # But simpler: matrix multiplication handles it if shapes match\n",
        "                # Check shape\n",
        "                if X_pred_full.shape[1] != len(betas):\n",
        "                    # Try to align by column names if available, otherwise simple slice\n",
        "                    if hasattr(res, 'get') and res.get('X_design_cols') is not None:\n",
        "                        # Robust matching using saved column names\n",
        "                        cols = res['X_design_cols']\n",
        "                        # Make sure X_pred_full has these columns (it should if formula is same)\n",
        "                        # Note: patsy names might differ slightly if not careful, but usually stable\n",
        "                        X_pred = X_pred_full.values[:, :len(betas)] # Fallback\n",
        "                    else:\n",
        "                         # Fallback: Assume the first K columns are the ones kept\n",
        "                         X_pred = X_pred_full.values[:, :len(betas)]\n",
        "                else:\n",
        "                    X_pred = X_pred_full.values\n",
        "\n",
        "                # Calculate\n",
        "                y_pred = X_pred @ betas\n",
        "                pred_var = np.sum((X_pred @ cov) * X_pred, axis=1)\n",
        "                pred_se = np.sqrt(pred_var)\n",
        "\n",
        "                # --- UPDATED: Dynamic Critical Value ---\n",
        "                q_val = 1 - (alpha / 2)\n",
        "                if dist_type == 't':\n",
        "                    # Splines often use Z, but if T is requested, we need a DF.\n",
        "                    # We can use (k_studies - n_params) as a conservative estimate\n",
        "                    df_spline_resid = max(1, len(df) - len(betas))\n",
        "                    crit_val = t.ppf(q_val, df_spline_resid)\n",
        "                else:\n",
        "                    crit_val = norm.ppf(q_val)\n",
        "                # ---------------------------------------\n",
        "\n",
        "                ci_lower = y_pred - crit_val * pred_se\n",
        "                ci_upper = y_pred + crit_val * pred_se\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error calculating curve: {e}\")\n",
        "                print(\"   (Did the model structure change?)\")\n",
        "                return\n",
        "\n",
        "            # 3. Prepare Plot\n",
        "            fig, ax = plt.subplots(figsize=(width_widget.value, height_widget.value))\n",
        "\n",
        "            # Handle Colors & Labels\n",
        "            color_col = color_mod_widget.value\n",
        "            label_map = {k: v.value for k, v in label_widgets_dict.items()}\n",
        "\n",
        "            # --- Plot Points ---\n",
        "            if show_points_widget.value:\n",
        "                if color_col != 'None' and color_col in analysis_data_init.columns:\n",
        "                    # Merge color data back if not in reg_df (reg_df might be aggregated)\n",
        "                    # If aggregated, we might lose the categorical info unless we merge back by ID\n",
        "                    # For simplicity, we try to use what's in df\n",
        "\n",
        "                    # Check if color_col exists in df, if not, try merge\n",
        "                    plot_df = df\n",
        "                    if color_col not in plot_df.columns:\n",
        "                        # Try to recover color info from initial data\n",
        "                        # This assumes 1-to-1 mapping if aggregated\n",
        "                        temp_merge = analysis_data_init[['id', color_col]].drop_duplicates()\n",
        "                        plot_df = plot_df.merge(temp_merge, on='id', how='left')\n",
        "\n",
        "                    # Get unique categories\n",
        "                    categories = plot_df[color_col].dropna().unique()\n",
        "                    cmap = plt.get_cmap('tab10')\n",
        "\n",
        "                    for i, cat in enumerate(categories):\n",
        "                        cat_str = str(cat)\n",
        "                        display_label = label_map.get(cat_str, cat_str)\n",
        "                        mask = plot_df[color_col] == cat\n",
        "\n",
        "                        ax.scatter(plot_df.loc[mask, mod_col], plot_df.loc[mask, eff_col],\n",
        "                                  color=cmap(i % 10), alpha=point_alpha_widget.value,\n",
        "                                  s=point_size_widget.value, label=display_label,\n",
        "                                  edgecolors='k', linewidth=0.5)\n",
        "\n",
        "                    # Legend title\n",
        "                    legend_title = label_map.get(color_col, color_col)\n",
        "\n",
        "                else:\n",
        "                    # Single color\n",
        "                    ax.scatter(df[mod_col], df[eff_col],\n",
        "                              color=point_color_widget.value, alpha=point_alpha_widget.value,\n",
        "                              s=point_size_widget.value, label='Observations',\n",
        "                              edgecolors='k', linewidth=0.5)\n",
        "                    legend_title = None\n",
        "\n",
        "            # --- Plot Curve ---\n",
        "            ax.plot(x_grid, y_pred, color=curve_color_widget.value,\n",
        "                   linewidth=curve_width_widget.value, label='Spline Fit')\n",
        "\n",
        "            if show_ci_widget.value:\n",
        "                ax.fill_between(x_grid, ci_lower, ci_upper,\n",
        "                               color=curve_color_widget.value, alpha=ci_alpha_widget.value,\n",
        "                               label=f'{ci_pct:.0f}% CI')\n",
        "\n",
        "            # --- Decoration ---\n",
        "            if show_null_line_widget.value:\n",
        "                ax.axhline(0, color='black', linestyle=':', linewidth=1.5, alpha=0.6)\n",
        "\n",
        "            if show_grid_widget.value:\n",
        "                ax.grid(True, linestyle=':', alpha=0.4)\n",
        "\n",
        "            if show_title_widget.value:\n",
        "                ax.set_title(title_widget.value, fontsize=14, fontweight='bold', pad=15)\n",
        "\n",
        "            ax.set_xlabel(xlabel_widget.value, fontsize=12, fontweight='bold')\n",
        "            ax.set_ylabel(ylabel_widget.value, fontsize=12, fontweight='bold')\n",
        "\n",
        "            if legend_loc_widget.value != 'none':\n",
        "                ax.legend(loc=legend_loc_widget.value, title=legend_title, frameon=True, fancybox=True)\n",
        "\n",
        "            # Stats annotation\n",
        "            if show_stats_widget.value:\n",
        "                # Try to get stats from results\n",
        "                p_val = res.get('f_pvalue', None)\n",
        "                tau2 = res.get('tau_sq', None)\n",
        "\n",
        "                stats_text = []\n",
        "                if p_val is not None:\n",
        "                    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
        "                    stats_text.append(f\"P-value: {p_val:.4g} {sig}\")\n",
        "                if tau2 is not None:\n",
        "                    stats_text.append(f\"τ²: {tau2:.3f}\")\n",
        "\n",
        "                if stats_text:\n",
        "                    ax.text(0.05, 0.95, \"\\n\".join(stats_text), transform=ax.transAxes,\n",
        "                           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # --- Export ---\n",
        "            ts = datetime.datetime.now().strftime(\"%H%M%S\")\n",
        "            fn = filename_prefix_widget.value\n",
        "\n",
        "            if save_pdf_widget.value:\n",
        "                plt.savefig(f\"{fn}_{ts}.pdf\", bbox_inches='tight')\n",
        "                print(f\"💾 Saved: {fn}_{ts}.pdf\")\n",
        "\n",
        "            if save_png_widget.value:\n",
        "                plt.savefig(f\"{fn}_{ts}.png\", dpi=png_dpi_widget.value, bbox_inches='tight')\n",
        "                print(f\"💾 Saved: {fn}_{ts}.png\")\n",
        "\n",
        "            plt.show()\n",
        "            print(f\"✅ Plot Generated (n={len(df)})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Plotting Error: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "run_plot_btn.on_click(generate_spline_plot)\n",
        "\n",
        "\n",
        "# Header\n",
        "header = widgets.HTML(\"\"\"\n",
        "    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "                padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n",
        "        <h2 style='color: white; margin: 0; text-align: center;'>\n",
        "            📊 Cell 11b: Publication-Ready Spline Plot\n",
        "        </h2>\n",
        "        <p style='color: rgba(255,255,255,0.9); margin: 5px 0 0 0; text-align: center; font-size: 14px;'>\n",
        "            Visualize spline analysis results with full customization\n",
        "        </p>\n",
        "    </div>\n",
        "\"\"\")\n",
        "\n",
        "# --- 4. DISPLAY ---\n",
        "display(widgets.VBox([\n",
        "    header,\n",
        "    tabs,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    run_plot_btn,\n",
        "    plot_output\n",
        "]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "V67vPDlUC9aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "TskkmGjojeOp"
      },
      "source": [
        "#@title 📉 Publication Bias Diagnostics\n",
        "\n",
        "# =============================================================================\n",
        "# CELL: PUBLICATION BIAS DIAGNOSTICS WITH DASHBOARD\n",
        "# Purpose: Test for publication bias using Egger's test and Trim-and-Fill\n",
        "# Enhancement: Tabbed interface for organized results and publication text\n",
        "# Note: Use Cells 12b and 14b for visualizations (funnel plots)\n",
        "# Variables preserved for plotting cells: funnel_results, trimfill_results\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import norm, t, rankdata\n",
        "import statsmodels.api as sm\n",
        "import datetime\n",
        "from scipy.optimize import minimize\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import warnings\n",
        "\n",
        "# --- 1. LAYOUT & WIDGETS ---\n",
        "tab_egger = widgets.Output()\n",
        "tab_trimfill = widgets.Output()\n",
        "tab_combined = widgets.Output()\n",
        "tab_publication = widgets.Output()\n",
        "\n",
        "tab_export = widgets.Output()\n",
        "tabs = widgets.Tab(children=[tab_egger, tab_trimfill, tab_combined, tab_publication, tab_export])\n",
        "\n",
        "tabs.set_title(0, '📊 Egger\\'s Test')\n",
        "tabs.set_title(1, '📉 Trim-and-Fill')\n",
        "tabs.set_title(2, '🔍 Combined Assessment')\n",
        "tabs.set_title(3, '📝 Publication Text')\n",
        "tabs.set_title(4, '💾 Export')\n",
        "\n",
        "# --- 2. HELPER FUNCTIONS ---\n",
        "\n",
        "\n",
        "def _run_robust_eggers_test(analysis_data, effect_col, var_col, se_col):\n",
        "    \"\"\"Runs Egger's Test using 3-Level Meta-Regression on SE.\"\"\"\n",
        "    grouped = analysis_data.groupby('id')\n",
        "    y_all, v_all, X_all = [], [], []\n",
        "\n",
        "    for _, group in grouped:\n",
        "        y_all.append(group[effect_col].values)\n",
        "        v_all.append(group[var_col].values)\n",
        "        X_i = sm.add_constant(group[se_col].values, prepend=True)\n",
        "        X_all.append(X_i)\n",
        "\n",
        "    N_total = len(analysis_data)\n",
        "    M_studies = len(y_all)\n",
        "    p_params = 2\n",
        "\n",
        "    # Optimization\n",
        "    best_res = None\n",
        "    best_fun = np.inf\n",
        "    start_points = [[0.1, 0.1], [1.0, 0.1], [5.0, 0.1]]\n",
        "\n",
        "    for start in start_points:\n",
        "        res = minimize(_neg_log_lik_reml_reg, x0=start, args=(y_all, v_all, X_all, N_total, M_studies, p_params),\n",
        "                       method='L-BFGS-B', bounds=[(1e-8, None), (1e-8, None)], options={'ftol': 1e-10})\n",
        "        if res.success and res.fun < best_fun:\n",
        "            best_fun = res.fun\n",
        "            best_res = res\n",
        "\n",
        "    if not best_res:\n",
        "        return None\n",
        "\n",
        "    final_res = minimize(_neg_log_lik_reml_reg, x0=best_res.x, args=(y_all, v_all, X_all, N_total, M_studies, p_params),\n",
        "                         method='Nelder-Mead', options={'xatol': 1e-10, 'fatol': 1e-10})\n",
        "\n",
        "    return _get_three_level_regression_estimates_v2(final_res.x, y_all, v_all, X_all, N_total, M_studies, p_params)\n",
        "\n",
        "def trimfill_analysis(data, effect_col, var_col, estimator='L0', side='auto', max_iter=100):\n",
        "    \"\"\"Duval & Tweedie Trim-and-Fill Method.\"\"\"\n",
        "    yi = data[effect_col].values\n",
        "    vi = data[var_col].values\n",
        "    ni = len(yi)\n",
        "\n",
        "    sort_indices = np.argsort(yi)\n",
        "    yi = yi[sort_indices]\n",
        "    vi = vi[sort_indices]\n",
        "\n",
        "    if side == 'auto':\n",
        "        wi = 1/vi\n",
        "        pooled_fe = np.sum(wi * yi) / np.sum(wi)\n",
        "        skew = np.sum(wi * (yi - pooled_fe)**3)\n",
        "        side = 'left' if skew > 0 else 'right'\n",
        "\n",
        "    k0 = 0\n",
        "    iter_safe = 0\n",
        "\n",
        "    while iter_safe < max_iter:\n",
        "        n_curr = ni - k0\n",
        "\n",
        "        if side == 'left':\n",
        "            yi_curr = yi[:n_curr]\n",
        "            vi_curr = vi[:n_curr]\n",
        "        else:\n",
        "            yi_curr = yi[k0:]\n",
        "            vi_curr = vi[k0:]\n",
        "\n",
        "        wi_curr = 1 / vi_curr\n",
        "        pooled_fe = np.sum(wi_curr * yi_curr) / np.sum(wi_curr)\n",
        "\n",
        "        residuals = yi - pooled_fe\n",
        "        signed_res = residuals if side == 'left' else -residuals\n",
        "        abs_res = np.abs(signed_res)\n",
        "        ranks = rankdata(abs_res, method='average')\n",
        "\n",
        "        pos_ranks = np.where(signed_res > 0, ranks, 0)\n",
        "        Sn = np.sum(pos_ranks)\n",
        "\n",
        "        k0_new = int(round((4 * Sn - ni * (ni + 1)) / (2 * ni - 1)))\n",
        "        k0_new = max(0, k0_new)\n",
        "\n",
        "        if k0_new == k0:\n",
        "            break\n",
        "\n",
        "        k0 = k0_new\n",
        "        k0 = min(k0, ni - 2)\n",
        "        iter_safe += 1\n",
        "\n",
        "    if k0 > 0:\n",
        "        if side == 'left':\n",
        "            idx_fill = slice(ni - k0, ni)\n",
        "        else:\n",
        "            idx_fill = slice(0, k0)\n",
        "\n",
        "        yi_excess = yi[idx_fill]\n",
        "        vi_excess = vi[idx_fill]\n",
        "\n",
        "        yi_filled = 2 * pooled_fe - yi_excess\n",
        "        vi_filled = vi_excess\n",
        "\n",
        "        yi_final = np.concatenate([yi, yi_filled])\n",
        "        vi_final = np.concatenate([vi, vi_filled])\n",
        "    else:\n",
        "        yi_final = yi\n",
        "        vi_final = vi\n",
        "        yi_filled = []\n",
        "        vi_filled = []\n",
        "\n",
        "    wi_final = 1 / vi_final\n",
        "    pooled_final = np.sum(wi_final * yi_final) / np.sum(wi_final)\n",
        "    var_final = 1 / np.sum(wi_final)\n",
        "    se_final = np.sqrt(var_final)\n",
        "\n",
        "    wi_orig = 1 / vi\n",
        "    pooled_orig = np.sum(wi_orig * yi) / np.sum(wi_orig)\n",
        "    se_orig = np.sqrt(1 / np.sum(wi_orig))\n",
        "\n",
        "# ---  Dynamic Inference ---\n",
        "    # 1. Get Settings\n",
        "    if 'ANALYSIS_CONFIG' in globals():\n",
        "        gs = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "        alpha_val = gs.get('alpha', 0.05)\n",
        "        dist_type = gs.get('dist_type', 'norm')\n",
        "    else:\n",
        "        alpha_val = 0.05\n",
        "        dist_type = 'norm'\n",
        "\n",
        "    q_val = 1 - (alpha_val / 2)\n",
        "\n",
        "    # 2. Calculate Original CI\n",
        "    k_orig = len(yi)\n",
        "    if dist_type == 't':\n",
        "        crit_val_orig = t.ppf(q_val, max(1, k_orig - 1))\n",
        "    else:\n",
        "        crit_val_orig = norm.ppf(q_val)\n",
        "\n",
        "    ci_lo_orig = pooled_orig - crit_val_orig * se_orig\n",
        "    ci_hi_orig = pooled_orig + crit_val_orig * se_orig\n",
        "\n",
        "    # 3. Calculate Filled CI (Different N if studies added)\n",
        "    k_final = len(yi_final)\n",
        "    if dist_type == 't':\n",
        "        crit_val_final = t.ppf(q_val, max(1, k_final - 1))\n",
        "    else:\n",
        "        crit_val_final = norm.ppf(q_val)\n",
        "\n",
        "    ci_lo_fill = pooled_final - crit_val_final * se_final\n",
        "    ci_hi_fill = pooled_final + crit_val_final * se_final\n",
        "    # ----------------------------------\n",
        "\n",
        "    return {\n",
        "        'k0': k0,\n",
        "        'side': side,\n",
        "        'pooled_original': pooled_orig,\n",
        "        'se_original': se_orig,\n",
        "        'ci_lower_original': ci_lo_orig,\n",
        "        'ci_upper_original': ci_hi_orig,\n",
        "        'pooled_filled': pooled_final,\n",
        "        'se_filled': se_final,\n",
        "        'ci_lower_filled': ci_lo_fill,\n",
        "        'ci_upper_filled': ci_hi_fill,\n",
        "        'yi_filled': yi_filled,\n",
        "        'vi_filled': vi_filled if k0 > 0 else [],\n",
        "        'yi_combined': yi_final,\n",
        "        'vi_combined': vi_final\n",
        "    }\n",
        "\n",
        "def generate_publication_bias_text(egger_result, tf_result, n_studies, ci_level=95):\n",
        "    \"\"\"Generate publication-ready text for publication bias assessment\"\"\"\n",
        "\n",
        "    egger_p = egger_result['p_value']\n",
        "    egger_int = egger_result['intercept']\n",
        "    egger_se = egger_result['se']\n",
        "\n",
        "    k0 = tf_result['k0']\n",
        "    side = tf_result['side']\n",
        "    orig_effect = tf_result['pooled_original']\n",
        "    adj_effect = tf_result['pooled_filled']\n",
        "\n",
        "    egger_sig = egger_p < 0.05\n",
        "    tf_bias = k0 > 0\n",
        "\n",
        "    # Significance text\n",
        "    egger_sig_text = \"significant\" if egger_sig else \"non-significant\"\n",
        "    p_format = f\"< 0.001\" if egger_p < 0.001 else f\"= {egger_p:.3f}\"\n",
        "\n",
        "    text = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff;'>\n",
        "\n",
        "<h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px;'>Publication Bias Assessment</h3>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "We assessed potential publication bias using two complementary methods: Egger's regression test for funnel plot asymmetry and the Duval and Tweedie trim-and-fill procedure.\n",
        "</p>\n",
        "\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Egger's Regression Test</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "Egger's regression test evaluates funnel plot asymmetry by regressing effect sizes on their standard errors, with the intercept testing for asymmetry. We employed a three-level meta-regression model to account for the nested structure of effect sizes within studies, providing robust estimates that accommodate within-study dependencies.\n",
        "</p>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "The Egger's test intercept was <b>{egger_sig_text}</b> (β₀ = {egger_int:.3f}, SE = {egger_se:.3f}, <i>p</i> {p_format}). \"\"\"\n",
        "\n",
        "    if egger_sig:\n",
        "        text += f\"\"\"This indicates <b>significant funnel plot asymmetry</b>, suggesting potential publication bias or other sources of small-study effects. The {'positive' if egger_int > 0 else 'negative'} intercept suggests that smaller studies tend to report {'larger' if egger_int > 0 else 'smaller'} effect sizes than larger studies.\n",
        "</p>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "However, it is important to note that funnel plot asymmetry can arise from sources other than publication bias, including genuine heterogeneity, chance, or differences in methodological quality between small and large studies. [<i>Consider discussing which alternative explanation(s) might apply to your meta-analysis.</i>]\n",
        "</p>\n",
        "\"\"\"\n",
        "    else:\n",
        "        text += f\"\"\"This suggests <b>no significant funnel plot asymmetry</b>, providing little evidence of publication bias based on this test. The symmetry of effect sizes across studies of different sizes supports the validity of the meta-analytic findings.\n",
        "</p>\n",
        "\"\"\"\n",
        "\n",
        "    # Trim-and-fill section\n",
        "    text += f\"\"\"\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Trim-and-Fill Analysis</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "The trim-and-fill method (Duval & Tweedie, 2000) provides a non-parametric approach to estimating the number of studies missing due to publication bias. The procedure iteratively trims the most extreme small studies from the {'positive' if side == 'right' else 'negative'} side of the funnel plot, re-computes the pooled effect, and then adds (fills) imputed mirror-image studies to restore funnel plot symmetry.\n",
        "</p>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "\"\"\"\n",
        "\n",
        "    if k0 == 0:\n",
        "        text += f\"\"\"The trim-and-fill procedure estimated <b>zero missing studies</b> (k₀ = 0), suggesting no asymmetry in the distribution of effect sizes. The pooled effect estimate remained unchanged at {orig_effect:.3f} ({ci_level:.0f} CI [{tf_result['ci_lower_original']:.3f}, {tf_result['ci_upper_original']:.3f}]).\n",
        "</p>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "This result is consistent with low risk of publication bias and suggests that the observed pooled effect size is robust to selective reporting.\n",
        "</p>\n",
        "\"\"\"\n",
        "    else:\n",
        "        pct_change = abs((adj_effect - orig_effect) / orig_effect) * 100 if orig_effect != 0 else 0\n",
        "        direction_change = \"decreased\" if adj_effect < orig_effect else \"increased\"\n",
        "\n",
        "        text += f\"\"\"The trim-and-fill procedure estimated <b>{k0} potentially missing studies</b> on the {side} side of the funnel plot. After imputing these missing studies, the adjusted pooled effect was {adj_effect:.3f} ({ci_level:.0f} CI [{tf_result['ci_lower_filled']:.3f}, {tf_result['ci_upper_filled']:.3f}]), compared to the original estimate of {orig_effect:.3f} (95% CI [{tf_result['ci_lower_original']:.3f}, {tf_result['ci_upper_original']:.3f}]).\n",
        "</p>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "The pooled effect {direction_change} by <b>{abs(adj_effect - orig_effect):.3f}</b> units ({pct_change:.1f}% relative change) after adjustment. \"\"\"\n",
        "\n",
        "        if pct_change > 20:\n",
        "            text += f\"\"\"This substantial change suggests that publication bias, if present, could have a meaningful impact on the meta-analytic conclusions. The adjusted estimate should be considered as a sensitivity analysis, though it should be noted that trim-and-fill can sometimes overestimate the number of missing studies.\n",
        "\"\"\"\n",
        "        elif pct_change > 10:\n",
        "            text += f\"\"\"This moderate change suggests some potential impact of publication bias on the pooled estimate, though the direction and significance of the effect remain {'' if adj_effect * orig_effect > 0 else 'un'}consistent between original and adjusted estimates.\n",
        "\"\"\"\n",
        "        else:\n",
        "            text += f\"\"\"This small change suggests that publication bias, if present, has minimal impact on the meta-analytic conclusions. The robustness of the pooled estimate to adjustment increases confidence in the findings.\n",
        "\"\"\"\n",
        "\n",
        "        text += \"</p>\"\n",
        "\n",
        "    # Combined interpretation\n",
        "    text += f\"\"\"\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Combined Interpretation</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "\"\"\"\n",
        "\n",
        "    if egger_sig and tf_bias:\n",
        "        text += f\"\"\"Both Egger's test and the trim-and-fill procedure suggest potential publication bias. Egger's test detected significant asymmetry (p {p_format}), and trim-and-fill estimated {k0} missing studies. This convergent evidence warrants cautious interpretation of the meta-analytic findings. \"\"\"\n",
        "        if k0 > 0:\n",
        "            pct_change = abs((adj_effect - orig_effect) / orig_effect) * 100 if orig_effect != 0 else 0\n",
        "            if pct_change > 10:\n",
        "                text += f\"\"\"Given the substantial adjustment to the pooled effect ({pct_change:.1f}% change), we recommend reporting both the original and adjusted estimates and considering the adjusted estimate in sensitivity analyses.\n",
        "\"\"\"\n",
        "            else:\n",
        "                text += f\"\"\"However, the modest change in the pooled effect ({pct_change:.1f}%) suggests the main conclusions are relatively robust to potential publication bias.\n",
        "\"\"\"\n",
        "    elif egger_sig or tf_bias:\n",
        "        which_test = \"Egger's test\" if egger_sig else \"trim-and-fill\"\n",
        "        text += f\"\"\"The evidence for publication bias is mixed. {which_test.capitalize()} suggests potential bias, but the other test does not. This inconsistency could reflect differences in what these tests detect (asymmetry vs. missing studies) or limited statistical power. We recommend interpreting results with appropriate caution and considering whether other factors (heterogeneity, methodological quality) might explain any observed asymmetry.\n",
        "\"\"\"\n",
        "    else:\n",
        "        text += f\"\"\"Neither Egger's test nor trim-and-fill provided evidence of publication bias. The non-significant Egger's intercept (p {p_format}) and absence of estimated missing studies (k₀ = 0) both suggest low risk of selective reporting. These results support the validity and robustness of the meta-analytic findings.\n",
        "\"\"\"\n",
        "\n",
        "    text += \"</p>\"\n",
        "\n",
        "    # Recommendations\n",
        "    text += f\"\"\"\n",
        "<h4 style='color: #34495e; margin-top: 25px;'>Recommendations</h4>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "Publication bias assessments should be interpreted in context with other factors:\n",
        "</p>\n",
        "\n",
        "<ul style='line-height: 2.0;'>\n",
        "<li><b>Sample size:</b> With k = {n_studies} studies, statistical power to detect publication bias is {'adequate' if n_studies >= 10 else 'limited' if n_studies >= 5 else 'very limited'}.</li>\n",
        "<li><b>Heterogeneity:</b> High heterogeneity can create asymmetry independent of publication bias.</li>\n",
        "<li><b>Study quality:</b> Smaller studies may differ systematically in design or quality.</li>\n",
        "<li><b>Registry searching:</b> {'Evidence from trial registries or grey literature could strengthen confidence in the absence of bias.' if not (egger_sig or tf_bias) else 'Searching trial registries and grey literature is recommended to identify potential unpublished studies.'}</li>\n",
        "</ul>\n",
        "\n",
        "<p style='text-align: justify;'>\n",
        "[<i>Add domain-specific discussion: Are there known reporting biases in this field? Were efforts made to locate unpublished data? Are funnel plots shown in the manuscript?</i>]\n",
        "</p>\n",
        "\n",
        "<hr style='margin: 30px 0; border: none; border-top: 1px solid #bdc3c7;'>\n",
        "\n",
        "<div style='background-color: #ecf0f1; padding: 20px; border-left: 4px solid #3498db; margin-top: 25px;'>\n",
        "<h4 style='margin-top: 0; color: #2c3e50;'>📊 Table 1. Publication Bias Assessment Summary</h4>\n",
        "<table style='width: 100%; border-collapse: collapse; margin-top: 15px; background-color: white;'>\n",
        "<thead style='background-color: #34495e; color: white;'>\n",
        "<tr>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: left;'>Test</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>Statistic</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>p-value</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: left;'>Interpretation</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr style='background-color: #f8f9fa;'>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>Egger's Test</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>β₀ = {egger_int:.3f} (SE = {egger_se:.3f})</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{\"<0.001\" if egger_p < 0.001 else f\"{egger_p:.3f}\"}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>{\"Significant asymmetry\" if egger_sig else \"No significant asymmetry\"}</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>Trim-and-Fill</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>k₀ = {k0} ({side} side)</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>—</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>{\"Missing studies detected\" if k0 > 0 else \"No missing studies\"}</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "</div>\n",
        "\n",
        "<div style='background-color: #ecf0f1; padding: 20px; border-left: 4px solid #3498db; margin-top: 15px;'>\n",
        "<h4 style='margin-top: 0; color: #2c3e50;'>📊 Table 2. Effect Size Estimates</h4>\n",
        "<table style='width: 100%; border-collapse: collapse; margin-top: 15px; background-color: white;'>\n",
        "<thead style='background-color: #34495e; color: white;'>\n",
        "<tr>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: left;'>Model</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>Pooled Effect</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>95% CI</th>\n",
        "<th style='border: 1px solid #bdc3c7; padding: 10px; text-align: center;'>Change</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr style='background-color: #f8f9fa;'>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>Original</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{orig_effect:.3f}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>[{tf_result['ci_lower_original']:.3f}, {tf_result['ci_upper_original']:.3f}]</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>—</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px;'>Trim-and-Fill Adjusted</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{adj_effect:.3f}</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>[{tf_result['ci_lower_filled']:.3f}, {tf_result['ci_upper_filled']:.3f}]</td>\n",
        "<td style='border: 1px solid #bdc3c7; padding: 8px; text-align: center;'>{adj_effect - orig_effect:+.3f}</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "<p style='margin-top: 10px; font-size: 0.9em; color: #6c757d;'><i>Note:</i> Negative change indicates adjusted effect is smaller than original.</p>\n",
        "</div>\n",
        "\n",
        "<hr style='margin: 30px 0; border: none; border-top: 1px solid #bdc3c7;'>\n",
        "\n",
        "<div style='background-color: #ecf0f1; padding: 15px; border-left: 4px solid #3498db; margin-top: 20px;'>\n",
        "<h4 style='margin-top: 0; color: #2c3e50;'>Interpretation Guidance:</h4>\n",
        "<ul style='margin-bottom: 0;'>\n",
        "<li>Customize interpretation based on your specific research domain and context</li>\n",
        "<li>Consider whether alternative explanations for asymmetry are plausible</li>\n",
        "<li>Discuss efforts to locate unpublished studies (registries, grey literature)</li>\n",
        "<li>Mention funnel plots if included in your manuscript (Cells 12b and 14b)</li>\n",
        "<li>If bias is detected, discuss impact on conclusions and consider sensitivity analyses</li>\n",
        "<li>Link to pre-registration or protocol if available</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n",
        "<div style='background-color: #fff3cd; padding: 10px; border-left: 4px solid #ffc107; margin-top: 15px;'>\n",
        "<p style='margin: 0;'><b>💡 Tip:</b> Select all text (Ctrl+A / Cmd+A), copy (Ctrl+C / Cmd+C), and paste into your word processor. Use Cells 12b and 14b to generate funnel plots for your manuscript figures.</p>\n",
        "</div>\n",
        "\n",
        "</div>\"\"\"\n",
        "\n",
        "    return text\n",
        "\n",
        "def generate_methods_text_pub_bias(es_config):\n",
        "    \"\"\"\n",
        "    Generates a 'Materials and Methods' section for Publication Bias Analysis.\n",
        "    \"\"\"\n",
        "    import sys\n",
        "\n",
        "    # 1. Extract Settings\n",
        "    es_label = es_config.get('effect_label', 'Effect Size')\n",
        "\n",
        "    # 2. Define Citation Database\n",
        "    db = {\n",
        "        'egger': \"Egger, M., Davey Smith, G., Schneider, M., & Minder, C. (1997). Bias in meta-analysis detected by a simple, graphical test. <i>BMJ</i>, 315(7109), 629-634.\",\n",
        "        'trimfill': \"Duval, S., & Tweedie, R. (2000). Trim and fill: A simple funnel-plot-based method of testing and adjusting for publication bias in meta-analysis. <i>Biometrics</i>, 56(2), 455-463.\",\n",
        "        'rothstein': \"Rothstein, H. R., Sutton, A. J., & Borenstein, M. (Eds.). (2005). <i>Publication Bias in Meta-Analysis: Prevention, Assessment and Adjustments</i>. Chichester, UK: John Wiley & Sons.\",\n",
        "        'tool': \"<b>[CITATION FOR THIS TOOL]:</b> Author, A. A. (202X). <i>Co-Meta: An Interactive Pipeline for Meta-Analysis</i>. [Repository/DOI].\"\n",
        "    }\n",
        "\n",
        "    # 3. Build HTML\n",
        "    html = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff; border: 1px solid #eee; margin-bottom: 20px;'>\n",
        "\n",
        "    <h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-top: 0;'>Materials and Methods</h3>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Publication Bias Assessment.</b> To assess the potential impact of publication bias (the \\\"file-drawer problem\\\"), we employed two complementary methods [1].\n",
        "    First, we used Egger's linear regression test to statistically evaluate funnel plot asymmetry [2].\n",
        "    This test regresses the standardized effect size on precision (1/SE), where a significant intercept indicates asymmetry.\n",
        "    </p>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Sensitivity Analysis.</b> Additionally, we applied the Trim-and-Fill method [3] to estimate the number of missing studies due to suppression of non-significant results.\n",
        "    This non-parametric method iteratively removes (trims) the most extreme small studies from the positive side of the funnel plot and re-computes the effect size, then adds (fills) imputed mirror-image studies to restore symmetry.\n",
        "    All analyses were conducted using the Co-Meta toolkit [4].\n",
        "    </p>\n",
        "\n",
        "    <h4 style='color: #34495e; margin-top: 20px; font-size: 11pt;'>References</h4>\n",
        "    <ol style='font-size: 10pt; color: #555;'>\n",
        "        <li>{db['rothstein']}</li>\n",
        "        <li>{db['egger']}</li>\n",
        "        <li>{db['trimfill']}</li>\n",
        "        <li>{db['tool']}</li>\n",
        "    </ol>\n",
        "    </div>\"\"\"\n",
        "\n",
        "    return html\n",
        "\n",
        "# --- 3. MAIN ANALYSIS FUNCTION ---\n",
        "def run_publication_bias_analysis():\n",
        "    \"\"\"Run both Egger's test and Trim-and-Fill\"\"\"\n",
        "\n",
        "    # Clear all tabs\n",
        "    for tab in [tab_egger, tab_trimfill, tab_combined, tab_publication]:\n",
        "        tab.clear_output()\n",
        "\n",
        "    # Check prerequisites\n",
        "    if 'ANALYSIS_CONFIG' not in globals():\n",
        "        with tab_egger:\n",
        "            display(HTML(\"<div style='color: red;'>❌ ANALYSIS_CONFIG not found. Run Step 1 first.</div>\"))\n",
        "        return\n",
        "\n",
        "    if 'three_level_results' not in ANALYSIS_CONFIG:\n",
        "        with tab_egger:\n",
        "            display(HTML(\"<div style='color: red;'>❌ Three-level results not found. Run Step 2 first.</div>\"))\n",
        "        return\n",
        "\n",
        "    effect_col = ANALYSIS_CONFIG.get('effect_col', 'hedges_g')\n",
        "    var_col = ANALYSIS_CONFIG.get('var_col', 'Vg')\n",
        "    se_col = ANALYSIS_CONFIG.get('se_col', 'SE_g')\n",
        "\n",
        "    if 'analysis_data' in ANALYSIS_CONFIG:\n",
        "        df_plot = ANALYSIS_CONFIG['analysis_data']\n",
        "    elif 'data_filtered' in globals():\n",
        "        df_plot = data_filtered\n",
        "    else:\n",
        "        with tab_egger:\n",
        "            display(HTML(\"<div style='color: red;'>❌ No analysis data found.</div>\"))\n",
        "        return\n",
        "\n",
        "    n_obs = len(df_plot)\n",
        "    n_studies = df_plot['id'].nunique()\n",
        "\n",
        "    # --- RUN EGGER'S TEST ---\n",
        "    egger_est = None\n",
        "    try:\n",
        "        egger_est = _run_robust_eggers_test(df_plot, effect_col, var_col, se_col)\n",
        "\n",
        "        if egger_est:\n",
        "            intercept = egger_est['betas'][0]\n",
        "            slope_val = egger_est['betas'][1] #added for validation\n",
        "            se_intercept = egger_est['se_betas'][0]\n",
        "            t_stat = intercept / se_intercept\n",
        "            df = egger_est.get('df', 100)\n",
        "            p_value = 2 * (1 - t.cdf(abs(t_stat), df))\n",
        "\n",
        "            # Save results (CRITICAL for plotting cell 12b)\n",
        "            ANALYSIS_CONFIG['funnel_results'] = {\n",
        "                'beta_slope': slope_val,\n",
        "                'timestamp': datetime.datetime.now(),\n",
        "                'intercept': intercept,\n",
        "                'se': se_intercept,\n",
        "                'p_value': p_value,\n",
        "                'estimates': egger_est\n",
        "            }\n",
        "\n",
        "            # --- TAB 1: EGGER'S TEST ---\n",
        "            with tab_egger:\n",
        "                sig = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
        "                color = \"#dc3545\" if p_value < 0.05 else \"#28a745\"\n",
        "\n",
        "                html = f\"\"\"\n",
        "                <div style='padding: 20px;'>\n",
        "                <h2 style='color: #2c3e50; margin-bottom: 20px;'>Egger's Regression Test for Funnel Plot Asymmetry</h2>\n",
        "\n",
        "                <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white; margin-bottom: 20px;'>\n",
        "                    <div style='text-align: center;'>\n",
        "                        <div style='font-size: 0.9em; margin-bottom: 10px;'>ASYMMETRY TEST</div>\n",
        "                        <h1 style='margin: 0; font-size: 2.5em;'>{\"Significant\" if p_value < 0.05 else \"Not Significant\"}</h1>\n",
        "                        <p style='margin: 10px 0 0 0; font-size: 1.2em;'>p {p_value:.4g} {sig}</p>\n",
        "                    </div>\n",
        "                </div>\n",
        "\n",
        "                <div style='display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-bottom: 20px;'>\n",
        "                    <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; border-left: 4px solid #007bff;'>\n",
        "                        <div style='color: #6c757d; font-size: 0.9em;'>Intercept (β₀)</div>\n",
        "                        <div style='font-size: 1.5em; font-weight: bold;'>{intercept:.4f}</div>\n",
        "                        <div style='color: #6c757d; font-size: 0.85em; margin-top: 5px;'>SE = {se_intercept:.4f}</div>\n",
        "                    </div>\n",
        "                    <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; border-left: 4px solid {color};'>\n",
        "                        <div style='color: #6c757d; font-size: 0.9em;'>P-value</div>\n",
        "                        <div style='font-size: 1.5em; font-weight: bold; color: {color};'>{p_value:.4g}</div>\n",
        "                        <div style='color: #6c757d; font-size: 0.85em; margin-top: 5px;'>t({df}) = {t_stat:.2f}</div>\n",
        "                    </div>\n",
        "                </div>\n",
        "\n",
        "                <div style='background-color: #e7f3ff; padding: 20px; border-radius: 5px; margin-bottom: 20px;'>\n",
        "                    <h4 style='margin-top: 0; color: #2c3e50;'>Interpretation</h4>\n",
        "                    <p style='margin: 0; font-size: 1.05em;'>\n",
        "                \"\"\"\n",
        "\n",
        "                if p_value < 0.05:\n",
        "                    html += f\"\"\"<b style='color: #dc3545;'>⚠️ Significant funnel plot asymmetry detected.</b><br>\n",
        "                        This suggests potential publication bias or other sources of small-study effects.\n",
        "                        Smaller studies tend to report {'larger' if intercept > 0 else 'smaller'} effect sizes than larger studies.\n",
        "                        <br><br>However, asymmetry can also arise from genuine heterogeneity, methodological differences, or chance.\"\"\"\n",
        "                elif p_value < 0.10:\n",
        "                    html += f\"\"\"<b style='color: #ffc107;'>⚡ Marginal evidence of asymmetry (p < 0.10).</b><br>\n",
        "                        Some suggestion of funnel plot asymmetry. Consider examining the funnel plot visually (Cell 12b).\"\"\"\n",
        "                else:\n",
        "                    html += f\"\"\"<b style='color: #28a745;'>✓ No significant funnel plot asymmetry.</b><br>\n",
        "                        Little evidence of publication bias based on Egger's test. The distribution of effect sizes appears symmetric across study sizes.\"\"\"\n",
        "\n",
        "                html += f\"\"\"\n",
        "                    </p>\n",
        "                </div>\n",
        "\n",
        "                <h3 style='color: #2c3e50; border-bottom: 2px solid #dee2e6; padding-bottom: 10px;'>Test Details</h3>\n",
        "                <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px;'>\n",
        "                    <p style='margin: 5px 0;'><b>Method:</b> Three-level meta-regression (robust to within-study dependencies)</p>\n",
        "                    <p style='margin: 5px 0;'><b>Predictor:</b> Standard error (SE)</p>\n",
        "                    <p style='margin: 5px 0;'><b>Outcome:</b> Effect size</p>\n",
        "                    <p style='margin: 5px 0;'><b>Sample:</b> {n_obs} observations from {n_studies} studies</p>\n",
        "                    <p style='margin: 5px 0;'><b>Degrees of Freedom:</b> {df}</p>\n",
        "                </div>\n",
        "\n",
        "                <div style='background-color: #fff3cd; padding: 15px; border-radius: 5px; border-left: 4px solid #ffc107; margin-top: 20px;'>\n",
        "                    <p style='margin: 0;'><b>📊 Next Step:</b> Use Cell 12b to generate the funnel plot for visual inspection of asymmetry.</p>\n",
        "                </div>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "\n",
        "                display(HTML(html))\n",
        "\n",
        "    except Exception as e:\n",
        "        with tab_egger:\n",
        "            display(HTML(f\"<div style='color: red;'>❌ Error running Egger's test: {e}</div>\"))\n",
        "\n",
        "    # --- RUN TRIM-AND-FILL ---\n",
        "    tf_res = None\n",
        "    try:\n",
        "        tf_res = trimfill_analysis(df_plot, effect_col, var_col, side='auto')\n",
        "\n",
        "        # Save results (CRITICAL for plotting cell 14b)\n",
        "        ANALYSIS_CONFIG['trimfill_results'] = tf_res\n",
        "\n",
        "        # --- TAB 2: TRIM-AND-FILL ---\n",
        "        with tab_trimfill:\n",
        "            k0 = tf_res['k0']\n",
        "            color = \"#dc3545\" if k0 > 0 else \"#28a745\"\n",
        "            pct_change = abs((tf_res['pooled_filled'] - tf_res['pooled_original']) / tf_res['pooled_original']) * 100 if tf_res['pooled_original'] != 0 else 0\n",
        "\n",
        "            html = f\"\"\"\n",
        "            <div style='padding: 20px;'>\n",
        "            <h2 style='color: #2c3e50; margin-bottom: 20px;'>Trim-and-Fill Analysis</h2>\n",
        "\n",
        "            <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white; margin-bottom: 20px;'>\n",
        "                <div style='text-align: center;'>\n",
        "                    <div style='font-size: 0.9em; margin-bottom: 10px;'>ESTIMATED MISSING STUDIES</div>\n",
        "                    <h1 style='margin: 0; font-size: 3em;'>{k0}</h1>\n",
        "                    <p style='margin: 10px 0 0 0; font-size: 1.2em;'>{tf_res['side'].capitalize()} side</p>\n",
        "                </div>\n",
        "            </div>\n",
        "\n",
        "            <div style='background-color: #e7f3ff; padding: 20px; border-radius: 5px; margin-bottom: 20px;'>\n",
        "                <h4 style='margin-top: 0; color: #2c3e50;'>Interpretation</h4>\n",
        "                <p style='margin: 0; font-size: 1.05em;'>\n",
        "            \"\"\"\n",
        "\n",
        "            if k0 == 0:\n",
        "                html += \"\"\"<b style='color: #28a745;'>✓ No missing studies detected.</b><br>\n",
        "                    The funnel plot appears symmetric. No evidence of publication bias via trim-and-fill.\"\"\"\n",
        "            else:\n",
        "                html += f\"\"\"<b style='color: #dc3545;'>⚠️ {k0} potentially missing studies estimated.</b><br>\n",
        "                    After imputation, the effect changes by {pct_change:.1f}%. This suggests {'substantial' if pct_change > 20 else 'moderate' if pct_change > 10 else 'minimal'} potential impact of publication bias.\"\"\"\n",
        "\n",
        "            html += f\"\"\"\n",
        "                </p>\n",
        "            </div>\n",
        "\n",
        "            <h3 style='color: #2c3e50; border-bottom: 2px solid #dee2e6; padding-bottom: 10px;'>Effect Size Comparison</h3>\n",
        "            <table style='width: 100%; border-collapse: collapse; margin: 20px 0;'>\n",
        "                <thead style='background-color: #2c3e50; color: white;'>\n",
        "                    <tr>\n",
        "                        <th style='padding: 12px; text-align: left; border: 1px solid #dee2e6;'>Estimate</th>\n",
        "                        <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>Effect</th>\n",
        "                        <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>SE</th>\n",
        "                        <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>95% CI</th>\n",
        "                    </tr>\n",
        "                </thead>\n",
        "                <tbody>\n",
        "                    <tr style='background-color: #f8f9fa;'>\n",
        "                        <td style='padding: 10px; border: 1px solid #dee2e6;'><b>Original</b></td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{tf_res['pooled_original']:.4f}</td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{tf_res['se_original']:.4f}</td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>[{tf_res['ci_lower_original']:.4f}, {tf_res['ci_upper_original']:.4f}]</td>\n",
        "                    </tr>\n",
        "                    <tr>\n",
        "                        <td style='padding: 10px; border: 1px solid #dee2e6;'><b>Adjusted</b></td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{tf_res['pooled_filled']:.4f}</td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{tf_res['se_filled']:.4f}</td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>[{tf_res['ci_lower_filled']:.4f}, {tf_res['ci_upper_filled']:.4f}]</td>\n",
        "                    </tr>\n",
        "                    <tr style='background-color: #fff3cd;'>\n",
        "                        <td style='padding: 10px; border: 1px solid #dee2e6;'><b>Change</b></td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6; font-weight: bold;'>{tf_res['pooled_filled'] - tf_res['pooled_original']:+.4f}</td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>—</td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>{pct_change:.1f}% change</td>\n",
        "                    </tr>\n",
        "                </tbody>\n",
        "            </table>\n",
        "\n",
        "            <h3 style='color: #2c3e50; border-bottom: 2px solid #dee2e6; padding-bottom: 10px;'>Method Details</h3>\n",
        "            <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px;'>\n",
        "                <p style='margin: 5px 0;'><b>Method:</b> Duval & Tweedie L0 estimator</p>\n",
        "                <p style='margin: 5px 0;'><b>Side:</b> {tf_res['side'].capitalize()} (automatically detected)</p>\n",
        "                <p style='margin: 5px 0;'><b>Original studies:</b> {n_obs} observations from {n_studies} studies</p>\n",
        "                <p style='margin: 5px 0;'><b>Imputed studies:</b> {k0}</p>\n",
        "                <p style='margin: 5px 0;'><b>Total after filling:</b> {n_obs + k0}</p>\n",
        "            </div>\n",
        "\n",
        "            <div style='background-color: #fff3cd; padding: 15px; border-radius: 5px; border-left: 4px solid #ffc107; margin-top: 20px;'>\n",
        "                <p style='margin: 0;'><b>📊 Next Step:</b> Use Cell 14b to visualize the trim-and-fill funnel plot showing original and imputed studies.</p>\n",
        "            </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "            display(HTML(html))\n",
        "\n",
        "    except Exception as e:\n",
        "        with tab_trimfill:\n",
        "            display(HTML(f\"<div style='color: red;'>❌ Error running Trim-and-Fill: {e}</div>\"))\n",
        "\n",
        "    # --- TAB 3: COMBINED ASSESSMENT ---\n",
        "    if egger_est and tf_res:\n",
        "        with tab_combined:\n",
        "            egger_p = ANALYSIS_CONFIG['funnel_results']['p_value']\n",
        "            k0 = tf_res['k0']\n",
        "\n",
        "            egger_bias = egger_p < 0.10\n",
        "            tf_bias = k0 > 0\n",
        "\n",
        "            # Determine overall assessment\n",
        "            if egger_bias and tf_bias:\n",
        "                assessment = \"HIGH RISK\"\n",
        "                color = \"#dc3545\"\n",
        "                icon = \"⚠️\"\n",
        "                message = \"Both tests suggest publication bias\"\n",
        "            elif egger_bias or tf_bias:\n",
        "                assessment = \"MODERATE RISK\"\n",
        "                color = \"#ffc107\"\n",
        "                icon = \"⚡\"\n",
        "                message = \"One test suggests publication bias\"\n",
        "            else:\n",
        "                assessment = \"LOW RISK\"\n",
        "                color = \"#28a745\"\n",
        "                icon = \"✓\"\n",
        "                message = \"Neither test suggests publication bias\"\n",
        "\n",
        "            html = f\"\"\"\n",
        "            <div style='padding: 20px;'>\n",
        "            <h2 style='color: #2c3e50; margin-bottom: 20px;'>Combined Publication Bias Assessment</h2>\n",
        "\n",
        "            <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white; margin-bottom: 20px;'>\n",
        "                <div style='text-align: center;'>\n",
        "                    <div style='font-size: 0.9em; margin-bottom: 10px;'>OVERALL ASSESSMENT</div>\n",
        "                    <h1 style='margin: 0; font-size: 2.5em;'>{icon} {assessment}</h1>\n",
        "                    <p style='margin: 10px 0 0 0; font-size: 1.2em;'>{message}</p>\n",
        "                </div>\n",
        "            </div>\n",
        "\n",
        "            <h3 style='color: #2c3e50; border-bottom: 2px solid #dee2e6; padding-bottom: 10px;'>Summary of Tests</h3>\n",
        "            <table style='width: 100%; border-collapse: collapse; margin: 20px 0;'>\n",
        "                <thead style='background-color: #2c3e50; color: white;'>\n",
        "                    <tr>\n",
        "                        <th style='padding: 12px; text-align: left; border: 1px solid #dee2e6;'>Test</th>\n",
        "                        <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>Result</th>\n",
        "                        <th style='padding: 12px; text-align: center; border: 1px solid #dee2e6;'>Evidence of Bias</th>\n",
        "                    </tr>\n",
        "                </thead>\n",
        "                <tbody>\n",
        "                    <tr style='background-color: #f8f9fa;'>\n",
        "                        <td style='padding: 10px; border: 1px solid #dee2e6;'><b>Egger's Test</b></td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>p = {egger_p:.4g}</td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>\n",
        "                            <span style='color: {\"#dc3545\" if egger_bias else \"#28a745\"}; font-weight: bold;'>\n",
        "                                {\"Yes\" if egger_bias else \"No\"}\n",
        "                            </span>\n",
        "                        </td>\n",
        "                    </tr>\n",
        "                    <tr>\n",
        "                        <td style='padding: 10px; border: 1px solid #dee2e6;'><b>Trim-and-Fill</b></td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>k₀ = {k0}</td>\n",
        "                        <td style='padding: 10px; text-align: center; border: 1px solid #dee2e6;'>\n",
        "                            <span style='color: {\"#dc3545\" if tf_bias else \"#28a745\"}; font-weight: bold;'>\n",
        "                                {\"Yes\" if tf_bias else \"No\"}\n",
        "                            </span>\n",
        "                        </td>\n",
        "                    </tr>\n",
        "                </tbody>\n",
        "            </table>\n",
        "\n",
        "            <h3 style='color: #2c3e50; border-bottom: 2px solid #dee2e6; padding-bottom: 10px;'>Detailed Interpretation</h3>\n",
        "            <div style='background-color: #e7f3ff; padding: 20px; border-radius: 5px; margin-bottom: 20px;'>\n",
        "            \"\"\"\n",
        "\n",
        "            if egger_bias and tf_bias:\n",
        "                html += f\"\"\"\n",
        "                <p style='margin: 5px 0;'><b>Convergent Evidence:</b> Both tests indicate potential publication bias.</p>\n",
        "                <ul style='margin: 10px 0;'>\n",
        "                    <li>Egger's test detected significant funnel plot asymmetry (p = {egger_p:.4g})</li>\n",
        "                    <li>Trim-and-fill estimated {k0} missing studies</li>\n",
        "                    <li>Effect size changed by {pct_change:.1f}% after adjustment</li>\n",
        "                </ul>\n",
        "                <p style='margin: 10px 0 0 0;'><b>Recommendation:</b> Interpret main results with caution. Consider reporting both original and adjusted estimates. Discuss potential impact of publication bias on conclusions.</p>\n",
        "                \"\"\"\n",
        "            elif egger_bias or tf_bias:\n",
        "                which = \"Egger's test\" if egger_bias else \"Trim-and-fill\"\n",
        "                which_not = \"Trim-and-fill\" if egger_bias else \"Egger's test\"\n",
        "                html += f\"\"\"\n",
        "                <p style='margin: 5px 0;'><b>Mixed Evidence:</b> {which} suggests bias, but {which_not} does not.</p>\n",
        "                <ul style='margin: 10px 0;'>\n",
        "                    <li>{'Funnel plot asymmetry detected' if egger_bias else 'No significant asymmetry'} (Egger p = {egger_p:.4g})</li>\n",
        "                    <li>{k0} missing studies estimated (Trim-and-fill)</li>\n",
        "                </ul>\n",
        "                <p style='margin: 10px 0 0 0;'><b>Recommendation:</b> Exercise appropriate caution. Differences between tests may reflect limited power or different aspects of bias. Visual inspection of funnel plots recommended.</p>\n",
        "                \"\"\"\n",
        "            else:\n",
        "                html += f\"\"\"\n",
        "                <p style='margin: 5px 0;'><b>Consistent Evidence:</b> Neither test provides evidence of publication bias.</p>\n",
        "                <ul style='margin: 10px 0;'>\n",
        "                    <li>No significant funnel plot asymmetry (Egger p = {egger_p:.4g})</li>\n",
        "                    <li>No missing studies estimated (k₀ = 0)</li>\n",
        "                </ul>\n",
        "                <p style='margin: 10px 0 0 0;'><b>Recommendation:</b> Main results appear robust to publication bias. Standard reporting is appropriate.</p>\n",
        "                \"\"\"\n",
        "\n",
        "            html += f\"\"\"\n",
        "            </div>\n",
        "\n",
        "            <h3 style='color: #2c3e50; border-bottom: 2px solid #dee2e6; padding-bottom: 10px;'>Contextual Factors</h3>\n",
        "            <div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px;'>\n",
        "                <p style='margin: 5px 0;'><b>Sample size:</b> k = {n_studies} studies {'(adequate power)' if n_studies >= 10 else '(limited power)' if n_studies >= 5 else '(very limited power)'}</p>\n",
        "                <p style='margin: 5px 0;'><i>Note: Publication bias tests have limited power with fewer than 10 studies</i></p>\n",
        "            </div>\n",
        "\n",
        "            <div style='background-color: #fff3cd; padding: 15px; border-radius: 5px; border-left: 4px solid #ffc107; margin-top: 20px;'>\n",
        "                <p style='margin: 0;'><b>📊 Visualization:</b> Use Cells 12b and 14b to create funnel plots for visual assessment and manuscript figures.</p>\n",
        "            </div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "            display(HTML(html))\n",
        "\n",
        "    # --- TAB 4: PUBLICATION TEXT ---\n",
        "        with tab_publication:\n",
        "            display(HTML(\"<h3 style='color: #2c3e50;'>📝 Publication-Ready Results Text</h3>\"))\n",
        "            display(HTML(\"<p style='color: #6c757d;'>Copy and paste this formatted text into your manuscript:</p>\"))\n",
        "\n",
        "            # A. Generate METHODS Text (New)\n",
        "            methods_html = generate_methods_text_pub_bias(ANALYSIS_CONFIG.get('es_config', {}))\n",
        "\n",
        "            # B. Generate RESULTS Text (Existing)\n",
        "            results_html = generate_publication_bias_text(\n",
        "                ANALYSIS_CONFIG['funnel_results'],\n",
        "                tf_res,\n",
        "                n_studies\n",
        "            )\n",
        "\n",
        "            # C. Display Both\n",
        "            display(HTML(methods_html))\n",
        "            display(HTML(results_html))\n",
        "\n",
        "            # D. Save Combined Text for Audit Report\n",
        "            ANALYSIS_CONFIG['bias_text'] = methods_html + \"<br><hr><br>\" + results_html\n",
        "\n",
        "\n",
        "\n",
        "# --- 4. RUN BUTTON ---\n",
        "run_button = widgets.Button(\n",
        "    description='▶ Run Publication Bias Analysis',\n",
        "    button_style='primary',\n",
        "    icon='play',\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "run_button.on_click(lambda b: run_publication_bias_analysis())\n",
        "\n",
        "# --- 5. DISPLAY UI ---\n",
        "display(HTML(\"<h3>📉 Publication Bias Diagnostics (V2)</h3>\"))\n",
        "display(HTML(\"<p style='color: #6c757d;'>Assess publication bias using Egger's test and Trim-and-Fill. Results appear in organized tabs below.</p>\"))\n",
        "display(run_button)\n",
        "display(tabs)\n",
        "\n",
        "with tab_export:\n",
        "    display(HTML(\"<h3 style='color: #2E86AB;'>💾 Download Audit Report</h3>\"))\n",
        "    display(HTML(\"<p>Export results of Egger's test and Trim-and-Fill analysis.</p>\"))\n",
        "\n",
        "    btn_pub_export = widgets.Button(\n",
        "        description=\"📥 Download Bias Report\",\n",
        "        button_style='info', icon='file-excel', layout=widgets.Layout(width='300px', height='40px')\n",
        "    )\n",
        "\n",
        "    def on_pub_export_click(b):\n",
        "        export_analysis_report(report_type='publication_bias', filename_prefix='Publication_Bias_Audit')\n",
        "\n",
        "    btn_pub_export.on_click(on_pub_export_click)\n",
        "    display(btn_pub_export)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pet_peese_16b"
      },
      "outputs": [],
      "source": [
        "#@title 📊 PET-PEESE Bias Correction\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 16b: PET-PEESE (Precision Effect Test - Precision Effect Estimate with SE)\n",
        "# Purpose: Correct for publication bias using conditional meta-regression\n",
        "# Logic:\n",
        "#   1. PET Model: Effect_Size ~ Standard_Error\n",
        "#   2. PEESE Model: Effect_Size ~ Variance (SE²)\n",
        "#   3. Decision: If PET p < 0.10 → use PEESE intercept, else use PET intercept\n",
        "# Dependencies: ANALYSIS_CONFIG, _run_three_level_reml_regression_v2\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import warnings\n",
        "\n",
        "# --- 1. LAYOUT & WIDGETS ---\n",
        "tab_summary = widgets.Output()\n",
        "tab_visuals = widgets.Output()\n",
        "tab_interpretation = widgets.Output()\n",
        "\n",
        "tabs = widgets.Tab(children=[tab_summary, tab_visuals, tab_interpretation])\n",
        "tabs.set_title(0, '📊 Summary')\n",
        "tabs.set_title(1, '📈 Visuals')\n",
        "tabs.set_title(2, '📝 Interpretation')\n",
        "\n",
        "run_button = widgets.Button(\n",
        "    description=\"▶ Run PET-PEESE\",\n",
        "    button_style='success',\n",
        "    icon='play',\n",
        "    layout=widgets.Layout(width='200px', height='40px')\n",
        ")\n",
        "\n",
        "# --- 2. GLOBAL STORAGE ---\n",
        "pet_peese_results = {}\n",
        "\n",
        "\n",
        "# --- 3. MAIN CALCULATION FUNCTION ---\n",
        "def run_pet_peese(b):\n",
        "    \"\"\"Execute PET-PEESE analysis with decision rule.\"\"\"\n",
        "    global pet_peese_results\n",
        "\n",
        "    # Clear all tabs\n",
        "    for tab in [tab_summary, tab_visuals, tab_interpretation]:\n",
        "        with tab:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "    # --- VALIDATION ---\n",
        "    try:\n",
        "        # Check for ANALYSIS_CONFIG\n",
        "        if 'ANALYSIS_CONFIG' not in globals():\n",
        "            raise NameError(\"ANALYSIS_CONFIG not found. Please run the analysis cells first.\")\n",
        "\n",
        "        # Get data\n",
        "        df = ANALYSIS_CONFIG['analysis_data'].copy()\n",
        "        effect_col = ANALYSIS_CONFIG['effect_col']\n",
        "        var_col = ANALYSIS_CONFIG['var_col']\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            raise ValueError(\"No analysis data found in ANALYSIS_CONFIG.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        with tab_summary:\n",
        "            display(HTML(f\"<div style='color: red; padding: 10px; border: 1px solid red; border-radius: 5px;'>\"\n",
        "                        f\"<b>⚠ Error:</b> {str(e)}</div>\"))\n",
        "        return\n",
        "\n",
        "    # --- DATA PREPARATION ---\n",
        "    try:\n",
        "        # Ensure SE and Var columns exist\n",
        "        if 'SE' not in df.columns:\n",
        "            if var_col in df.columns:\n",
        "                df['SE'] = np.sqrt(df[var_col])\n",
        "            else:\n",
        "                raise ValueError(f\"Cannot compute SE: '{var_col}' column not found.\")\n",
        "\n",
        "        if 'Var' not in df.columns:\n",
        "            if 'SE' in df.columns:\n",
        "                df['Var'] = df['SE'] ** 2\n",
        "            elif var_col in df.columns:\n",
        "                df['Var'] = df[var_col]\n",
        "            else:\n",
        "                raise ValueError(\"Cannot compute Variance.\")\n",
        "\n",
        "        # Remove missing values\n",
        "        df_clean = df[[effect_col, 'SE', 'Var', 'id']].dropna()\n",
        "\n",
        "        if len(df_clean) == 0:\n",
        "            raise ValueError(\"No valid data after removing missing values.\")\n",
        "\n",
        "        # Update the dataframe for regression\n",
        "        df_analysis = df_clean.copy()\n",
        "\n",
        "    except Exception as e:\n",
        "        with tab_summary:\n",
        "            display(HTML(f\"<div style='color: red; padding: 10px; border: 1px solid red; border-radius: 5px;'>\"\n",
        "                        f\"<b>⚠ Data Preparation Error:</b> {str(e)}</div>\"))\n",
        "        return\n",
        "\n",
        "    # --- RUN PET MODEL (Effect_Size ~ SE) ---\n",
        "    try:\n",
        "        with tab_summary:\n",
        "            display(HTML(\"<p style='color: #6c757d;'>⏳ Running PET model (Effect ~ SE)...</p>\"))\n",
        "\n",
        "        pet_estimates, pet_metadata, pet_opt = _run_three_level_reml_regression_v2(\n",
        "            df_analysis,\n",
        "            moderator_col='SE',\n",
        "            effect_col=effect_col,\n",
        "            var_col='Var'\n",
        "        )\n",
        "\n",
        "        if pet_estimates is None:\n",
        "            raise ValueError(\"PET model failed to converge.\")\n",
        "\n",
        "        pet_intercept = pet_estimates['betas'][0]\n",
        "        pet_slope = pet_estimates['betas'][1]\n",
        "\n",
        "        pet_intercept_se = pet_estimates['se_betas'][0]\n",
        "        pet_slope_se = pet_estimates['se_betas'][1]\n",
        "\n",
        "        pet_slope_p = pet_estimates['p_values'][1]\n",
        "        pet_intercept_ci = (pet_estimates['ci_lower'][0], pet_estimates['ci_upper'][0])\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        with tab_summary:\n",
        "            display(HTML(f\"<div style='color: red; padding: 10px; border: 1px solid red; border-radius: 5px;'>\"\n",
        "                        f\"<b>⚠ PET Model Error:</b> {str(e)}</div>\"))\n",
        "        return\n",
        "\n",
        "    # --- RUN PEESE MODEL (Effect_Size ~ Variance) ---\n",
        "    try:\n",
        "        with tab_summary:\n",
        "            display(HTML(\"<p style='color: #6c757d;'>⏳ Running PEESE model (Effect ~ Variance)...</p>\"))\n",
        "\n",
        "        peese_estimates, peese_metadata, peese_opt = _run_three_level_reml_regression_v2(\n",
        "            df_analysis,\n",
        "            moderator_col='Var',\n",
        "            effect_col=effect_col,\n",
        "            var_col='Var'\n",
        "        )\n",
        "\n",
        "        if peese_estimates is None:\n",
        "            raise ValueError(\"PEESE model failed to converge.\")\n",
        "\n",
        "        peese_intercept = peese_estimates['betas'][0]\n",
        "        peese_slope = peese_estimates['betas'][1]\n",
        "        peese_intercept_se = peese_estimates['se_betas'][0]\n",
        "        peese_slope_se = peese_estimates['se_betas'][1]\n",
        "        peese_slope_p = peese_estimates['p_values'][1]\n",
        "        peese_intercept_ci = (peese_estimates['ci_lower'][0], peese_estimates['ci_upper'][0])\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        with tab_summary:\n",
        "            display(HTML(f\"<div style='color: red; padding: 10px; border: 1px solid red; border-radius: 5px;'>\"\n",
        "                        f\"<b>⚠ PEESE Model Error:</b> {str(e)}</div>\"))\n",
        "        return\n",
        "\n",
        "    # --- DECISION RULE ---\n",
        "    p_threshold = 0.10\n",
        "    bias_detected = pet_slope_p < p_threshold\n",
        "\n",
        "    if bias_detected:\n",
        "        recommended_estimate = peese_intercept\n",
        "        recommended_se = peese_intercept_se\n",
        "        recommended_ci = peese_intercept_ci\n",
        "        recommended_model = \"PEESE\"\n",
        "        decision_rationale = (f\"The PET slope was statistically significant (p = {pet_slope_p:.4f} < 0.10), \"\n",
        "                             f\"indicating small-study effects/publication bias. The PEESE estimate is recommended.\")\n",
        "    else:\n",
        "        recommended_estimate = pet_intercept\n",
        "        recommended_se = pet_intercept_se\n",
        "        recommended_ci = pet_intercept_ci\n",
        "        recommended_model = \"PET\"\n",
        "        decision_rationale = (f\"The PET slope was not statistically significant (p = {pet_slope_p:.4f} ≥ 0.10), \"\n",
        "                             f\"suggesting no strong evidence of small-study effects. The PET estimate is recommended.\")\n",
        "\n",
        "    # Get original naive estimate\n",
        "    try:\n",
        "        naive_estimate = ANALYSIS_CONFIG['overall_results']['pooled_effect_random']\n",
        "        naive_se = ANALYSIS_CONFIG['overall_results']['pooled_SE_random_reported']\n",
        "        naive_ci_lower = ANALYSIS_CONFIG['overall_results']['ci_lower_random']\n",
        "        naive_ci_upper = ANALYSIS_CONFIG['overall_results']['ci_upper_random']\n",
        "    except:\n",
        "        naive_estimate = None\n",
        "        naive_se = None\n",
        "        naive_ci_lower = None\n",
        "        naive_ci_upper = None\n",
        "\n",
        "    # --- STORE RESULTS ---\n",
        "    pet_peese_results = {\n",
        "        'data': df_analysis,\n",
        "        'effect_col': effect_col,\n",
        "        'pet': {\n",
        "            'intercept': pet_intercept,\n",
        "            'intercept_se': pet_intercept_se,\n",
        "            'intercept_ci': pet_intercept_ci,\n",
        "            'slope': pet_slope,\n",
        "            'slope_se': pet_slope_se,\n",
        "            'slope_p': pet_slope_p,\n",
        "        },\n",
        "        'peese': {\n",
        "            'intercept': peese_intercept,\n",
        "            'intercept_se': peese_intercept_se,\n",
        "            'intercept_ci': peese_intercept_ci,\n",
        "            'slope': peese_slope,\n",
        "            'slope_se': peese_slope_se,\n",
        "            'slope_p': peese_slope_p,\n",
        "        },\n",
        "        'decision': {\n",
        "            'bias_detected': bias_detected,\n",
        "            'p_threshold': p_threshold,\n",
        "            'recommended_model': recommended_model,\n",
        "            'recommended_estimate': recommended_estimate,\n",
        "            'recommended_se': recommended_se,\n",
        "            'recommended_ci': recommended_ci,\n",
        "            'rationale': decision_rationale,\n",
        "        },\n",
        "        'naive': {\n",
        "            'estimate': naive_estimate,\n",
        "            'se': naive_se,\n",
        "            'ci_lower': naive_ci_lower,\n",
        "            'ci_upper': naive_ci_upper,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # --- RENDER TABS ---\n",
        "    render_summary_tab()\n",
        "    render_visuals_tab()\n",
        "    render_interpretation_tab()\n",
        "\n",
        "\n",
        "# --- 4. TAB RENDERING FUNCTIONS ---\n",
        "\n",
        "def render_summary_tab():\n",
        "    \"\"\"Render the Summary tab with comparison table.\"\"\"\n",
        "    with tab_summary:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        res = pet_peese_results\n",
        "\n",
        "        # Build comparison table\n",
        "        rows = []\n",
        "\n",
        "        # Naive estimate\n",
        "        if res['naive']['estimate'] is not None:\n",
        "            rows.append({\n",
        "                'Model': 'Original (Naive)',\n",
        "                'Estimate': f\"{res['naive']['estimate']:.4f}\",\n",
        "                'SE': f\"{res['naive']['se']:.4f}\" if res['naive']['se'] else \"—\",\n",
        "                '95% CI': f\"[{res['naive']['ci_lower']:.4f}, {res['naive']['ci_upper']:.4f}]\"\n",
        "                         if res['naive']['ci_lower'] is not None else \"—\",\n",
        "                'Note': 'Unadjusted pooled estimate'\n",
        "            })\n",
        "\n",
        "        # PET estimate\n",
        "        rows.append({\n",
        "            'Model': 'PET',\n",
        "            'Estimate': f\"{res['pet']['intercept']:.4f}\",\n",
        "            'SE': f\"{res['pet']['intercept_se']:.4f}\",\n",
        "            '95% CI': f\"[{res['pet']['intercept_ci'][0]:.4f}, {res['pet']['intercept_ci'][1]:.4f}]\",\n",
        "            'Note': f\"Slope p = {res['pet']['slope_p']:.4f}\"\n",
        "        })\n",
        "\n",
        "        # PEESE estimate\n",
        "        rows.append({\n",
        "            'Model': 'PEESE',\n",
        "            'Estimate': f\"{res['peese']['intercept']:.4f}\",\n",
        "            'SE': f\"{res['peese']['intercept_se']:.4f}\",\n",
        "            '95% CI': f\"[{res['peese']['intercept_ci'][0]:.4f}, {res['peese']['intercept_ci'][1]:.4f}]\",\n",
        "            'Note': f\"Slope p = {res['peese']['slope_p']:.4f}\"\n",
        "        })\n",
        "\n",
        "        df_table = pd.DataFrame(rows)\n",
        "\n",
        "        # HTML rendering\n",
        "        html = \"<h3 style='color: #2E86AB;'>📊 PET-PEESE Summary</h3>\"\n",
        "        html += \"<p style='color: #6c757d;'>Comparison of effect size estimates with and without publication bias correction.</p>\"\n",
        "\n",
        "        # Table\n",
        "        html += \"<table style='width: 100%; border-collapse: collapse; margin-top: 20px;'>\"\n",
        "        html += \"<thead><tr style='background-color: #f8f9fa; border-bottom: 2px solid #dee2e6;'>\"\n",
        "        for col in df_table.columns:\n",
        "            html += f\"<th style='padding: 12px; text-align: left; font-weight: bold;'>{col}</th>\"\n",
        "        html += \"</tr></thead><tbody>\"\n",
        "\n",
        "        for idx, row in df_table.iterrows():\n",
        "            is_recommended = row['Model'] == res['decision']['recommended_model']\n",
        "            bg_color = \"#d4edda\" if is_recommended else \"white\"\n",
        "            html += f\"<tr style='background-color: {bg_color}; border-bottom: 1px solid #dee2e6;'>\"\n",
        "            for col in df_table.columns:\n",
        "                html += f\"<td style='padding: 10px;'>{row[col]}</td>\"\n",
        "            html += \"</tr>\"\n",
        "\n",
        "        html += \"</tbody></table>\"\n",
        "\n",
        "        # Recommendation box\n",
        "        html += f\"\"\"\n",
        "        <div style='margin-top: 30px; padding: 20px; background-color: #d4edda;\n",
        "                    border-left: 5px solid #28a745; border-radius: 5px;'>\n",
        "            <h4 style='margin-top: 0; color: #155724;'>✓ Recommended Adjustment</h4>\n",
        "            <p style='font-size: 16px; margin: 10px 0;'>\n",
        "                <b>Model:</b> {res['decision']['recommended_model']}<br>\n",
        "                <b>Adjusted Estimate:</b> {res['decision']['recommended_estimate']:.4f}\n",
        "                (SE = {res['decision']['recommended_se']:.4f})<br>\n",
        "                <b>95% CI:</b> [{res['decision']['recommended_ci'][0]:.4f}, {res['decision']['recommended_ci'][1]:.4f}]\n",
        "            </p>\n",
        "            <p style='color: #6c757d; font-size: 14px; margin-top: 15px;'>\n",
        "                {res['decision']['rationale']}\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        display(HTML(html))\n",
        "\n",
        "\n",
        "def render_visuals_tab():\n",
        "    \"\"\"Render the Visuals tab with scatter plot and regression lines.\"\"\"\n",
        "    with tab_visuals:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        res = pet_peese_results\n",
        "        df = res['data']\n",
        "        effect_col = res['effect_col']\n",
        "\n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Scatter plot\n",
        "        ax.scatter(df['SE'], df[effect_col], alpha=0.6, s=80, color='steelblue',\n",
        "                   edgecolors='black', linewidth=0.5, label='Observed Studies')\n",
        "\n",
        "        # PET regression line (linear in SE)\n",
        "        se_range = np.linspace(df['SE'].min(), df['SE'].max(), 100)\n",
        "        pet_line = res['pet']['intercept'] + res['pet']['slope'] * se_range\n",
        "        ax.plot(se_range, pet_line, color='red', linewidth=2, linestyle='--',\n",
        "                label=f\"PET: y = {res['pet']['intercept']:.3f} + {res['pet']['slope']:.3f}·SE\")\n",
        "\n",
        "        # PEESE regression line (quadratic in SE, linear in Var)\n",
        "        # Effect = intercept + slope * Var = intercept + slope * SE²\n",
        "        peese_line = res['peese']['intercept'] + res['peese']['slope'] * (se_range ** 2)\n",
        "        ax.plot(se_range, peese_line, color='green', linewidth=2, linestyle='-',\n",
        "                label=f\"PEESE: y = {res['peese']['intercept']:.3f} + {res['peese']['slope']:.3f}·SE²\")\n",
        "\n",
        "        # Highlight recommended estimate\n",
        "        recommended_model = res['decision']['recommended_model']\n",
        "        if recommended_model == 'PET':\n",
        "            ax.axhline(y=res['pet']['intercept'], color='red', linestyle=':', linewidth=2,\n",
        "                      label=f\"Recommended (PET): {res['pet']['intercept']:.3f}\")\n",
        "        else:\n",
        "            ax.axhline(y=res['peese']['intercept'], color='green', linestyle=':', linewidth=2,\n",
        "                      label=f\"Recommended (PEESE): {res['peese']['intercept']:.3f}\")\n",
        "\n",
        "        # Labels and styling\n",
        "        ax.set_xlabel('Standard Error', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('Effect Size', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('PET-PEESE Analysis: Effect Size vs. Standard Error',\n",
        "                    fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.legend(loc='best', frameon=True, shadow=True, fontsize=10)\n",
        "        ax.grid(True, alpha=0.3, linestyle='--')\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Add interpretation note\n",
        "        html = \"\"\"\n",
        "        <div style='margin-top: 20px; padding: 15px; background-color: #f8f9fa;\n",
        "                    border-left: 4px solid #6c757d; border-radius: 5px;'>\n",
        "            <h4 style='margin-top: 0; color: #495057;'>📝 Plot Interpretation</h4>\n",
        "            <ul style='color: #6c757d; font-size: 14px;'>\n",
        "                <li><b>Red dashed line (PET):</b> Linear relationship between effect size and standard error.</li>\n",
        "                <li><b>Green solid line (PEESE):</b> Quadratic relationship (effect ~ SE²), appears curved when plotted against SE.</li>\n",
        "                <li><b>Horizontal dotted line:</b> The recommended bias-corrected estimate (intercept of chosen model).</li>\n",
        "                <li>If studies cluster above/below the zero-effect line asymmetrically, publication bias may be present.</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(html))\n",
        "\n",
        "\n",
        "def render_interpretation_tab():\n",
        "    \"\"\"Render the Interpretation tab with detailed explanation.\"\"\"\n",
        "    with tab_interpretation:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        res = pet_peese_results\n",
        "\n",
        "        # Build interpretation text\n",
        "        html = \"<h3 style='color: #2E86AB;'>📝 PET-PEESE Interpretation</h3>\"\n",
        "\n",
        "        # Decision summary\n",
        "        if res['decision']['bias_detected']:\n",
        "            decision_color = \"#856404\"\n",
        "            decision_bg = \"#fff3cd\"\n",
        "            decision_icon = \"⚠\"\n",
        "            decision_title = \"Publication Bias Detected\"\n",
        "        else:\n",
        "            decision_color = \"#155724\"\n",
        "            decision_bg = \"#d4edda\"\n",
        "            decision_icon = \"✓\"\n",
        "            decision_title = \"No Strong Evidence of Bias\"\n",
        "\n",
        "        html += f\"\"\"\n",
        "        <div style='margin-top: 20px; padding: 20px; background-color: {decision_bg};\n",
        "                    border-left: 5px solid {decision_color}; border-radius: 5px;'>\n",
        "            <h4 style='margin-top: 0; color: {decision_color};'>{decision_icon} {decision_title}</h4>\n",
        "            <p style='font-size: 14px; color: #6c757d;'>\n",
        "                {res['decision']['rationale']}\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        # Detailed explanation\n",
        "        html += \"\"\"\n",
        "        <div style='margin-top: 30px;'>\n",
        "            <h4 style='color: #495057;'>Understanding PET-PEESE</h4>\n",
        "            <p style='color: #6c757d; font-size: 14px; line-height: 1.6;'>\n",
        "                <b>PET-PEESE</b> is a conditional meta-regression method for correcting publication bias:\n",
        "            </p>\n",
        "            <ul style='color: #6c757d; font-size: 14px; line-height: 1.8;'>\n",
        "                <li><b>PET (Precision Effect Test):</b> Regresses effect size on standard error.\n",
        "                    A significant slope suggests that smaller studies report larger effects (publication bias).</li>\n",
        "                <li><b>PEESE (Precision Effect Estimate with SE):</b> Regresses effect size on variance (SE²).\n",
        "                    Provides a better-calibrated estimate when bias is detected.</li>\n",
        "                <li><b>Decision Rule:</b> If PET slope p < 0.10, use the PEESE intercept as the\n",
        "                    bias-corrected estimate. Otherwise, use the PET intercept.</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        # Model details\n",
        "        html += f\"\"\"\n",
        "        <div style='margin-top: 30px; padding: 15px; background-color: #f8f9fa; border-radius: 5px;'>\n",
        "            <h4 style='color: #495057;'>Model Details</h4>\n",
        "\n",
        "            <h5 style='color: #6c757d; margin-top: 20px;'>PET Model (Effect ~ SE)</h5>\n",
        "            <p style='color: #6c757d; font-size: 14px; margin-left: 20px;'>\n",
        "                <b>Intercept:</b> {res['pet']['intercept']:.4f} (SE = {res['pet']['intercept_se']:.4f})<br>\n",
        "                <b>Slope:</b> {res['pet']['slope']:.4f} (SE = {res['pet']['slope_se']:.4f}, p = {res['pet']['slope_p']:.4f})<br>\n",
        "                <b>95% CI for Intercept:</b> [{res['pet']['intercept_ci'][0]:.4f}, {res['pet']['intercept_ci'][1]:.4f}]\n",
        "            </p>\n",
        "\n",
        "            <h5 style='color: #6c757d; margin-top: 20px;'>PEESE Model (Effect ~ Variance)</h5>\n",
        "            <p style='color: #6c757d; font-size: 14px; margin-left: 20px;'>\n",
        "                <b>Intercept:</b> {res['peese']['intercept']:.4f} (SE = {res['peese']['intercept_se']:.4f})<br>\n",
        "                <b>Slope:</b> {res['peese']['slope']:.4f} (SE = {res['peese']['slope_se']:.4f}, p = {res['peese']['slope_p']:.4f})<br>\n",
        "                <b>95% CI for Intercept:</b> [{res['peese']['intercept_ci'][0]:.4f}, {res['peese']['intercept_ci'][1]:.4f}]\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        # Recommendations\n",
        "        html += \"\"\"\n",
        "        <div style='margin-top: 30px; padding: 15px; background-color: #e7f3ff;\n",
        "                    border-left: 4px solid #2E86AB; border-radius: 5px;'>\n",
        "            <h4 style='color: #2E86AB; margin-top: 0;'>💡 Recommendations</h4>\n",
        "            <ul style='color: #6c757d; font-size: 14px; line-height: 1.8;'>\n",
        "                <li>Report both the original pooled estimate and the PET-PEESE adjusted estimate.</li>\n",
        "                <li>Consider PET-PEESE as one of several bias assessment methods (alongside Egger's test, trim-and-fill, etc.).</li>\n",
        "                <li>If the adjusted estimate differs substantially from the original, investigate potential sources of bias.</li>\n",
        "                <li>PET-PEESE assumes bias is related to precision; other bias mechanisms may not be captured.</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        display(HTML(html))\n",
        "\n",
        "\n",
        "# --- 5. CONNECT BUTTON ---\n",
        "run_button.on_click(run_pet_peese)\n",
        "\n",
        "\n",
        "# --- 6. DISPLAY UI ---\n",
        "display(HTML(\"<h3>📊 PET-PEESE: Publication Bias Correction</h3>\"))\n",
        "display(HTML(\"<p style='color: #6c757d;'>Estimate the true effect size adjusted for small-study effects using conditional meta-regression.</p>\"))\n",
        "display(run_button)\n",
        "display(tabs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "v8QbYh-gC8A_"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 📊 Funnel Plot (ENHANCED)\n",
        "# =============================================================================\n",
        "# CELL 12b: PUBLICATION-READY FUNNEL PLOT\n",
        "# Purpose: Visualize publication bias with full customization\n",
        "# Enhanced: Complete GUI matching Forest/Orchard Plot functionality\n",
        "# Bug Fix: Added missing png_dpi_widget that was causing crashes\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import datetime\n",
        "import traceback\n",
        "\n",
        "# --- 1. INITIALIZATION ---\n",
        "print(\"=\"*70)\n",
        "print(\"FUNNEL PLOT CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "default_title = \"Funnel Plot\"\n",
        "default_xlabel = \"Effect Size\"\n",
        "default_ylabel = \"Standard Error\"\n",
        "\n",
        "try:\n",
        "    if 'ANALYSIS_CONFIG' in globals():\n",
        "        es_config = ANALYSIS_CONFIG.get('es_config', {})\n",
        "        default_xlabel = es_config.get('effect_label', 'Effect Size')\n",
        "        if 'funnel_results' in ANALYSIS_CONFIG:\n",
        "            default_title = \"Funnel Plot with Pseudo-95% CI\"\n",
        "    print(\"✓ Configuration loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️  Warning: {e}\")\n",
        "\n",
        "# --- 2. DEFINE CUSTOMIZATION WIDGETS ---\n",
        "\n",
        "# ========== TAB 1: STYLE & LAYOUT ==========\n",
        "style_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>🎨 Style & Layout</h3>\")\n",
        "\n",
        "# Dimensions Section\n",
        "width_widget = widgets.FloatSlider(\n",
        "    value=10.0, min=5.0, max=16.0, step=0.5,\n",
        "    description='Plot Width (in):',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "height_widget = widgets.FloatSlider(\n",
        "    value=7.0, min=4.0, max=14.0, step=0.5,\n",
        "    description='Plot Height (in):',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Typography Section\n",
        "title_fontsize_widget = widgets.IntSlider(\n",
        "    value=14, min=8, max=24, step=1,\n",
        "    description='Title Font Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "label_fontsize_widget = widgets.IntSlider(\n",
        "    value=12, min=8, max=18, step=1,\n",
        "    description='Axis Label Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "tick_fontsize_widget = widgets.IntSlider(\n",
        "    value=10, min=6, max=16, step=1,\n",
        "    description='Tick Label Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "stats_fontsize_widget = widgets.IntSlider(\n",
        "    value=10, min=6, max=14, step=1,\n",
        "    description='Stats Box Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Visual Style Section\n",
        "color_scheme_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Gray (Publication)', 'gray'),\n",
        "        ('Steel Blue', 'steelblue'),\n",
        "        ('Black', 'black'),\n",
        "        ('Viridis', 'viridis'),\n",
        "        ('Plasma', 'plasma'),\n",
        "        ('Red', 'red'),\n",
        "        ('Purple', 'purple')\n",
        "    ],\n",
        "    value='gray',\n",
        "    description='Point Color:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "style_tab = widgets.VBox([\n",
        "    style_header,\n",
        "    widgets.HTML(\"<b>Dimensions:</b>\"),\n",
        "    preset_widget,\n",
        "    width_widget,\n",
        "    height_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Typography:</b>\"),\n",
        "    title_fontsize_widget,\n",
        "    label_fontsize_widget,\n",
        "    tick_fontsize_widget,\n",
        "    stats_fontsize_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Color Scheme:</b>\"),\n",
        "    color_scheme_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 2: TEXT & LABELS ==========\n",
        "text_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>📝 Text & Labels</h3>\")\n",
        "\n",
        "show_title_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Plot Title',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "title_widget = widgets.Text(\n",
        "    value=default_title,\n",
        "    description='Plot Title:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "xlabel_widget = widgets.Text(\n",
        "    value=default_xlabel,\n",
        "    description='X-Axis Label:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "ylabel_widget = widgets.Text(\n",
        "    value=default_ylabel,\n",
        "    description='Y-Axis Label:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "show_ylabel_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Y-Axis Label',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "text_tab = widgets.VBox([\n",
        "    text_header,\n",
        "    widgets.HTML(\"<b>Title:</b>\"),\n",
        "    show_title_widget,\n",
        "    title_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Axis Labels:</b>\"),\n",
        "    xlabel_widget,\n",
        "    show_ylabel_widget,\n",
        "    ylabel_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 3: POINTS & DATA ==========\n",
        "points_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>⚫ Points & Data</h3>\")\n",
        "\n",
        "# Point Appearance\n",
        "point_size_widget = widgets.IntSlider(\n",
        "    value=40, min=10, max=200, step=5,\n",
        "    description='Point Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "point_alpha_widget = widgets.FloatSlider(\n",
        "    value=0.6, min=0.1, max=1.0, step=0.05,\n",
        "    description='Point Opacity:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "point_shape_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Circle (●)', 'o'),\n",
        "        ('Diamond (◆)', 'D'),\n",
        "        ('Square (■)', 's'),\n",
        "        ('Triangle Up (▲)', '^'),\n",
        "        ('Triangle Down (▼)', 'v')\n",
        "    ],\n",
        "    value='o',\n",
        "    description='Point Shape:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Point Edges\n",
        "point_edge_color_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Black', 'black'),\n",
        "        ('Gray', 'gray'),\n",
        "        ('White', 'white'),\n",
        "        ('None', 'none')\n",
        "    ],\n",
        "    value='black',\n",
        "    description='Edge Color:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "point_edge_width_widget = widgets.FloatSlider(\n",
        "    value=0.5, min=0.0, max=3.0, step=0.1,\n",
        "    description='Edge Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "points_tab = widgets.VBox([\n",
        "    points_header,\n",
        "    widgets.HTML(\"<b>Point Appearance:</b>\"),\n",
        "    point_size_widget,\n",
        "    point_alpha_widget,\n",
        "    point_shape_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Point Edges:</b>\"),\n",
        "    point_edge_color_widget,\n",
        "    point_edge_width_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 4: LINES & CONTOURS ==========\n",
        "lines_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>📐 Lines & Contours</h3>\")\n",
        "\n",
        "# Pooled Effect Line\n",
        "show_center_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Pooled Effect Line',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "center_color_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Red', 'red'),\n",
        "        ('Black', 'black'),\n",
        "        ('Blue', 'blue'),\n",
        "        ('Green', 'green')\n",
        "    ],\n",
        "    value='red',\n",
        "    description='Line Color:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "center_width_widget = widgets.FloatSlider(\n",
        "    value=2.0, min=0.5, max=5.0, step=0.5,\n",
        "    description='Line Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "center_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Solid', '-'),\n",
        "        ('Dashed', '--'),\n",
        "        ('Dotted', ':'),\n",
        "        ('Dash-Dot', '-.')\n",
        "    ],\n",
        "    value='-',\n",
        "    description='Line Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# CI Funnel\n",
        "show_ci_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show 95% CI Funnel',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "ci_fill_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Fill CI Region',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "ci_fill_alpha_widget = widgets.FloatSlider(\n",
        "    value=0.1, min=0.0, max=0.5, step=0.05,\n",
        "    description='Fill Opacity:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "ci_line_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Dashed', '--'),\n",
        "        ('Solid', '-'),\n",
        "        ('Dotted', ':')\n",
        "    ],\n",
        "    value='--',\n",
        "    description='CI Line Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "ci_line_width_widget = widgets.FloatSlider(\n",
        "    value=1.5, min=0.5, max=3.0, step=0.25,\n",
        "    description='CI Line Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Significance Contours\n",
        "show_contours_widget = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Show Significance Contours',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "contour_levels_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('p < 0.05 only', '0.05'),\n",
        "        ('p < 0.05 and 0.01', '0.05_0.01'),\n",
        "        ('p < 0.05, 0.01, 0.001', 'all')\n",
        "    ],\n",
        "    value='0.05_0.01',\n",
        "    description='Contour Levels:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "lines_tab = widgets.VBox([\n",
        "    lines_header,\n",
        "    widgets.HTML(\"<b>Pooled Effect Line:</b>\"),\n",
        "    show_center_widget,\n",
        "    center_color_widget,\n",
        "    center_width_widget,\n",
        "    center_style_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>95% CI Funnel:</b>\"),\n",
        "    show_ci_widget,\n",
        "    ci_fill_widget,\n",
        "    ci_fill_alpha_widget,\n",
        "    ci_line_style_widget,\n",
        "    ci_line_width_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Significance Contours:</b>\"),\n",
        "    show_contours_widget,\n",
        "    contour_levels_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 5: AXES & GRID ==========\n",
        "axes_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>📏 Axes & Grid</h3>\")\n",
        "\n",
        "# Axis Scaling\n",
        "auto_scale_x_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Auto-Scale X-Axis',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "x_min_widget = widgets.FloatText(\n",
        "    value=-2.0,\n",
        "    description='X-Min:',\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width='220px', visibility='hidden')\n",
        ")\n",
        "\n",
        "x_max_widget = widgets.FloatText(\n",
        "    value=2.0,\n",
        "    description='X-Max:',\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width='220px', visibility='hidden')\n",
        ")\n",
        "\n",
        "manual_x_box = widgets.HBox([x_min_widget, x_max_widget])\n",
        "\n",
        "def toggle_manual_x_scale(change):\n",
        "    if change['new']:\n",
        "        x_min_widget.layout.visibility = 'hidden'\n",
        "        x_max_widget.layout.visibility = 'hidden'\n",
        "    else:\n",
        "        x_min_widget.layout.visibility = 'visible'\n",
        "        x_max_widget.layout.visibility = 'visible'\n",
        "\n",
        "auto_scale_x_widget.observe(toggle_manual_x_scale, names='value')\n",
        "\n",
        "# Y-Axis Inversion\n",
        "invert_y_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Invert Y-Axis (Standard for Funnel Plots)',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Grid\n",
        "show_grid_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Grid',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "grid_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Dotted (Light)', 'dotted'),\n",
        "        ('Dashed (Light)', 'dashed'),\n",
        "        ('Solid (Light)', 'solid')\n",
        "    ],\n",
        "    value='dotted',\n",
        "    description='Grid Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "grid_alpha_widget = widgets.FloatSlider(\n",
        "    value=0.4, min=0.1, max=1.0, step=0.1,\n",
        "    description='Grid Opacity:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "axes_tab = widgets.VBox([\n",
        "    axes_header,\n",
        "    widgets.HTML(\"<b>X-Axis Scaling:</b>\"),\n",
        "    auto_scale_x_widget,\n",
        "    manual_x_box,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Y-Axis:</b>\"),\n",
        "    invert_y_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Grid:</b>\"),\n",
        "    show_grid_widget,\n",
        "    grid_style_widget,\n",
        "    grid_alpha_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 6: EXPORT & STATS ==========\n",
        "export_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>💾 Export & Statistics</h3>\")\n",
        "\n",
        "# Statistics Display\n",
        "show_stats_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description=\"Show Egger's Test Result\",\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "stats_position_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Bottom Right', 'bottom_right'),\n",
        "        ('Bottom Left', 'bottom_left'),\n",
        "        ('Top Right', 'top_right'),\n",
        "        ('Top Left', 'top_left')\n",
        "    ],\n",
        "    value='bottom_right',\n",
        "    description='Stats Position:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Legend\n",
        "legend_loc_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Best', 'best'),\n",
        "        ('Upper Right', 'upper right'),\n",
        "        ('Upper Left', 'upper left'),\n",
        "        ('Lower Right', 'lower right'),\n",
        "        ('Lower Left', 'lower left'),\n",
        "        ('None', 'none')\n",
        "    ],\n",
        "    value='upper right',\n",
        "    description='Legend Position:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "legend_frame_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Legend Frame',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Export Options\n",
        "save_pdf_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Save as PDF',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "save_png_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Save as PNG',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# BUG FIX: This widget was missing in the original code!\n",
        "png_dpi_widget = widgets.IntSlider(\n",
        "    value=300, min=150, max=600, step=50,\n",
        "    description='PNG DPI:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "filename_prefix_widget = widgets.Text(\n",
        "    value='Funnel_Plot',\n",
        "    description='Filename Prefix:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "transparent_bg_widget = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Transparent Background',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "include_timestamp_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Include Timestamp',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "export_tab = widgets.VBox([\n",
        "    export_header,\n",
        "    widgets.HTML(\"<b>Statistics Display:</b>\"),\n",
        "    show_stats_widget,\n",
        "    stats_position_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Legend:</b>\"),\n",
        "    legend_loc_widget,\n",
        "    legend_frame_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>File Formats:</b>\"),\n",
        "    save_pdf_widget,\n",
        "    save_png_widget,\n",
        "    png_dpi_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>File Settings:</b>\"),\n",
        "    filename_prefix_widget,\n",
        "    transparent_bg_widget,\n",
        "    include_timestamp_widget\n",
        "])\n",
        "\n",
        "# ========== CREATE TAB WIDGET ==========\n",
        "tab_children = [style_tab, text_tab, points_tab, lines_tab, axes_tab, export_tab]\n",
        "tabs = widgets.Tab(children=tab_children)\n",
        "tabs.set_title(0, '🎨 Style')\n",
        "tabs.set_title(1, '📝 Text')\n",
        "tabs.set_title(2, '⚫ Points')\n",
        "tabs.set_title(3, '📐 Lines')\n",
        "tabs.set_title(4, '📏 Axes')\n",
        "tabs.set_title(5, '💾 Export')\n",
        "\n",
        "print(\"\\n✓ GUI widgets created\")\n",
        "print(f\"✓ Tabs: 6 professional tabs\")\n",
        "print(f\"✓ Controls: 50+ customization options\")\n",
        "\n",
        "# --- 3. DEFINE PLOT GENERATION FUNCTION ---\n",
        "plot_output = widgets.Output()\n",
        "\n",
        "def generate_funnel_plot(b):\n",
        "    with plot_output:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"GENERATING FUNNEL PLOT\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        try:\n",
        "            # --- Get Global Settings ---\n",
        "            gs = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "            alpha = gs.get('alpha', 0.05)\n",
        "            dist_type = gs.get('dist_type', 'norm')\n",
        "            ci_pct = (1 - alpha) * 100\n",
        "\n",
        "            # --- GET WIDGET VALUES ---\n",
        "            plot_width = width_widget.value\n",
        "            plot_height = height_widget.value\n",
        "            title_fontsize = title_fontsize_widget.value\n",
        "            label_fontsize = label_fontsize_widget.value\n",
        "            tick_fontsize = tick_fontsize_widget.value\n",
        "            stats_fontsize = stats_fontsize_widget.value\n",
        "            point_color = color_scheme_widget.value\n",
        "\n",
        "            show_title = show_title_widget.value\n",
        "            graph_title = title_widget.value\n",
        "            x_label = xlabel_widget.value\n",
        "            show_ylabel = show_ylabel_widget.value\n",
        "            y_label = ylabel_widget.value\n",
        "\n",
        "            point_size = point_size_widget.value\n",
        "            point_alpha = point_alpha_widget.value\n",
        "            point_shape = point_shape_widget.value\n",
        "            point_edge_color = point_edge_color_widget.value\n",
        "            point_edge_width = point_edge_width_widget.value\n",
        "\n",
        "            show_center = show_center_widget.value\n",
        "            center_color = center_color_widget.value\n",
        "            center_width = center_width_widget.value\n",
        "            center_style = center_style_widget.value\n",
        "\n",
        "            show_ci = show_ci_widget.value\n",
        "            ci_fill = ci_fill_widget.value\n",
        "            ci_fill_alpha = ci_fill_alpha_widget.value\n",
        "            ci_line_style = ci_line_style_widget.value\n",
        "            ci_line_width = ci_line_width_widget.value\n",
        "\n",
        "            show_contours = show_contours_widget.value\n",
        "            contour_levels = contour_levels_widget.value\n",
        "\n",
        "            auto_scale_x = auto_scale_x_widget.value\n",
        "            x_min_manual = x_min_widget.value\n",
        "            x_max_manual = x_max_widget.value\n",
        "            invert_y = invert_y_widget.value\n",
        "\n",
        "            show_grid = show_grid_widget.value\n",
        "            grid_style = grid_style_widget.value\n",
        "            grid_alpha = grid_alpha_widget.value\n",
        "\n",
        "            show_stats = show_stats_widget.value\n",
        "            stats_position = stats_position_widget.value\n",
        "            legend_loc = legend_loc_widget.value\n",
        "            legend_frame = legend_frame_widget.value\n",
        "\n",
        "            save_pdf = save_pdf_widget.value\n",
        "            save_png = save_png_widget.value\n",
        "            png_dpi = png_dpi_widget.value\n",
        "            filename_prefix = filename_prefix_widget.value\n",
        "            transparent_bg = transparent_bg_widget.value\n",
        "            include_timestamp = include_timestamp_widget.value\n",
        "\n",
        "            print(f\"📊 Configuration:\")\n",
        "            print(f\"  Dimensions: {plot_width}\\\" × {plot_height}\\\"\")\n",
        "            print(f\"  Point color: {point_color}\")\n",
        "            print(f\"  Y-axis inverted: {invert_y}\")\n",
        "\n",
        "            # --- LOAD DATA ---\n",
        "            if 'ANALYSIS_CONFIG' not in globals():\n",
        "                print(\"❌ Error: ANALYSIS_CONFIG not found.\")\n",
        "                print(\"   Please run Cell 6 (Three-Level Analysis) first.\")\n",
        "                return\n",
        "\n",
        "            if 'analysis_data' in globals():\n",
        "                df = analysis_data.copy()\n",
        "            elif 'data_filtered' in globals():\n",
        "                df = data_filtered.copy()\n",
        "            else:\n",
        "                print(\"❌ Data not found.\")\n",
        "                return\n",
        "\n",
        "            if 'three_level_results' not in ANALYSIS_CONFIG:\n",
        "                print(\"❌ Error: Run Cell 6 (Three-Level Analysis) first.\")\n",
        "                return\n",
        "\n",
        "            eff_col = ANALYSIS_CONFIG.get('effect_col', 'hedges_g')\n",
        "            se_col = ANALYSIS_CONFIG.get('se_col', 'SE_g')\n",
        "\n",
        "            # Get Pooled Effect\n",
        "            pooled_effect = ANALYSIS_CONFIG['three_level_results']['pooled_effect']\n",
        "\n",
        "            # Clean Data\n",
        "            df = df.dropna(subset=[eff_col, se_col])\n",
        "            df = df[df[se_col] > 0]\n",
        "\n",
        "            print(f\"  Studies: {len(df)}\")\n",
        "            print(f\"  Pooled effect: {pooled_effect:.3f}\")\n",
        "\n",
        "            # --- CREATE FIGURE ---\n",
        "            fig, ax = plt.subplots(figsize=(plot_width, plot_height))\n",
        "\n",
        "            if transparent_bg:\n",
        "                fig.patch.set_alpha(0)\n",
        "                ax.patch.set_alpha(0)\n",
        "\n",
        "            # Max SE for Y-axis\n",
        "            max_se = df[se_col].max() * 1.1\n",
        "            y_range = np.linspace(0, max_se, 100)\n",
        "\n",
        "            print(f\"\\n🎨 Plotting...\")\n",
        "\n",
        "            # --- PLOT CI FUNNEL ---\n",
        "            if show_ci:\n",
        "                # --- UPDATED: Dynamic Critical Value ---\n",
        "                q_val = 1 - (alpha / 2)\n",
        "                if dist_type == 't':\n",
        "                    # Funnel plots typically use the overall DF\n",
        "                    df_funnel = ANALYSIS_CONFIG['overall_results']['k'] - 1\n",
        "                    crit_val = t.ppf(q_val, df_funnel)\n",
        "                else:\n",
        "                    crit_val = norm.ppf(q_val)\n",
        "                # ---------------------------------------\n",
        "\n",
        "                x_left = pooled_effect - crit_val * y_range\n",
        "                x_right = pooled_effect + crit_val * y_range\n",
        "\n",
        "                ax.plot(x_left, y_range, color='gray', linestyle=ci_line_style,\n",
        "                       linewidth=ci_line_width, alpha=0.7, label=f'{ci_pct:.0f}% CI')\n",
        "                ax.plot(x_right, y_range, color='gray', linestyle=ci_line_style,\n",
        "                       linewidth=ci_line_width, alpha=0.7)\n",
        "\n",
        "                if ci_fill:\n",
        "                    ax.fill_betweenx(y_range, x_left, x_right, color='lightgray',\n",
        "                                    alpha=funnel_fill_alpha, label=f'{ci_pct:.0f}% CI Region')\n",
        "\n",
        "            # --- PLOT SIGNIFICANCE CONTOURS ---\n",
        "            if show_contours:\n",
        "                if contour_levels in ['0.05_0.01', 'all']:\n",
        "                    # 99% CI (p < 0.01)\n",
        "                    x_99_left = pooled_effect - 2.58 * y_range\n",
        "                    x_99_right = pooled_effect + 2.58 * y_range\n",
        "                    ax.plot(x_99_left, y_range, color='gray', linestyle=':',\n",
        "                           linewidth=1, alpha=0.5)\n",
        "                    ax.plot(x_99_right, y_range, color='gray', linestyle=':',\n",
        "                           linewidth=1, alpha=0.5, label='99% CI (p<0.01)')\n",
        "\n",
        "                if contour_levels == 'all':\n",
        "                    # 99.9% CI (p < 0.001)\n",
        "                    x_999_left = pooled_effect - 3.29 * y_range\n",
        "                    x_999_right = pooled_effect + 3.29 * y_range\n",
        "                    ax.plot(x_999_left, y_range, color='gray', linestyle='-.',\n",
        "                           linewidth=0.8, alpha=0.4)\n",
        "                    ax.plot(x_999_right, y_range, color='gray', linestyle='-.',\n",
        "                           linewidth=0.8, alpha=0.4, label='99.9% CI (p<0.001)')\n",
        "\n",
        "            # --- PLOT POINTS ---\n",
        "            edge_col = None if point_edge_color == 'none' else point_edge_color\n",
        "            ax.scatter(df[eff_col], df[se_col],\n",
        "                      c=point_color,\n",
        "                      s=point_size,\n",
        "                      alpha=point_alpha,\n",
        "                      marker=point_shape,\n",
        "                      edgecolors=edge_col,\n",
        "                      linewidth=point_edge_width,\n",
        "                      label='Studies',\n",
        "                      zorder=3)\n",
        "\n",
        "            # --- PLOT CENTER LINE ---\n",
        "            if show_center:\n",
        "                ax.axvline(pooled_effect, color=center_color, linestyle=center_style,\n",
        "                          linewidth=center_width,\n",
        "                          label=f'Pooled Effect ({pooled_effect:.3f})',\n",
        "                          zorder=2)\n",
        "\n",
        "            # --- AXIS SETTINGS ---\n",
        "            if invert_y:\n",
        "                ax.set_ylim(max_se, 0)  # Inverted Y-axis\n",
        "            else:\n",
        "                ax.set_ylim(0, max_se)\n",
        "\n",
        "            if not auto_scale_x:\n",
        "                ax.set_xlim(x_min_manual, x_max_manual)\n",
        "\n",
        "            if show_title:\n",
        "                ax.set_title(graph_title, fontsize=title_fontsize, fontweight='bold', pad=15)\n",
        "\n",
        "            ax.set_xlabel(x_label, fontsize=label_fontsize, fontweight='bold')\n",
        "\n",
        "            if show_ylabel:\n",
        "                ax.set_ylabel(y_label, fontsize=label_fontsize, fontweight='bold')\n",
        "\n",
        "            ax.tick_params(labelsize=tick_fontsize)\n",
        "\n",
        "            if show_grid:\n",
        "                grid_ls = ':' if grid_style == 'dotted' else ('--' if grid_style == 'dashed' else '-')\n",
        "                ax.grid(True, linestyle=grid_ls, alpha=grid_alpha)\n",
        "\n",
        "            if legend_loc != 'none':\n",
        "                ax.legend(loc=legend_loc, frameon=legend_frame, fancybox=True,\n",
        "                         fontsize=tick_fontsize)\n",
        "\n",
        "            # --- SHOW EGGER'S TEST STATS ---\n",
        "            if show_stats and 'funnel_results' in ANALYSIS_CONFIG:\n",
        "                res_funnel = ANALYSIS_CONFIG['funnel_results']\n",
        "                if res_funnel.get('egger_p') is not None:\n",
        "                    p_val = res_funnel['egger_p']\n",
        "                    beta = res_funnel.get('beta_slope', 0)\n",
        "                    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
        "\n",
        "                    stats_text = f\"Egger's Test:\\nSlope = {beta:.3f}\\np = {p_val:.4f} {sig}\"\n",
        "\n",
        "                    # Determine position\n",
        "                    if stats_position == 'bottom_right':\n",
        "                        x_pos, y_pos, ha, va = 0.95, 0.05, 'right', 'bottom'\n",
        "                    elif stats_position == 'bottom_left':\n",
        "                        x_pos, y_pos, ha, va = 0.05, 0.05, 'left', 'bottom'\n",
        "                    elif stats_position == 'top_right':\n",
        "                        x_pos, y_pos, ha, va = 0.95, 0.95, 'right', 'top'\n",
        "                    else:  # top_left\n",
        "                        x_pos, y_pos, ha, va = 0.05, 0.95, 'left', 'top'\n",
        "\n",
        "                    ax.text(x_pos, y_pos, stats_text, transform=ax.transAxes,\n",
        "                           ha=ha, va=va, fontsize=stats_fontsize,\n",
        "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # --- SAVE FILES ---\n",
        "            print(f\"\\n💾 Saving files...\")\n",
        "\n",
        "            saved_files = []\n",
        "\n",
        "            if include_timestamp:\n",
        "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                base_filename = f\"{filename_prefix}_{timestamp}\"\n",
        "            else:\n",
        "                base_filename = filename_prefix\n",
        "\n",
        "            if save_pdf:\n",
        "                pdf_filename = f\"{base_filename}.pdf\"\n",
        "                fig.savefig(pdf_filename, bbox_inches='tight', transparent=transparent_bg)\n",
        "                saved_files.append(pdf_filename)\n",
        "                print(f\"  ✓ {pdf_filename}\")\n",
        "\n",
        "            if save_png:\n",
        "                png_filename = f\"{base_filename}.png\"\n",
        "                fig.savefig(png_filename, dpi=png_dpi, bbox_inches='tight',\n",
        "                           transparent=transparent_bg)\n",
        "                saved_files.append(png_filename)\n",
        "                print(f\"  ✓ {png_filename} (DPI: {png_dpi})\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            print(f\"\\n\" + \"=\"*70)\n",
        "            print(\"✅ FUNNEL PLOT COMPLETE\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"Files: {', '.join(saved_files)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ ERROR: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "# --- 4. CREATE BUTTON AND DISPLAY ---\n",
        "run_plot_btn = widgets.Button(\n",
        "    description='📊 Generate Funnel Plot',\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='450px', height='50px'),\n",
        "    style={'font_weight': 'bold'}\n",
        ")\n",
        "\n",
        "run_plot_btn.on_click(generate_funnel_plot)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ FUNNEL PLOT INTERFACE READY\")\n",
        "print(\"=\"*70)\n",
        "print(\"👆 Customize your plot using the tabs above, then click Generate\")\n",
        "print(\"\\n📝 Tips:\")\n",
        "print(\"  • 🐛 BUG FIXED: Added missing PNG DPI control!\")\n",
        "print(\"  • Standard funnel plots have inverted Y-axis (larger SE at top)\")\n",
        "print(\"  • 95% CI funnel shows expected distribution under no bias\")\n",
        "print(\"  • Egger's test detects funnel plot asymmetry\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='color: #2E86AB;'>📊 Funnel Plot Generator (Enhanced)</h3>\"),\n",
        "    widgets.HTML(\"<p style='color: #666;'>Create publication-ready funnel plots with full customization</p>\"),\n",
        "    widgets.HTML(\"<hr style='margin: 15px 0;'>\"),\n",
        "    tabs,\n",
        "    widgets.HTML(\"<hr style='margin: 15px 0;'>\"),\n",
        "    run_plot_btn,\n",
        "    plot_output\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "oY3uzFPrC8A_"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title 📊 Trim-and-Fill Plot (ENHANCED)\n",
        "# =============================================================================\n",
        "# CELL 14b: PUBLICATION-READY TRIM-AND-FILL PLOT\n",
        "# Purpose: Visualize publication bias sensitivity with full customization\n",
        "# Enhanced: Complete GUI matching Forest/Orchard/Funnel Plot functionality\n",
        "# Bug Fixes:\n",
        "#   - Fixed missing png_dpi_widget (was causing crashes)\n",
        "#   - Fixed tab title typo (\"zk Lines\" → \"📐 Lines\")\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import datetime\n",
        "import traceback\n",
        "\n",
        "# --- 1. INITIALIZATION ---\n",
        "print(\"=\"*70)\n",
        "print(\"TRIM-AND-FILL PLOT CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "default_title = \"Trim-and-Fill Funnel Plot\"\n",
        "default_xlabel = \"Effect Size\"\n",
        "default_ylabel = \"Standard Error\"\n",
        "\n",
        "try:\n",
        "    if 'ANALYSIS_CONFIG' in globals():\n",
        "        es_config = ANALYSIS_CONFIG.get('es_config', {})\n",
        "        default_xlabel = es_config.get('effect_label', 'Effect Size')\n",
        "    print(\"✓ Configuration loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️  Warning: {e}\")\n",
        "\n",
        "# --- 2. DEFINE CUSTOMIZATION WIDGETS ---\n",
        "\n",
        "# ========== TAB 1: STYLE & LAYOUT ==========\n",
        "style_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>🎨 Style & Layout</h3>\")\n",
        "\n",
        "# Dimensions Section\n",
        "width_widget = widgets.FloatSlider(\n",
        "    value=10.0, min=5.0, max=16.0, step=0.5,\n",
        "    description='Plot Width (in):',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "height_widget = widgets.FloatSlider(\n",
        "    value=7.0, min=4.0, max=14.0, step=0.5,\n",
        "    description='Plot Height (in):',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Typography Section\n",
        "title_fontsize_widget = widgets.IntSlider(\n",
        "    value=14, min=8, max=24, step=1,\n",
        "    description='Title Font Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "label_fontsize_widget = widgets.IntSlider(\n",
        "    value=12, min=8, max=18, step=1,\n",
        "    description='Axis Label Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "tick_fontsize_widget = widgets.IntSlider(\n",
        "    value=10, min=6, max=16, step=1,\n",
        "    description='Tick Label Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "legend_fontsize_widget = widgets.IntSlider(\n",
        "    value=10, min=6, max=14, step=1,\n",
        "    description='Legend Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "style_tab = widgets.VBox([\n",
        "    style_header,\n",
        "    widgets.HTML(\"<b>Dimensions:</b>\"),\n",
        "    preset_widget,\n",
        "    width_widget,\n",
        "    height_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Typography:</b>\"),\n",
        "    title_fontsize_widget,\n",
        "    label_fontsize_widget,\n",
        "    tick_fontsize_widget,\n",
        "    legend_fontsize_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 2: TEXT & LABELS ==========\n",
        "text_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>📝 Text & Labels</h3>\")\n",
        "\n",
        "show_title_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Plot Title',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "title_widget = widgets.Text(\n",
        "    value=default_title,\n",
        "    description='Plot Title:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "xlabel_widget = widgets.Text(\n",
        "    value=default_xlabel,\n",
        "    description='X-Axis Label:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "ylabel_widget = widgets.Text(\n",
        "    value=default_ylabel,\n",
        "    description='Y-Axis Label:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "show_ylabel_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Y-Axis Label',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "text_tab = widgets.VBox([\n",
        "    text_header,\n",
        "    widgets.HTML(\"<b>Title:</b>\"),\n",
        "    show_title_widget,\n",
        "    title_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Axis Labels:</b>\"),\n",
        "    xlabel_widget,\n",
        "    show_ylabel_widget,\n",
        "    ylabel_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 3: POINTS & DATA ==========\n",
        "points_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>⚫ Points & Data</h3>\")\n",
        "\n",
        "# Observed Studies\n",
        "obs_color_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Black', 'black'),\n",
        "        ('Gray', 'gray'),\n",
        "        ('Steel Blue', 'steelblue'),\n",
        "        ('Blue', 'blue'),\n",
        "        ('Dark Green', 'darkgreen')\n",
        "    ],\n",
        "    value='black',\n",
        "    description='Observed Color:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "obs_shape_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Circle (●)', 'o'),\n",
        "        ('Diamond (◆)', 'D'),\n",
        "        ('Square (■)', 's'),\n",
        "        ('Triangle Up (▲)', '^'),\n",
        "        ('Pentagon', 'p')\n",
        "    ],\n",
        "    value='o',\n",
        "    description='Observed Shape:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "obs_edge_width_widget = widgets.FloatSlider(\n",
        "    value=0.5, min=0.0, max=3.0, step=0.1,\n",
        "    description='Obs Edge Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Imputed Studies\n",
        "imp_color_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('White (Hollow)', 'white'),\n",
        "        ('Red', 'red'),\n",
        "        ('Orange', 'orange'),\n",
        "        ('Yellow', 'yellow'),\n",
        "        ('Light Gray', 'lightgray'),\n",
        "        ('None (Transparent)', 'none')\n",
        "    ],\n",
        "    value='white',\n",
        "    description='Imputed Color:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "imp_edge_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Red', 'red'),\n",
        "        ('Black', 'black'),\n",
        "        ('Orange', 'orange'),\n",
        "        ('Dark Red', 'darkred')\n",
        "    ],\n",
        "    value='red',\n",
        "    description='Imputed Edge:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "imp_edge_width_widget = widgets.FloatSlider(\n",
        "    value=1.5, min=0.5, max=4.0, step=0.25,\n",
        "    description='Imp Edge Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "imp_shape_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Circle (●)', 'o'),\n",
        "        ('Diamond (◆)', 'D'),\n",
        "        ('Square (■)', 's'),\n",
        "        ('Triangle Down (▼)', 'v'),\n",
        "        ('Star (★)', '*')\n",
        "    ],\n",
        "    value='o',\n",
        "    description='Imputed Shape:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Common Point Settings\n",
        "point_size_widget = widgets.IntSlider(\n",
        "    value=50, min=10, max=200, step=5,\n",
        "    description='Point Size:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "point_alpha_widget = widgets.FloatSlider(\n",
        "    value=0.7, min=0.1, max=1.0, step=0.05,\n",
        "    description='Point Opacity:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "points_tab = widgets.VBox([\n",
        "    points_header,\n",
        "    widgets.HTML(\"<b>Observed Studies:</b>\"),\n",
        "    obs_color_widget,\n",
        "    obs_shape_widget,\n",
        "    obs_edge_width_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Imputed Studies:</b>\"),\n",
        "    imp_color_widget,\n",
        "    imp_edge_widget,\n",
        "    imp_edge_width_widget,\n",
        "    imp_shape_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Common Settings:</b>\"),\n",
        "    point_size_widget,\n",
        "    point_alpha_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 4: LINES & FUNNEL ==========\n",
        "lines_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>📐 Lines & Funnel</h3>\")\n",
        "\n",
        "# Original Mean Line\n",
        "show_orig_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Original Mean Line',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "orig_color_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Black', 'black'),\n",
        "        ('Gray', 'gray'),\n",
        "        ('Blue', 'blue'),\n",
        "        ('Dark Blue', 'darkblue')\n",
        "    ],\n",
        "    value='black',\n",
        "    description='Orig Line Color:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "orig_width_widget = widgets.FloatSlider(\n",
        "    value=2.0, min=0.5, max=5.0, step=0.5,\n",
        "    description='Orig Line Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "orig_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Dashed', '--'),\n",
        "        ('Solid', '-'),\n",
        "        ('Dotted', ':'),\n",
        "        ('Dash-Dot', '-.')\n",
        "    ],\n",
        "    value='--',\n",
        "    description='Orig Line Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Adjusted Mean Line\n",
        "show_adj_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Adjusted Mean Line',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "adj_color_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Red', 'red'),\n",
        "        ('Orange', 'orange'),\n",
        "        ('Magenta', 'magenta'),\n",
        "        ('Dark Red', 'darkred')\n",
        "    ],\n",
        "    value='red',\n",
        "    description='Adj Line Color:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "adj_width_widget = widgets.FloatSlider(\n",
        "    value=2.0, min=0.5, max=5.0, step=0.5,\n",
        "    description='Adj Line Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "adj_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Solid', '-'),\n",
        "        ('Dashed', '--'),\n",
        "        ('Dotted', ':'),\n",
        "        ('Dash-Dot', '-.')\n",
        "    ],\n",
        "    value='-',\n",
        "    description='Adj Line Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Funnel Guidelines\n",
        "show_funnel_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show 95% CI Funnel Guidelines',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "funnel_fill_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Fill Funnel Region',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "funnel_fill_alpha_widget = widgets.FloatSlider(\n",
        "    value=0.1, min=0.0, max=0.5, step=0.05,\n",
        "    description='Fill Opacity:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "funnel_line_width_widget = widgets.FloatSlider(\n",
        "    value=1.0, min=0.5, max=3.0, step=0.25,\n",
        "    description='Funnel Line Width:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "lines_tab = widgets.VBox([\n",
        "    lines_header,\n",
        "    widgets.HTML(\"<b>Original Mean Line:</b>\"),\n",
        "    show_orig_widget,\n",
        "    orig_color_widget,\n",
        "    orig_width_widget,\n",
        "    orig_style_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Adjusted Mean Line:</b>\"),\n",
        "    show_adj_widget,\n",
        "    adj_color_widget,\n",
        "    adj_width_widget,\n",
        "    adj_style_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Funnel Guidelines:</b>\"),\n",
        "    show_funnel_widget,\n",
        "    funnel_fill_widget,\n",
        "    funnel_fill_alpha_widget,\n",
        "    funnel_line_width_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 5: AXES & GRID ==========\n",
        "axes_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>📏 Axes & Grid</h3>\")\n",
        "\n",
        "# Axis Scaling\n",
        "auto_scale_x_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Auto-Scale X-Axis',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "x_min_widget = widgets.FloatText(\n",
        "    value=-2.0,\n",
        "    description='X-Min:',\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width='220px', visibility='hidden')\n",
        ")\n",
        "\n",
        "x_max_widget = widgets.FloatText(\n",
        "    value=2.0,\n",
        "    description='X-Max:',\n",
        "    style={'description_width': '80px'},\n",
        "    layout=widgets.Layout(width='220px', visibility='hidden')\n",
        ")\n",
        "\n",
        "manual_x_box = widgets.HBox([x_min_widget, x_max_widget])\n",
        "\n",
        "def toggle_manual_x_scale(change):\n",
        "    if change['new']:\n",
        "        x_min_widget.layout.visibility = 'hidden'\n",
        "        x_max_widget.layout.visibility = 'hidden'\n",
        "    else:\n",
        "        x_min_widget.layout.visibility = 'visible'\n",
        "        x_max_widget.layout.visibility = 'visible'\n",
        "\n",
        "auto_scale_x_widget.observe(toggle_manual_x_scale, names='value')\n",
        "\n",
        "# Y-Axis Inversion\n",
        "invert_y_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Invert Y-Axis (Standard for Funnel Plots)',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Grid\n",
        "show_grid_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Show Grid',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "grid_style_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Dotted (Light)', 'dotted'),\n",
        "        ('Dashed (Light)', 'dashed'),\n",
        "        ('Solid (Light)', 'solid')\n",
        "    ],\n",
        "    value='dotted',\n",
        "    description='Grid Style:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "grid_alpha_widget = widgets.FloatSlider(\n",
        "    value=0.4, min=0.1, max=1.0, step=0.1,\n",
        "    description='Grid Opacity:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "axes_tab = widgets.VBox([\n",
        "    axes_header,\n",
        "    widgets.HTML(\"<b>X-Axis Scaling:</b>\"),\n",
        "    auto_scale_x_widget,\n",
        "    manual_x_box,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Y-Axis:</b>\"),\n",
        "    invert_y_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>Grid:</b>\"),\n",
        "    show_grid_widget,\n",
        "    grid_style_widget,\n",
        "    grid_alpha_widget\n",
        "])\n",
        "\n",
        "# ========== TAB 6: EXPORT & LEGEND ==========\n",
        "export_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>💾 Export & Legend</h3>\")\n",
        "\n",
        "# Legend\n",
        "legend_loc_widget = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Best', 'best'),\n",
        "        ('Upper Right', 'upper right'),\n",
        "        ('Upper Left', 'upper left'),\n",
        "        ('Lower Right', 'lower right'),\n",
        "        ('Lower Left', 'lower left'),\n",
        "        ('Center Right', 'center right'),\n",
        "        ('None', 'none')\n",
        "    ],\n",
        "    value='upper right',\n",
        "    description='Legend Position:',\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "legend_frame_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Legend Frame',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "legend_fancybox_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Fancy Box (Rounded Corners)',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# Export Options\n",
        "save_pdf_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Save as PDF',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "save_png_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Save as PNG',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "# BUG FIX: This widget was missing in the original code!\n",
        "png_dpi_widget = widgets.IntSlider(\n",
        "    value=300, min=150, max=600, step=50,\n",
        "    description='PNG DPI:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': '130px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "filename_prefix_widget = widgets.Text(\n",
        "    value='TrimFill_Plot',\n",
        "    description='Filename Prefix:',\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    style={'description_width': '130px'}\n",
        ")\n",
        "\n",
        "transparent_bg_widget = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Transparent Background',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "include_timestamp_widget = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Include Timestamp',\n",
        "    indent=False,\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "export_tab = widgets.VBox([\n",
        "    export_header,\n",
        "    widgets.HTML(\"<b>Legend:</b>\"),\n",
        "    legend_loc_widget,\n",
        "    legend_frame_widget,\n",
        "    legend_fancybox_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>File Formats:</b>\"),\n",
        "    save_pdf_widget,\n",
        "    save_png_widget,\n",
        "    png_dpi_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    widgets.HTML(\"<b>File Settings:</b>\"),\n",
        "    filename_prefix_widget,\n",
        "    transparent_bg_widget,\n",
        "    include_timestamp_widget\n",
        "])\n",
        "\n",
        "# ========== CREATE TAB WIDGET ==========\n",
        "tab_children = [style_tab, text_tab, points_tab, lines_tab, axes_tab, export_tab]\n",
        "tabs = widgets.Tab(children=tab_children)\n",
        "tabs.set_title(0, '🎨 Style')\n",
        "tabs.set_title(1, '📝 Text')\n",
        "tabs.set_title(2, '⚫ Points')\n",
        "tabs.set_title(3, '📐 Lines')  # BUG FIX: Was \"zk Lines\" (typo!)\n",
        "tabs.set_title(4, '📏 Axes')\n",
        "tabs.set_title(5, '💾 Export')\n",
        "\n",
        "print(\"\\n✓ GUI widgets created\")\n",
        "print(f\"✓ Tabs: 6 professional tabs\")\n",
        "print(f\"✓ Controls: 50+ customization options\")\n",
        "print(f\"✓ Bugs fixed: 2 (typo + missing DPI widget)\")\n",
        "\n",
        "# --- 3. DEFINE PLOT GENERATION FUNCTION ---\n",
        "plot_output = widgets.Output()\n",
        "\n",
        "def generate_tf_plot(b):\n",
        "    with plot_output:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"GENERATING TRIM-AND-FILL PLOT\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        try:\n",
        "            # --- GET WIDGET VALUES ---\n",
        "            plot_width = width_widget.value\n",
        "            plot_height = height_widget.value\n",
        "            title_fontsize = title_fontsize_widget.value\n",
        "            label_fontsize = label_fontsize_widget.value\n",
        "            tick_fontsize = tick_fontsize_widget.value\n",
        "            legend_fontsize = legend_fontsize_widget.value\n",
        "\n",
        "            show_title = show_title_widget.value\n",
        "            graph_title = title_widget.value\n",
        "            x_label = xlabel_widget.value\n",
        "            show_ylabel = show_ylabel_widget.value\n",
        "            y_label = ylabel_widget.value\n",
        "\n",
        "            obs_color = obs_color_widget.value\n",
        "            obs_shape = obs_shape_widget.value\n",
        "            obs_edge_width = obs_edge_width_widget.value\n",
        "\n",
        "            imp_color = imp_color_widget.value\n",
        "            imp_edge = imp_edge_widget.value\n",
        "            imp_edge_width = imp_edge_width_widget.value\n",
        "            imp_shape = imp_shape_widget.value\n",
        "\n",
        "            point_size = point_size_widget.value\n",
        "            point_alpha = point_alpha_widget.value\n",
        "\n",
        "            show_orig = show_orig_widget.value\n",
        "            orig_color = orig_color_widget.value\n",
        "            orig_width = orig_width_widget.value\n",
        "            orig_style = orig_style_widget.value\n",
        "\n",
        "            show_adj = show_adj_widget.value\n",
        "            adj_color = adj_color_widget.value\n",
        "            adj_width = adj_width_widget.value\n",
        "            adj_style = adj_style_widget.value\n",
        "\n",
        "            show_funnel = show_funnel_widget.value\n",
        "            funnel_fill = funnel_fill_widget.value\n",
        "            funnel_fill_alpha = funnel_fill_alpha_widget.value\n",
        "            funnel_line_width = funnel_line_width_widget.value\n",
        "\n",
        "            auto_scale_x = auto_scale_x_widget.value\n",
        "            x_min_manual = x_min_widget.value\n",
        "            x_max_manual = x_max_widget.value\n",
        "            invert_y = invert_y_widget.value\n",
        "\n",
        "            show_grid = show_grid_widget.value\n",
        "            grid_style = grid_style_widget.value\n",
        "            grid_alpha = grid_alpha_widget.value\n",
        "\n",
        "            legend_loc = legend_loc_widget.value\n",
        "            legend_frame = legend_frame_widget.value\n",
        "            legend_fancybox = legend_fancybox_widget.value\n",
        "\n",
        "            save_pdf = save_pdf_widget.value\n",
        "            save_png = save_png_widget.value\n",
        "            png_dpi = png_dpi_widget.value\n",
        "            filename_prefix = filename_prefix_widget.value\n",
        "            transparent_bg = transparent_bg_widget.value\n",
        "            include_timestamp = include_timestamp_widget.value\n",
        "\n",
        "            print(f\"📊 Configuration:\")\n",
        "            print(f\"  Dimensions: {plot_width}\\\" × {plot_height}\\\"\")\n",
        "            print(f\"  Observed: {obs_color}, Imputed: {imp_color}\")\n",
        "\n",
        "            # --- LOAD DATA ---\n",
        "            if 'ANALYSIS_CONFIG' not in globals() or 'trimfill_results' not in ANALYSIS_CONFIG:\n",
        "                print(\"❌ Error: Run Cell 16 (Publication Bias Diagnostics) first.\")\n",
        "                print(\"   This will compute Trim-and-Fill analysis.\")\n",
        "                return\n",
        "\n",
        "            tf_res = ANALYSIS_CONFIG['trimfill_results']\n",
        "\n",
        "            # Extract data\n",
        "            yi_all = tf_res['yi_combined']\n",
        "            vi_all = tf_res['vi_combined']\n",
        "            se_all = np.sqrt(vi_all)\n",
        "\n",
        "            k0 = tf_res['k0']\n",
        "            n_orig = len(yi_all) - k0\n",
        "\n",
        "            yi_orig = yi_all[:n_orig]\n",
        "            se_orig = se_all[:n_orig]\n",
        "\n",
        "            yi_fill = yi_all[n_orig:]\n",
        "            se_fill = se_all[n_orig:]\n",
        "\n",
        "            orig_mean = tf_res['pooled_original']\n",
        "            fill_mean = tf_res['pooled_filled']\n",
        "\n",
        "            print(f\"  Original studies: {n_orig}\")\n",
        "            print(f\"  Imputed studies: {k0}\")\n",
        "            print(f\"  Original mean: {orig_mean:.3f}\")\n",
        "            print(f\"  Adjusted mean: {fill_mean:.3f}\")\n",
        "\n",
        "            # --- CREATE FIGURE ---\n",
        "            fig, ax = plt.subplots(figsize=(plot_width, plot_height))\n",
        "\n",
        "            if transparent_bg:\n",
        "                fig.patch.set_alpha(0)\n",
        "                ax.patch.set_alpha(0)\n",
        "\n",
        "            # Max SE for Y-axis\n",
        "            max_se = np.max(se_all) * 1.1 if len(se_all) > 0 else 1.0\n",
        "            y_range = np.linspace(0, max_se, 100)\n",
        "\n",
        "            print(f\"\\n🎨 Plotting...\")\n",
        "\n",
        "            # --- FUNNEL GUIDELINES ---\n",
        "            if show_funnel:\n",
        "                x_left = fill_mean - 1.96 * y_range\n",
        "                x_right = fill_mean + 1.96 * y_range\n",
        "\n",
        "                ax.plot(x_left, y_range, color='gray', linestyle='--',\n",
        "                       linewidth=funnel_line_width, alpha=0.5)\n",
        "                ax.plot(x_right, y_range, color='gray', linestyle='--',\n",
        "                       linewidth=funnel_line_width, alpha=0.5)\n",
        "\n",
        "                if funnel_fill:\n",
        "                    ax.fill_betweenx(y_range, x_left, x_right, color='lightgray',\n",
        "                                    alpha=funnel_fill_alpha)\n",
        "\n",
        "            # --- PLOT OBSERVED STUDIES ---\n",
        "            ax.scatter(yi_orig, se_orig,\n",
        "                      c=obs_color,\n",
        "                      s=point_size,\n",
        "                      alpha=point_alpha,\n",
        "                      marker=obs_shape,\n",
        "                      edgecolors='black',\n",
        "                      linewidth=obs_edge_width,\n",
        "                      label='Observed Studies',\n",
        "                      zorder=3)\n",
        "\n",
        "            # --- PLOT IMPUTED STUDIES ---\n",
        "            if k0 > 0:\n",
        "                ax.scatter(yi_fill, se_fill,\n",
        "                          c=imp_color,\n",
        "                          s=point_size,\n",
        "                          alpha=point_alpha,\n",
        "                          marker=imp_shape,\n",
        "                          edgecolors=imp_edge,\n",
        "                          linewidth=imp_edge_width,\n",
        "                          label=f'Imputed Studies (k={k0})',\n",
        "                          zorder=3)\n",
        "\n",
        "            # --- PLOT MEAN LINES ---\n",
        "            if show_orig:\n",
        "                ax.axvline(orig_mean, color=orig_color, linestyle=orig_style,\n",
        "                          linewidth=orig_width,\n",
        "                          label=f'Original: {orig_mean:.3f}',\n",
        "                          zorder=2)\n",
        "\n",
        "            if show_adj:\n",
        "                ax.axvline(fill_mean, color=adj_color, linestyle=adj_style,\n",
        "                          linewidth=adj_width,\n",
        "                          label=f'Adjusted: {fill_mean:.3f}',\n",
        "                          zorder=2)\n",
        "\n",
        "            # --- AXIS SETTINGS ---\n",
        "            if invert_y:\n",
        "                ax.set_ylim(max_se, 0)  # Inverted Y-axis\n",
        "            else:\n",
        "                ax.set_ylim(0, max_se)\n",
        "\n",
        "            if not auto_scale_x:\n",
        "                ax.set_xlim(x_min_manual, x_max_manual)\n",
        "\n",
        "            if show_title:\n",
        "                ax.set_title(graph_title, fontsize=title_fontsize,\n",
        "                           fontweight='bold', pad=15)\n",
        "\n",
        "            ax.set_xlabel(x_label, fontsize=label_fontsize, fontweight='bold')\n",
        "\n",
        "            if show_ylabel:\n",
        "                ax.set_ylabel(y_label, fontsize=label_fontsize, fontweight='bold')\n",
        "\n",
        "            ax.tick_params(labelsize=tick_fontsize)\n",
        "\n",
        "            if show_grid:\n",
        "                grid_ls = ':' if grid_style == 'dotted' else ('--' if grid_style == 'dashed' else '-')\n",
        "                ax.grid(True, linestyle=grid_ls, alpha=grid_alpha)\n",
        "\n",
        "            if legend_loc != 'none':\n",
        "                ax.legend(loc=legend_loc, frameon=legend_frame,\n",
        "                         fancybox=legend_fancybox, fontsize=legend_fontsize)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # --- SAVE FILES ---\n",
        "            print(f\"\\n💾 Saving files...\")\n",
        "\n",
        "            saved_files = []\n",
        "\n",
        "            if include_timestamp:\n",
        "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                base_filename = f\"{filename_prefix}_{timestamp}\"\n",
        "            else:\n",
        "                base_filename = filename_prefix\n",
        "\n",
        "            if save_pdf:\n",
        "                pdf_filename = f\"{base_filename}.pdf\"\n",
        "                fig.savefig(pdf_filename, bbox_inches='tight', transparent=transparent_bg)\n",
        "                saved_files.append(pdf_filename)\n",
        "                print(f\"  ✓ {pdf_filename}\")\n",
        "\n",
        "            if save_png:\n",
        "                png_filename = f\"{base_filename}.png\"\n",
        "                fig.savefig(png_filename, dpi=png_dpi, bbox_inches='tight',\n",
        "                           transparent=transparent_bg)\n",
        "                saved_files.append(png_filename)\n",
        "                print(f\"  ✓ {png_filename} (DPI: {png_dpi})\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            print(f\"\\n\" + \"=\"*70)\n",
        "            print(\"✅ TRIM-AND-FILL PLOT COMPLETE\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"Files: {', '.join(saved_files)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ ERROR: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "# --- 4. CREATE BUTTON AND DISPLAY ---\n",
        "run_plot_btn = widgets.Button(\n",
        "    description='📊 Generate Trim-and-Fill Plot',\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='450px', height='50px'),\n",
        "    style={'font_weight': 'bold'}\n",
        ")\n",
        "\n",
        "run_plot_btn.on_click(generate_tf_plot)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ TRIM-AND-FILL PLOT INTERFACE READY\")\n",
        "print(\"=\"*70)\n",
        "print(\"👆 Customize your plot using the tabs above, then click Generate\")\n",
        "print(\"\\n📝 Tips:\")\n",
        "print(\"  • 🐛 BUGS FIXED: Tab typo + missing PNG DPI control!\")\n",
        "print(\"  • Imputed studies show where missing studies might be\")\n",
        "print(\"  • Adjusted mean accounts for potential publication bias\")\n",
        "print(\"  • White/hollow imputed points are standard in literature\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='color: #2E86AB;'>📊 Trim-and-Fill Plot Generator (Enhanced)</h3>\"),\n",
        "    widgets.HTML(\"<p style='color: #666;'>Visualize publication bias sensitivity with full customization</p>\"),\n",
        "    widgets.HTML(\"<hr style='margin: 15px 0;'>\"),\n",
        "    tabs,\n",
        "    widgets.HTML(\"<hr style='margin: 15px 0;'>\"),\n",
        "    run_plot_btn,\n",
        "    plot_output\n",
        "]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔄 Leave-One-Out Sensitivity - Execution\n",
        "# =============================================================================\n",
        "# CELL 13: ROBUST LEAVE-ONE-OUT ANALYSIS (Math Only)\n",
        "# Purpose: Calculate influence of each study on the 3-level pooled effect.\n",
        "# Note: Plots have been moved to Cell 13b.\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import norm\n",
        "import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import warnings\n",
        "\n",
        "# --- 1. ROBUST ENGINE (Same as Cell 6.5) ---\n",
        "def _run_three_level_reml_loo(analysis_data, effect_col, var_col):\n",
        "    \"\"\"Optimization with Two-Pass High Precision Strategy.\"\"\"\n",
        "    grouped = analysis_data.groupby('id')\n",
        "    y_all = [group[effect_col].values for _, group in grouped]\n",
        "    v_all = [group[var_col].values for _, group in grouped]\n",
        "    N_total = len(analysis_data)\n",
        "    M_studies = len(y_all)\n",
        "\n",
        "    if M_studies < 2: return None\n",
        "\n",
        "    # 1. Global Search (L-BFGS-B)\n",
        "    start_points = [[0.01, 0.01], [0.5, 0.1], [0.1, 0.5]]\n",
        "    best_res = None\n",
        "    best_fun = np.inf\n",
        "\n",
        "    for start in start_points:\n",
        "        res = minimize(\n",
        "            _neg_log_lik_reml_loo, x0=start,\n",
        "            args=(y_all, v_all, N_total, M_studies),\n",
        "            method='L-BFGS-B', bounds=[(1e-8, None), (1e-8, None)],\n",
        "            options={'ftol': 1e-10}\n",
        "        )\n",
        "        if res.success and res.fun < best_fun:\n",
        "            best_fun = res.fun\n",
        "            best_res = res\n",
        "\n",
        "    if not best_res: return None\n",
        "\n",
        "    # 2. Polishing (Nelder-Mead)\n",
        "    final_res = minimize(\n",
        "        _neg_log_lik_reml_loo, x0=best_res.x,\n",
        "        args=(y_all, v_all, N_total, M_studies),\n",
        "        method='Nelder-Mead', bounds=[(1e-8, None), (1e-8, None)],\n",
        "        options={'xatol': 1e-10, 'fatol': 1e-10}\n",
        "    )\n",
        "\n",
        "    return _get_three_level_estimates_loo(\n",
        "        final_res.x, y_all, v_all, N_total, M_studies\n",
        "    )\n",
        "\n",
        "# --- 2. WIDGETS ---\n",
        "header = widgets.HTML(\n",
        "    \"<h3 style='color: #2E86AB;'>Three-Level Leave-One-Out Sensitivity Analysis</h3>\"\n",
        "    \"<p style='color: #666;'><i>Calculates the influence of each study. (Math Only - Plotting in Cell 13b)</i></p>\"\n",
        "    \"<p style='color: red;'>⚠️ This is computationally intensive.</p>\"\n",
        ")\n",
        "\n",
        "run_loo_btn = widgets.Button(description='▶ Run LOO Calculation', button_style='success', layout=widgets.Layout(width='400px'))\n",
        "loo_output = widgets.Output()\n",
        "\n",
        "# --- 3. MAIN LOGIC ---\n",
        "def run_loo_analysis(b):\n",
        "    global ANALYSIS_CONFIG\n",
        "    # --- Load Global Settings ---\n",
        "    global_settings = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "    alpha = global_settings.get('alpha', 0.05)\n",
        "    dist_type = global_settings.get('dist_type', 't')\n",
        "\n",
        "    with loo_output:\n",
        "        clear_output(wait=True)\n",
        "        print(\"=\"*70)\n",
        "        print(\"RUNNING HIGH-PRECISION LEAVE-ONE-OUT ANALYSIS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        try:\n",
        "            # Load Data\n",
        "            if 'analysis_data' in globals(): df_loo = analysis_data.copy()\n",
        "            elif 'data_filtered' in globals(): df_loo = data_filtered.copy()\n",
        "            else: print(\"❌ Data not found.\"); return\n",
        "\n",
        "            if 'three_level_results' not in ANALYSIS_CONFIG:\n",
        "                print(\"❌ Run Cell 6.5 first.\")\n",
        "                return\n",
        "\n",
        "            # Get Config\n",
        "            effect_col = ANALYSIS_CONFIG.get('effect_col', 'hedges_g')\n",
        "            var_col = ANALYSIS_CONFIG.get('var_col', 'Vg')\n",
        "            es_config = ANALYSIS_CONFIG.get('es_config', {})\n",
        "            orig_res = ANALYSIS_CONFIG['three_level_results']\n",
        "            orig_eff = orig_res['pooled_effect']\n",
        "            orig_ci_lower = orig_res['ci_lower']\n",
        "            orig_ci_upper = orig_res['ci_upper']\n",
        "\n",
        "            # Run Loop\n",
        "            studies = df_loo['id'].unique()\n",
        "            results = []\n",
        "            print(f\"  Processing {len(studies)} studies...\")\n",
        "\n",
        "            for i, study in enumerate(studies):\n",
        "                if i % 5 == 0: print(f\"  ... {i}/{len(studies)}\", end='\\r')\n",
        "\n",
        "                # Remove study\n",
        "                subset = df_loo[df_loo['id'] != study]\n",
        "\n",
        "                # Run Robust Optimizer\n",
        "                est = _run_three_level_reml_loo(subset, effect_col, var_col)\n",
        "\n",
        "                if est:\n",
        "                    mu = est['mu']\n",
        "                    se = est['se_mu']\n",
        "\n",
        "                    # --- UPDATED: Dynamic Inference Logic ---\n",
        "                    # 1. Retrieve Global Settings\n",
        "                    gs = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "                    alpha_val = gs.get('alpha', 0.05)\n",
        "                    dist_type = gs.get('dist_type', 'norm')\n",
        "\n",
        "                    # 2. Calculate Critical Value\n",
        "                    q_val = 1 - (alpha_val / 2)\n",
        "\n",
        "                    if dist_type == 't':\n",
        "                        # Use t-distribution (df = k_studies_in_subset - 1)\n",
        "                        n_sub_studies = subset['id'].nunique()\n",
        "                        df_sub = max(1, n_sub_studies - 1)\n",
        "                        crit_val = t.ppf(q_val, df_sub)\n",
        "                    else:\n",
        "                        # Use Normal distribution (Z-score)\n",
        "                        crit_val = norm.ppf(q_val)\n",
        "\n",
        "                    # 3. Calculate Intervals\n",
        "                    ci_lo = mu - crit_val * se\n",
        "                    ci_hi = mu + crit_val * se\n",
        "\n",
        "                    # 4. Check Significance Change\n",
        "                    null_val = es_config.get('null_value', 0)\n",
        "                    # Check if original was significant (using original CI bounds passed earlier)\n",
        "                    orig_sig = not (orig_ci_lower <= null_val <= orig_ci_upper)\n",
        "                    # Check if new LOO estimate is significant\n",
        "                    loo_sig = not (ci_lo <= null_val <= ci_hi)\n",
        "                    # ----------------------------------------\n",
        "\n",
        "                    results.append({\n",
        "                        'unit_removed': str(study),\n",
        "                        'k_studies': subset['id'].nunique(),\n",
        "                        'k_obs': len(subset),\n",
        "                        'pooled_effect': mu,\n",
        "                        'se': se,\n",
        "                        'ci_lower': ci_lo,\n",
        "                        'ci_upper': ci_hi,\n",
        "                        'effect_diff': mu - orig_eff,\n",
        "                        'abs_diff': abs(mu - orig_eff),\n",
        "                        'changes_sig': (orig_sig != loo_sig),\n",
        "                        'tau_squared': est['tau_sq'],\n",
        "                        'sigma_squared': est['sigma_sq']\n",
        "                    })\n",
        "\n",
        "            print(f\"  ✓ Completed {len(results)} iterations.\\n\")\n",
        "\n",
        "            if len(results) == 0:\n",
        "                print(\"❌ Error: No iterations succeeded.\")\n",
        "                return\n",
        "\n",
        "            results_df = pd.DataFrame(results)\n",
        "\n",
        "            # Check for Significance Changes\n",
        "            sig_changers = results_df[results_df['changes_sig'] == True]\n",
        "\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"RESULTS SUMMARY\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"  Original Effect: {orig_eff:.4f}\")\n",
        "            print(f\"  Range of LOO Effects: {results_df['pooled_effect'].min():.4f} to {results_df['pooled_effect'].max():.4f}\")\n",
        "\n",
        "            if not sig_changers.empty:\n",
        "                print(f\"\\n⚠️  WARNING: Removing these studies changed statistical significance:\")\n",
        "                print(f\"    {', '.join(sig_changers['unit_removed'].tolist())}\")\n",
        "            else:\n",
        "                print(\"\\n✅ ROBUST: No single study removal changed the statistical significance.\")\n",
        "\n",
        "            # --- SAVE RESULTS ---\n",
        "            ANALYSIS_CONFIG['loo_3level_results'] = {\n",
        "                'timestamp': datetime.datetime.now(),\n",
        "                'results_df': results_df,\n",
        "                'removal_unit': 'study',\n",
        "                'original_effect': orig_eff,\n",
        "                'n_sig_changers': len(sig_changers)\n",
        "            }\n",
        "            print(\"\\n✅ DONE: Results saved to 'loo_3level_results'\")\n",
        "            print(\"   👉 NOW RUN CELL 13b TO SEE THE PLOT\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "run_loo_btn.on_click(run_loo_analysis)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    header,\n",
        "    run_loo_btn,\n",
        "    loo_output\n",
        "]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VDPgbSC5oACY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔄 Leave-One-Out Sensitivity Analysis (Structure)\n",
        "# =============================================================================\n",
        "# CELL 13: ROBUST LEAVE-ONE-OUT ANALYSIS (TABBED INTERFACE)\n",
        "# Purpose: Calculate influence of each study on the 3-level pooled effect.\n",
        "# Structure: Tabs for Results, Influence, Text, and Export.\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import datetime\n",
        "\n",
        "# --- 1. UI LAYOUT ---\n",
        "tab_results = widgets.Output()\n",
        "tab_influence = widgets.Output()\n",
        "tab_text = widgets.Output()\n",
        "tab_export = widgets.Output()\n",
        "\n",
        "tabs = widgets.Tab(children=[tab_results, tab_influence, tab_text, tab_export])\n",
        "tabs.set_title(0, '📊 Analysis Summary')\n",
        "tabs.set_title(1, '⚠️ Influence Diagnostics')\n",
        "tabs.set_title(2, '📝 Publication Text')\n",
        "tabs.set_title(3, '💾 Export')\n",
        "\n",
        "# --- 2. HELPER: PLACEHOLDER GENERATORS (We will replace these later) ---\n",
        "def generate_loo_publication_text(res_df, orig_est, es_label=\"Effect Size\"):\n",
        "    \"\"\"\n",
        "    Generates a dynamic 'Results' section for Sensitivity Analysis.\n",
        "    Detects if results are robust or sensitive to specific studies.\n",
        "    \"\"\"\n",
        "    # 1. Analyze Data\n",
        "    min_est = res_df['pooled_effect'].min()\n",
        "    max_est = res_df['pooled_effect'].max()\n",
        "    max_change_row = res_df.loc[res_df['abs_diff'].idxmax()]\n",
        "\n",
        "    sig_changers = res_df[res_df['changes_sig'] == True]\n",
        "    num_changers = len(sig_changers)\n",
        "\n",
        "    # 2. Determine Robustness Status\n",
        "    if num_changers == 0:\n",
        "        status = \"robust\"\n",
        "        conclusion = \"indicating that the meta-analytic conclusion is not dependent on any single study\"\n",
        "    else:\n",
        "        status = \"sensitive\"\n",
        "        conclusion = f\"indicating that the statistical significance of the pooled effect depends on the inclusion of {num_changers} specific stud{'y' if num_changers==1 else 'ies'}\"\n",
        "\n",
        "    # 3. Build Text\n",
        "    html = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff;'>\n",
        "\n",
        "    <h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px;'>Sensitivity Analysis Results</h3>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    To evaluate the robustness of the pooled {es_label}, we conducted a leave-one-out sensitivity analysis.\n",
        "    We iteratively removed one study at a time and re-calculated the pooled effect size using the same three-level random-effects model.\n",
        "    </p>\n",
        "\n",
        "    <h4 style='color: #34495e; margin-top: 20px;'>Impact on Effect Size</h4>\n",
        "    <p style='text-align: justify;'>\n",
        "    The pooled effect size varied from <b>{min_est:.3f}</b> to <b>{max_est:.3f}</b> across all iterations (Original estimate: {orig_est:.3f}).\n",
        "    The most influential study was <b>{max_change_row['unit_removed']}</b>; its exclusion caused the pooled effect to shift by {max_change_row['pct_change']:.1f}% (New estimate: {max_change_row['pooled_effect']:.3f}).\n",
        "    </p>\n",
        "\n",
        "    <h4 style='color: #34495e; margin-top: 20px;'>Impact on Statistical Significance</h4>\n",
        "    <p style='text-align: justify;'>\n",
        "    Crucially, the removal of any single study <b>did {'' if num_changers > 0 else 'not'}</b> alter the statistical significance of the pooled effect, {conclusion}.\n",
        "    </p>\n",
        "    \"\"\"\n",
        "\n",
        "    # 4. Add details if sensitive\n",
        "    if num_changers > 0:\n",
        "        studies_list = \", \".join(sig_changers['unit_removed'].tolist()[:3])\n",
        "        if num_changers > 3: studies_list += f\", and {num_changers-3} others\"\n",
        "\n",
        "        html += f\"\"\"\n",
        "        <div style='background-color: #fff3cd; padding: 15px; border-left: 4px solid #ffc107; margin-top: 15px;'>\n",
        "        <p style='margin: 0;'><b>⚠️ Sensitivity Note:</b> The result became non-significant (or significant) when omitting: {studies_list}.\n",
        "        These studies should be discussed as critical drivers of the observed effect.</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    else:\n",
        "        html += f\"\"\"\n",
        "        <div style='background-color: #d4edda; padding: 15px; border-left: 4px solid #28a745; margin-top: 15px;'>\n",
        "        <p style='margin: 0;'><b>✅ Robustness Confirmed:</b> The finding remains statistically consistent regardless of which study is omitted.</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    html += \"</div>\"\n",
        "\n",
        "    return html\n",
        "\n",
        "def generate_methods_text_loo(es_config):\n",
        "    \"\"\"\n",
        "    Generates a 'Materials and Methods' section for Sensitivity Analysis.\n",
        "    \"\"\"\n",
        "    import sys\n",
        "\n",
        "    # 1. Extract Settings\n",
        "    es_label = es_config.get('effect_label', 'Effect Size')\n",
        "\n",
        "    # 2. Define Citation Database\n",
        "    db = {\n",
        "        'viechtbauer': \"Viechtbauer, W., & Cheung, M. W. L. (2010). Outlier and influence diagnostics for meta-analysis. <i>Research Synthesis Methods</i>, 1(2), 112-125.\",\n",
        "        'borenstein': \"Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2009). <i>Introduction to Meta-Analysis</i>. Chichester, UK: John Wiley & Sons.\",\n",
        "        'copas': \"Copas, J. B., & Shi, J. Q. (2000). Meta-analysis, funnel plots and sensitivity analysis. <i>Biostatistics</i>, 1(3), 247-262.\",\n",
        "        'tool': \"<b>[CITATION FOR THIS TOOL]:</b> Author, A. A. (202X). <i>Co-Meta: An Interactive Pipeline for Meta-Analysis</i>. [Repository/DOI].\"\n",
        "    }\n",
        "\n",
        "    # 3. Build HTML\n",
        "    html = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff; border: 1px solid #eee; margin-bottom: 20px;'>\n",
        "\n",
        "    <h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-top: 0;'>Materials and Methods</h3>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Sensitivity Analysis.</b> To assess the robustness of the meta-analytic findings and identify potentially influential studies, we conducted a Leave-One-Out sensitivity analysis [1].\n",
        "    This procedure involves iteratively removing one study at a time from the dataset and re-estimating the pooled {es_label} using the same three-level random-effects model specified in the primary analysis.\n",
        "    </p>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Influence Diagnostics.</b> We evaluated the influence of each study by examining the change in the pooled estimate and, crucially, whether the exclusion of any single study altered the statistical significance of the result [2].\n",
        "    Studies whose removal caused a shift in significance or a substantial change in the magnitude of the effect were identified as influential cases that warrant careful consideration [3].\n",
        "    All analyses were performed using the Co-Meta toolkit [4].\n",
        "    </p>\n",
        "\n",
        "    <h4 style='color: #34495e; margin-top: 20px; font-size: 11pt;'>References</h4>\n",
        "    <ol style='font-size: 10pt; color: #555;'>\n",
        "        <li>{db['borenstein']}</li>\n",
        "        <li>{db['viechtbauer']}</li>\n",
        "        <li>{db['copas']}</li>\n",
        "        <li>{db['tool']}</li>\n",
        "    </ol>\n",
        "    </div>\"\"\"\n",
        "\n",
        "    return html\n",
        "\n",
        "# --- 3. MAIN ANALYSIS ENGINE ---\n",
        "def _run_three_level_reml_loo(analysis_data, effect_col, var_col):\n",
        "    \"\"\"Optimization with Two-Pass High Precision Strategy.\"\"\"\n",
        "    grouped = analysis_data.groupby('id')\n",
        "    y_all = [group[effect_col].values for _, group in grouped]\n",
        "    v_all = [group[var_col].values for _, group in grouped]\n",
        "    N_total = len(analysis_data)\n",
        "    M_studies = len(y_all)\n",
        "\n",
        "    if M_studies < 2: return None\n",
        "\n",
        "    # 1. Global Search (L-BFGS-B)\n",
        "    start_points = [[0.01, 0.01], [0.5, 0.1], [0.1, 0.5]]\n",
        "    best_res = None\n",
        "    best_fun = np.inf\n",
        "\n",
        "    for start in start_points:\n",
        "        res = minimize(\n",
        "            _neg_log_lik_reml_loo, x0=start,\n",
        "            args=(y_all, v_all, N_total, M_studies),\n",
        "            method='L-BFGS-B', bounds=[(1e-8, None), (1e-8, None)],\n",
        "            options={'ftol': 1e-10}\n",
        "        )\n",
        "        if res.success and res.fun < best_fun:\n",
        "            best_fun = res.fun\n",
        "            best_res = res\n",
        "\n",
        "    if not best_res: return None\n",
        "\n",
        "    # 2. Polishing (Nelder-Mead)\n",
        "    final_res = minimize(\n",
        "        _neg_log_lik_reml_loo, x0=best_res.x,\n",
        "        args=(y_all, v_all, N_total, M_studies),\n",
        "        method='Nelder-Mead', bounds=[(1e-8, None), (1e-8, None)],\n",
        "        options={'xatol': 1e-10, 'fatol': 1e-10}\n",
        "    )\n",
        "\n",
        "    return _get_three_level_estimates_loo(\n",
        "        final_res.x, y_all, v_all, N_total, M_studies\n",
        "    )\n",
        "\n",
        "def run_loo_analysis(b):\n",
        "    # Clear all tabs\n",
        "    for t in [tab_results, tab_influence, tab_text, tab_export]: t.clear_output()\n",
        "\n",
        "    # Initial Checks\n",
        "    if 'ANALYSIS_CONFIG' not in globals(): return\n",
        "    if 'three_level_results' not in ANALYSIS_CONFIG:\n",
        "        with tab_results: display(HTML(\"<div style='color:red'>❌ Run Cell 6 (Overall Analysis) first.</div>\"))\n",
        "        return\n",
        "\n",
        "    # Load Data\n",
        "    if 'analysis_data' in globals(): df_loo = analysis_data.copy()\n",
        "    elif 'data_filtered' in globals(): df_loo = data_filtered.copy()\n",
        "    else: return\n",
        "\n",
        "    # Get Config\n",
        "    effect_col = ANALYSIS_CONFIG.get('effect_col', 'hedges_g')\n",
        "    var_col = ANALYSIS_CONFIG.get('var_col', 'Vg')\n",
        "    es_config = ANALYSIS_CONFIG.get('es_config', {})\n",
        "    orig_res = ANALYSIS_CONFIG['three_level_results']\n",
        "    orig_eff = orig_res['pooled_effect']\n",
        "    orig_ci_lower = orig_res['ci_lower']\n",
        "    orig_ci_upper = orig_res['ci_upper']\n",
        "    null_val = es_config.get('null_value', 0)\n",
        "    orig_sig = not (orig_ci_lower <= null_val <= orig_ci_upper)\n",
        "\n",
        "    # Run Loop\n",
        "    studies = df_loo['id'].unique()\n",
        "    results = []\n",
        "\n",
        "    with tab_results:\n",
        "        print(f\"⏳ Processing {len(studies)} studies (High-Precision Mode)...\")\n",
        "        progress = widgets.IntProgress(value=0, min=0, max=len(studies))\n",
        "        display(progress)\n",
        "\n",
        "        for i, study in enumerate(studies):\n",
        "            progress.value = i + 1\n",
        "\n",
        "            # Remove study & Re-run\n",
        "            subset = df_loo[df_loo['id'] != study]\n",
        "            est = _run_three_level_reml_loo(subset, effect_col, var_col)\n",
        "\n",
        "            if est:\n",
        "                mu, se = est['mu'], est['se_mu']\n",
        "                ci_lo, ci_hi = mu - 1.96*se, mu + 1.96*se\n",
        "                loo_sig = not (ci_lo <= null_val <= ci_hi)\n",
        "\n",
        "                results.append({\n",
        "                    'unit_removed': str(study),\n",
        "                    'pooled_effect': mu,\n",
        "                    'ci_lower': ci_lo, 'ci_upper': ci_hi,\n",
        "                    'effect_diff': mu - orig_eff,\n",
        "                    'abs_diff': abs(mu - orig_eff),\n",
        "                    'pct_change': ((mu - orig_eff) / orig_eff * 100) if orig_eff != 0 else 0,\n",
        "                    'changes_sig': (orig_sig != loo_sig)\n",
        "                })\n",
        "\n",
        "        clear_output()\n",
        "\n",
        "        if not results:\n",
        "            display(HTML(\"<div style='color:red'>❌ Analysis failed.</div>\"))\n",
        "            return\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        # --- SAVE TO GLOBAL CONFIG ---\n",
        "        ANALYSIS_CONFIG['loo_3level_results'] = {\n",
        "            'timestamp': datetime.datetime.now(),\n",
        "            'results_df': results_df,\n",
        "            'original_effect': orig_eff,\n",
        "            'n_sig_changers': results_df['changes_sig'].sum()\n",
        "        }\n",
        "\n",
        "        # --- POPULATE TABS ---\n",
        "\n",
        "        # Tab 1: Summary\n",
        "        min_eff = results_df['pooled_effect'].min()\n",
        "        max_eff = results_df['pooled_effect'].max()\n",
        "        max_diff = results_df['abs_diff'].max()\n",
        "\n",
        "        summary_html = f\"\"\"\n",
        "        <div style='background-color:#f8f9fa; padding:15px; border-radius:5px;'>\n",
        "            <h4 style='margin-top:0; color:#2E86AB;'>Sensitivity Summary</h4>\n",
        "            <p><b>Original Pooled Effect:</b> {orig_eff:.4f}</p>\n",
        "            <p><b>Range after Leave-One-Out:</b> {min_eff:.4f} to {max_eff:.4f}</p>\n",
        "            <p><b>Max Deviation:</b> {max_diff:.4f}</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        display(HTML(summary_html))\n",
        "\n",
        "        # Tab 2: Influence\n",
        "        with tab_influence:\n",
        "            sig_changers = results_df[results_df['changes_sig'] == True]\n",
        "            if not sig_changers.empty:\n",
        "                display(HTML(f\"<div style='background:#fff3cd; padding:10px; border-left:4px solid #ffc107;'>\"\n",
        "                             f\"<b>⚠️ Critical Influence Detected:</b> Removing these studies changes the statistical significance:\"\n",
        "                             f\"</div>\"))\n",
        "                display(sig_changers[['unit_removed', 'pooled_effect', 'pct_change']])\n",
        "            else:\n",
        "                display(HTML(f\"<div style='background:#d4edda; padding:10px; border-left:4px solid #28a745;'>\"\n",
        "                             f\"<b>✅ Robust Result:</b> No single study removal changes the statistical significance.\"\n",
        "                             f\"</div>\"))\n",
        "\n",
        "            # Top 5 Influencers\n",
        "            display(HTML(\"<h4>Top 5 Most Influential Studies</h4>\"))\n",
        "            top5 = results_df.sort_values('abs_diff', ascending=False).head(5)\n",
        "            display(top5[['unit_removed', 'pooled_effect', 'pct_change']].style.format({'pooled_effect': '{:.4f}', 'pct_change': '{:.2f}%'}))\n",
        "\n",
        "        # Tab 3: Text\n",
        "        with tab_text:\n",
        "            display(HTML(\"<h3 style='color: #2c3e50;'>📝 Publication-Ready Results Text</h3>\"))\n",
        "            display(HTML(\"<p style='color: #6c757d;'>Copy and paste this formatted text into your manuscript:</p>\"))\n",
        "\n",
        "            # 1. Generate Results Text (Part 2)\n",
        "            # Get label safely\n",
        "            label = \"Effect Size\"\n",
        "            if 'ANALYSIS_CONFIG' in globals() and 'es_config' in ANALYSIS_CONFIG:\n",
        "                label = ANALYSIS_CONFIG['es_config'].get('effect_label', 'Effect Size')\n",
        "\n",
        "            results_html = generate_loo_publication_text(results_df, orig_eff, label)\n",
        "\n",
        "            methods_html = generate_methods_text_loo(ANALYSIS_CONFIG.get('es_config', {})            )\n",
        "\n",
        "            display(HTML(methods_html))\n",
        "            display(HTML(results_html))\n",
        "\n",
        "            # Save for Export\n",
        "            ANALYSIS_CONFIG['loo_text'] = methods_html + \"<br><hr><br>\" + results_html\n",
        "\n",
        "        # Tab 4: Export\n",
        "        with tab_export:\n",
        "            display(HTML(\"<h3 style='color: #2E86AB;'>💾 Download Audit Report</h3>\"))\n",
        "            btn_loo_export = widgets.Button(description=\"📥 Download Sensitivity Report\", button_style='info', icon='file-excel', layout=widgets.Layout(width='300px', height='40px'))\n",
        "\n",
        "            def on_loo_export_click(b):\n",
        "                export_analysis_report(report_type='loo', filename_prefix='Sensitivity_Analysis_LOO')\n",
        "\n",
        "            btn_loo_export.on_click(on_loo_export_click)\n",
        "            display(btn_loo_export)\n",
        "\n",
        "# --- 4. DISPLAY ---\n",
        "run_btn = widgets.Button(description='▶ Run Sensitivity Analysis', button_style='success', layout=widgets.Layout(width='400px', height='50px'), style={'font_weight':'bold'})\n",
        "run_btn.on_click(run_loo_analysis)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='color: #2E86AB;'>🔄 Leave-One-Out Sensitivity Analysis</h3>\"),\n",
        "    widgets.HTML(\"<p style='color: #666;'>Assess whether your results rely heavily on any single study.</p>\"),\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    run_btn,\n",
        "    widgets.HTML(\"<br>\"),\n",
        "    tabs\n",
        "]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RPizIMRf2ueO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 📊 Leave-One-Out Plot\n",
        "# =============================================================================\n",
        "# CELL 13b: ADVANCED LEAVE-ONE-OUT PLOTTER\n",
        "# Purpose: Visualize sensitivity analysis with full customization.\n",
        "# Fix: Corrected 'ecolor' error by splitting plots for normal/highlighted studies.\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import datetime\n",
        "\n",
        "# --- 1. INITIALIZATION ---\n",
        "default_title = \"Leave-One-Out Sensitivity Analysis\"\n",
        "default_xlabel = \"Pooled Effect Size\"\n",
        "default_ylabel = \"Study Removed\"\n",
        "\n",
        "try:\n",
        "    if 'ANALYSIS_CONFIG' in globals():\n",
        "        es_config = ANALYSIS_CONFIG.get('es_config', {})\n",
        "        default_xlabel = f\"Pooled {es_config.get('effect_label', 'Effect Size')}\"\n",
        "except: pass\n",
        "\n",
        "# --- 2. WIDGET DEFINITIONS ---\n",
        "\n",
        "# === TAB 1: STYLE ===\n",
        "show_title_widget = widgets.Checkbox(value=True, description='Show Plot Title', indent=False)\n",
        "title_widget = widgets.Text(value=default_title, description='Title:', layout=widgets.Layout(width='450px'))\n",
        "xlabel_widget = widgets.Text(value=default_xlabel, description='X Label:', layout=widgets.Layout(width='450px'))\n",
        "ylabel_widget = widgets.Text(value=default_ylabel, description='Y Label:', layout=widgets.Layout(width='450px'))\n",
        "width_widget = widgets.FloatSlider(value=10.0, min=5.0, max=16.0, step=0.5, description='Width (in):', continuous_update=False)\n",
        "height_auto_widget = widgets.Checkbox(value=True, description='Auto-Height (based on # studies)', indent=False)\n",
        "height_widget = widgets.FloatSlider(value=8.0, min=4.0, max=20.0, step=0.5, description='Manual Height:', continuous_update=False)\n",
        "\n",
        "style_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Labels & Dimensions</h4>\"),\n",
        "    show_title_widget, title_widget,\n",
        "    xlabel_widget, ylabel_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 5px 0;'>\"),\n",
        "    preset_widget,\n",
        "    width_widget, height_auto_widget, height_widget\n",
        "])\n",
        "\n",
        "# === TAB 2: DATA & SORTING ===\n",
        "sort_by_widget = widgets.Dropdown(\n",
        "    options=[('Effect Size (Low to High)', 'effect'),\n",
        "             ('Influence (Diff from Original)', 'influence'),\n",
        "             ('Study ID (Alphabetical)', 'id')],\n",
        "    value='effect', description='Sort By:', layout=widgets.Layout(width='400px')\n",
        ")\n",
        "\n",
        "highlight_sig_widget = widgets.Checkbox(value=True, description='Highlight Significance Changers (Red)', indent=False)\n",
        "point_color_widget = widgets.Dropdown(options=['blue', 'black', 'gray', 'steelblue'], value='blue', description='Point Color:')\n",
        "point_size_widget = widgets.IntSlider(value=6, min=2, max=20, description='Point Size:')\n",
        "\n",
        "data_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Data Presentation</h4>\"),\n",
        "    sort_by_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 5px 0;'>\"),\n",
        "    highlight_sig_widget,\n",
        "    point_color_widget,\n",
        "    point_size_widget\n",
        "])\n",
        "\n",
        "# === TAB 3: REFERENCE LINES ===\n",
        "show_orig_line_widget = widgets.Checkbox(value=True, description='Show Original Effect Line', indent=False)\n",
        "orig_color_widget = widgets.Dropdown(options=['red', 'black', 'green'], value='red', description='Line Color:')\n",
        "show_orig_ci_widget = widgets.Checkbox(value=True, description='Show Original 95% CI Band', indent=False)\n",
        "ci_band_alpha_widget = widgets.FloatSlider(value=0.1, min=0.05, max=0.5, step=0.05, description='Band Alpha:')\n",
        "show_null_line_widget = widgets.Checkbox(value=True, description='Show Null Effect Line', indent=False)\n",
        "\n",
        "lines_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Reference Lines</h4>\"),\n",
        "    show_orig_line_widget, orig_color_widget,\n",
        "    show_orig_ci_widget, ci_band_alpha_widget,\n",
        "    widgets.HTML(\"<hr style='margin: 5px 0;'>\"),\n",
        "    show_null_line_widget\n",
        "])\n",
        "\n",
        "# === TAB 4: EXPORT ===\n",
        "save_pdf_widget = widgets.Checkbox(value=True, description='Save as PDF', indent=False)\n",
        "save_png_widget = widgets.Checkbox(value=True, description='Save as PNG', indent=False)\n",
        "filename_prefix_widget = widgets.Text(value='LOO_Plot', description='Filename:', layout=widgets.Layout(width='300px'))\n",
        "\n",
        "export_tab = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #2E86AB;'>Export</h4>\"),\n",
        "    save_pdf_widget, save_png_widget, filename_prefix_widget\n",
        "])\n",
        "\n",
        "# Assemble Tabs\n",
        "tabs = widgets.Tab(children=[style_tab, data_tab, lines_tab, export_tab])\n",
        "tabs.set_title(0, '🎨 Style')\n",
        "tabs.set_title(1, '📊 Data')\n",
        "tabs.set_title(2, '📐 Lines')\n",
        "tabs.set_title(3, '💾 Export')\n",
        "\n",
        "run_plot_btn = widgets.Button(\n",
        "    description='📊 Generate LOO Plot',\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='450px', height='50px'),\n",
        "    style={'font_weight': 'bold'}\n",
        ")\n",
        "plot_output = widgets.Output()\n",
        "\n",
        "# --- 3. PLOTTING LOGIC ---\n",
        "def generate_loo_plot(b):\n",
        "    with plot_output:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        try:\n",
        "            # 1. Load Results\n",
        "            if 'ANALYSIS_CONFIG' not in globals() or 'loo_3level_results' not in ANALYSIS_CONFIG:\n",
        "                print(\"❌ Error: Run Cell 13 (Leave-One-Out Analysis) first.\")\n",
        "                return\n",
        "\n",
        "            loo_res = ANALYSIS_CONFIG['loo_3level_results']\n",
        "            df = loo_res['results_df'].copy()\n",
        "\n",
        "            # Get original results for reference\n",
        "            if 'three_level_results' in ANALYSIS_CONFIG:\n",
        "                orig_res = ANALYSIS_CONFIG['three_level_results']\n",
        "                orig_eff = orig_res['pooled_effect']\n",
        "                orig_ci_lower = orig_res['ci_lower']\n",
        "                orig_ci_upper = orig_res['ci_upper']\n",
        "            else:\n",
        "                # Fallback if cell 13 was run but cell 6.5 missing (unlikely)\n",
        "                orig_eff = loo_res['original_effect']\n",
        "                orig_ci_lower = df['ci_lower'].mean() # Approx\n",
        "                orig_ci_upper = df['ci_upper'].mean() # Approx\n",
        "\n",
        "            # 2. Sorting\n",
        "            sort_mode = sort_by_widget.value\n",
        "            if sort_mode == 'influence':\n",
        "                df = df.sort_values('abs_diff', ascending=True) # Small diff at bottom\n",
        "            elif sort_mode == 'id':\n",
        "                df = df.sort_values('unit_removed', ascending=False) # Z-A (so A is at top)\n",
        "            else: # effect\n",
        "                df = df.sort_values('pooled_effect', ascending=True)\n",
        "\n",
        "            df = df.reset_index(drop=True)\n",
        "\n",
        "            # 3. Prepare Plot\n",
        "            n_studies = len(df)\n",
        "\n",
        "            # Auto-height calculation: Base + (studies * factor)\n",
        "            if height_auto_widget.value:\n",
        "                plot_height = max(5, 1 + n_studies * 0.25)\n",
        "            else:\n",
        "                plot_height = height_widget.value\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(width_widget.value, plot_height))\n",
        "\n",
        "            y_pos = np.arange(n_studies)\n",
        "\n",
        "            # --- Create Splitted Dataframes for Error Bars ---\n",
        "            # ax.errorbar doesn't accept a list of colors in all matplotlib versions.\n",
        "            # Solution: Plot normal and highlighted bars separately.\n",
        "\n",
        "            if highlight_sig_widget.value:\n",
        "                # Identify rows that changed significance\n",
        "                mask_sig = df['changes_sig'] == True\n",
        "                mask_norm = ~mask_sig\n",
        "            else:\n",
        "                # Treat all as normal\n",
        "                mask_sig = pd.Series([False] * n_studies)\n",
        "                mask_norm = pd.Series([True] * n_studies)\n",
        "\n",
        "            # Plot Normal Error Bars\n",
        "            if mask_norm.any():\n",
        "                ax.errorbar(df.loc[mask_norm, 'pooled_effect'], y_pos[mask_norm],\n",
        "                           xerr=[df.loc[mask_norm, 'pooled_effect'] - df.loc[mask_norm, 'ci_lower'],\n",
        "                                 df.loc[mask_norm, 'ci_upper'] - df.loc[mask_norm, 'pooled_effect']],\n",
        "                           fmt='none', ecolor=point_color_widget.value, alpha=0.5, capsize=3)\n",
        "\n",
        "            # Plot Highlighted Error Bars (Red)\n",
        "            if mask_sig.any():\n",
        "                ax.errorbar(df.loc[mask_sig, 'pooled_effect'], y_pos[mask_sig],\n",
        "                           xerr=[df.loc[mask_sig, 'pooled_effect'] - df.loc[mask_sig, 'ci_lower'],\n",
        "                                 df.loc[mask_sig, 'ci_upper'] - df.loc[mask_sig, 'pooled_effect']],\n",
        "                           fmt='none', ecolor='red', alpha=0.8, capsize=3)\n",
        "\n",
        "            # Plot Points (Scatter accepts list of colors)\n",
        "            colors = ['red' if (x and highlight_sig_widget.value) else point_color_widget.value for x in df['changes_sig']]\n",
        "            ax.scatter(df['pooled_effect'], y_pos, c=colors, s=point_size_widget.value*5, zorder=3)\n",
        "\n",
        "            # --- Reference Lines ---\n",
        "            # Null Line\n",
        "            null_val = ANALYSIS_CONFIG.get('es_config', {}).get('null_value', 0)\n",
        "            if show_null_line_widget.value:\n",
        "                ax.axvline(null_val, color='black', linestyle='-', linewidth=1, alpha=0.5, zorder=1)\n",
        "\n",
        "            # Original CI Band\n",
        "            if show_orig_ci_widget.value:\n",
        "                # Get dynamic label\n",
        "                gs = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "                alpha_val = gs.get('alpha', 0.05)\n",
        "                ci_pct = (1 - alpha_val) * 100\n",
        "\n",
        "                ax.axvspan(orig_ci_lower, orig_ci_upper, color=orig_color_widget.value,\n",
        "                          alpha=ci_band_alpha_widget.value, label=f'Original {ci_pct:.0f}% CI', zorder=0)\n",
        "\n",
        "            # Original Mean Line\n",
        "            if show_orig_line_widget.value:\n",
        "                ax.axvline(orig_eff, color=orig_color_widget.value, linestyle='--', linewidth=2,\n",
        "                          label=f'Original Effect ({orig_eff:.3f})', zorder=2)\n",
        "\n",
        "            # --- Layout ---\n",
        "            ax.set_yticks(y_pos)\n",
        "            ax.set_yticklabels(df['unit_removed'], fontsize=9)\n",
        "\n",
        "            if show_title_widget.value:\n",
        "                ax.set_title(title_widget.value, fontsize=14, fontweight='bold', pad=15)\n",
        "            ax.set_xlabel(xlabel_widget.value, fontsize=12, fontweight='bold')\n",
        "            ax.set_ylabel(ylabel_widget.value, fontsize=12, fontweight='bold')\n",
        "\n",
        "            # Add grid for easier reading\n",
        "            ax.grid(axis='y', linestyle=':', alpha=0.3)\n",
        "            ax.grid(axis='x', linestyle=':', alpha=0.3)\n",
        "\n",
        "            # Legend\n",
        "            handles, labels = ax.get_legend_handles_labels()\n",
        "            # Add custom handle for \"Changed Significance\" if needed\n",
        "            if highlight_sig_widget.value and df['changes_sig'].any():\n",
        "                handles.append(mpatches.Patch(color='red', label='Changed Significance'))\n",
        "\n",
        "            ax.legend(handles=handles, loc='best', frameon=True, fancybox=True)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # --- Export ---\n",
        "            ts = datetime.datetime.now().strftime(\"%H%M%S\")\n",
        "            fn = filename_prefix_widget.value\n",
        "\n",
        "            if save_pdf_widget.value:\n",
        "                plt.savefig(f\"{fn}_{ts}.pdf\", bbox_inches='tight')\n",
        "                print(f\"💾 Saved: {fn}_{ts}.pdf\")\n",
        "\n",
        "            if save_png_widget.value:\n",
        "                plt.savefig(f\"{fn}_{ts}.png\", dpi=png_dpi_widget.value, bbox_inches='tight')\n",
        "                print(f\"💾 Saved: {fn}_{ts}.png\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Plotting Error: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "run_plot_btn.on_click(generate_loo_plot)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    header,\n",
        "    tabs,\n",
        "    widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "    run_plot_btn,\n",
        "    plot_output\n",
        "]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UEY-WLRKRDNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cell-13c"
      },
      "outputs": [],
      "source": [
        "#@title 📊 Baujat Plot - Diagnostic Visualization\n",
        "# =============================================================================\n",
        "# CELL 13c: BAUJAT PLOT (Heterogeneity vs Influence)\n",
        "# Purpose: Identify studies that contribute heavily to both\n",
        "#          heterogeneity AND influence the pooled effect.\n",
        "# Dependencies: Requires Cell 13 (LOO analysis) and Cell 6 (Overall analysis)\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import datetime\n",
        "\n",
        "# --- 0. VALIDATION ---\n",
        "if 'ANALYSIS_CONFIG' not in globals():\n",
        "    print(\"❌ ERROR: ANALYSIS_CONFIG not found. Please run Cell 6 first.\")\n",
        "elif 'loo_3level_results' not in ANALYSIS_CONFIG:\n",
        "    print(\"❌ ERROR: LOO results not found. Please run Cell 13 first.\")\n",
        "else:\n",
        "    print(\"✅ Configuration loaded successfully\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"BAUJAT PLOT CONFIGURATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # --- 1. WIDGET DEFINITIONS ---\n",
        "\n",
        "    # ========== TAB 1: PLOT STYLE ==========\n",
        "    style_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>Plot Style & Layout</h3>\")\n",
        "\n",
        "    width_widget = widgets.FloatSlider(\n",
        "        value=10.0, min=6.0, max=16.0, step=0.5,\n",
        "        description='Plot Width (in):',\n",
        "        continuous_update=False,\n",
        "        style={'description_width': '130px'},\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    height_widget = widgets.FloatSlider(\n",
        "        value=8.0, min=6.0, max=14.0, step=0.5,\n",
        "        description='Plot Height (in):',\n",
        "        continuous_update=False,\n",
        "        style={'description_width': '130px'},\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    point_size_widget = widgets.IntSlider(\n",
        "        value=80, min=20, max=200, step=10,\n",
        "        description='Point Size:',\n",
        "        continuous_update=False,\n",
        "        style={'description_width': '130px'},\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    point_color_widget = widgets.Dropdown(\n",
        "        options=['steelblue', 'darkblue', 'black', 'gray', 'crimson', 'forestgreen'],\n",
        "        value='steelblue',\n",
        "        description='Point Color:',\n",
        "        style={'description_width': '130px'},\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    alpha_widget = widgets.FloatSlider(\n",
        "        value=0.6, min=0.1, max=1.0, step=0.05,\n",
        "        description='Opacity (Alpha):',\n",
        "        continuous_update=False,\n",
        "        style={'description_width': '130px'},\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    show_median_lines_widget = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description='Show Median Reference Lines',\n",
        "        indent=False,\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    style_tab = widgets.VBox([\n",
        "        style_header,\n",
        "        width_widget,\n",
        "        height_widget,\n",
        "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "        widgets.HTML(\"<b>Point Style:</b>\"),\n",
        "        point_size_widget,\n",
        "        point_color_widget,\n",
        "        alpha_widget,\n",
        "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "        show_median_lines_widget\n",
        "    ])\n",
        "\n",
        "    # ========== TAB 2: LABELS ==========\n",
        "    text_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>Text & Labels</h3>\")\n",
        "\n",
        "    try:\n",
        "        if 'ANALYSIS_CONFIG' in globals():\n",
        "            es_config = ANALYSIS_CONFIG.get('es_config', {})\n",
        "            effect_label = es_config.get('effect_label', 'Effect Size')\n",
        "        else:\n",
        "            effect_label = 'Effect Size'\n",
        "    except:\n",
        "        effect_label = 'Effect Size'\n",
        "\n",
        "    show_title_widget = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description='Show Plot Title',\n",
        "        indent=False,\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    title_widget = widgets.Text(\n",
        "        value='Baujat Plot: Heterogeneity vs Influence',\n",
        "        description='Plot Title:',\n",
        "        layout=widgets.Layout(width='450px'),\n",
        "        style={'description_width': '130px'}\n",
        "    )\n",
        "\n",
        "    xlabel_widget = widgets.Text(\n",
        "        value='Contribution to Heterogeneity (Q)',\n",
        "        description='X-Axis Label:',\n",
        "        layout=widgets.Layout(width='450px'),\n",
        "        style={'description_width': '130px'}\n",
        "    )\n",
        "\n",
        "    ylabel_widget = widgets.Text(\n",
        "        value=f'Influence on Pooled {effect_label}',\n",
        "        description='Y-Axis Label:',\n",
        "        layout=widgets.Layout(width='450px'),\n",
        "        style={'description_width': '130px'}\n",
        "    )\n",
        "\n",
        "    title_fontsize_widget = widgets.IntSlider(\n",
        "        value=14, min=8, max=20, step=1,\n",
        "        description='Title Font Size:',\n",
        "        continuous_update=False,\n",
        "        style={'description_width': '130px'},\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    label_fontsize_widget = widgets.IntSlider(\n",
        "        value=12, min=8, max=16, step=1,\n",
        "        description='Axis Label Size:',\n",
        "        continuous_update=False,\n",
        "        style={'description_width': '130px'},\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    text_tab = widgets.VBox([\n",
        "        text_header,\n",
        "        show_title_widget,\n",
        "        title_widget,\n",
        "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "        xlabel_widget,\n",
        "        ylabel_widget,\n",
        "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "        title_fontsize_widget,\n",
        "        label_fontsize_widget\n",
        "    ])\n",
        "\n",
        "    # ========== TAB 3: OUTLIER LABELS ==========\n",
        "    outlier_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>Outlier Study Labels</h3>\")\n",
        "\n",
        "    show_labels_widget = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description='Show Study Labels',\n",
        "        indent=False,\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    label_method_widget = widgets.Dropdown(\n",
        "        options=[\n",
        "            ('Top Outliers (Combined Score)', 'combined'),\n",
        "            ('Top by Heterogeneity Only', 'heterogeneity'),\n",
        "            ('Top by Influence Only', 'influence'),\n",
        "            ('All Studies', 'all')\n",
        "        ],\n",
        "        value='combined',\n",
        "        description='Labeling Method:',\n",
        "        style={'description_width': '130px'},\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    label_threshold_widget = widgets.IntSlider(\n",
        "        value=5, min=1, max=20, step=1,\n",
        "        description='Max Labels:',\n",
        "        continuous_update=False,\n",
        "        style={'description_width': '130px'},\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    label_position_widget = widgets.Dropdown(\n",
        "        options=[\n",
        "            ('Auto (Best Position)', 'best'),\n",
        "            ('Right', 'right'),\n",
        "            ('Left', 'left'),\n",
        "            ('Top', 'top'),\n",
        "            ('Bottom', 'bottom')\n",
        "        ],\n",
        "        value='best',\n",
        "        description='Label Position:',\n",
        "        style={'description_width': '130px'},\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    label_fontsize_widget = widgets.IntSlider(\n",
        "        value=9, min=6, max=14, step=1,\n",
        "        description='Label Font Size:',\n",
        "        continuous_update=False,\n",
        "        style={'description_width': '130px'},\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    outlier_tab = widgets.VBox([\n",
        "        outlier_header,\n",
        "        show_labels_widget,\n",
        "        label_method_widget,\n",
        "        label_threshold_widget,\n",
        "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "        label_position_widget,\n",
        "        label_fontsize_widget\n",
        "    ])\n",
        "\n",
        "    # ========== TAB 4: EXPORT ==========\n",
        "    export_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>Export Options</h3>\")\n",
        "\n",
        "    save_pdf_widget = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description='Save as PDF',\n",
        "        indent=False,\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    save_png_widget = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description='Save as PNG',\n",
        "        indent=False,\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    png_dpi_widget = widgets.IntSlider(\n",
        "        value=300, min=72, max=600, step=50,\n",
        "        description='PNG DPI:',\n",
        "        continuous_update=False,\n",
        "        style={'description_width': '130px'},\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    filename_prefix_widget = widgets.Text(\n",
        "        value='Baujat_Plot',\n",
        "        description='Filename Prefix:',\n",
        "        layout=widgets.Layout(width='450px'),\n",
        "        style={'description_width': '130px'}\n",
        "    )\n",
        "\n",
        "    include_timestamp_widget = widgets.Checkbox(\n",
        "        value=True,\n",
        "        description='Include Timestamp in Filename',\n",
        "        indent=False,\n",
        "        layout=widgets.Layout(width='450px')\n",
        "    )\n",
        "\n",
        "    export_tab = widgets.VBox([\n",
        "        export_header,\n",
        "        save_pdf_widget,\n",
        "        save_png_widget,\n",
        "        png_dpi_widget,\n",
        "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
        "        filename_prefix_widget,\n",
        "        include_timestamp_widget\n",
        "    ])\n",
        "\n",
        "    # ========== ASSEMBLE TABS ==========\n",
        "    tabs = widgets.Tab(children=[style_tab, text_tab, outlier_tab, export_tab])\n",
        "    tabs.set_title(0, '🎨 Style')\n",
        "    tabs.set_title(1, '📝 Labels')\n",
        "    tabs.set_title(2, '🏷️ Outlier Labels')\n",
        "    tabs.set_title(3, '💾 Export')\n",
        "\n",
        "    # --- 2. MAIN PLOTTING FUNCTION ---\n",
        "\n",
        "    def generate_baujat_plot(b):\n",
        "        \"\"\"Generate the Baujat Plot.\"\"\"\n",
        "        with plot_output:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "            print(\"=\" * 70)\n",
        "            print(\"GENERATING BAUJAT PLOT\")\n",
        "            print(\"=\" * 70)\n",
        "\n",
        "            try:\n",
        "                # --- A. LOAD DATA ---\n",
        "                if 'loo_3level_results' not in ANALYSIS_CONFIG:\n",
        "                    print(\"❌ ERROR: LOO results not found. Run Cell 13 first.\")\n",
        "                    return\n",
        "\n",
        "                if 'overall_results' not in ANALYSIS_CONFIG:\n",
        "                    print(\"❌ ERROR: Overall results not found. Run Cell 6 first.\")\n",
        "                    return\n",
        "\n",
        "                if 'analysis_data' not in globals():\n",
        "                    print(\"❌ ERROR: Raw data not found.\")\n",
        "                    return\n",
        "\n",
        "                # Get LOO results (Y-axis: Influence)\n",
        "                loo_results = ANALYSIS_CONFIG['loo_3level_results']['results_df'].copy()\n",
        "\n",
        "                if 'abs_diff' not in loo_results.columns:\n",
        "                    print(\"❌ ERROR: 'abs_diff' column not found in LOO results.\")\n",
        "                    return\n",
        "\n",
        "                # Get raw study data\n",
        "                raw_data = analysis_data.copy()\n",
        "\n",
        "                # Get fixed-effect pooled mean\n",
        "                fe_pooled = ANALYSIS_CONFIG['overall_results']['pooled_effect_fixed']\n",
        "\n",
        "                # Get effect and variance columns\n",
        "                effect_col = ANALYSIS_CONFIG.get('effect_col', 'hedges_g')\n",
        "                var_col = ANALYSIS_CONFIG.get('var_col', 'Vg')\n",
        "\n",
        "                print(f\"✓ Loaded LOO results: {len(loo_results)} studies\")\n",
        "                print(f\"✓ Fixed-Effect Pooled Mean: {fe_pooled:.4f}\")\n",
        "                print(f\"✓ Effect column: {effect_col}, Variance column: {var_col}\")\n",
        "\n",
        "                # --- B. CALCULATE Q CONTRIBUTION (X-axis: Heterogeneity) ---\n",
        "                # For each study, calculate: Q_contrib = w_i × (y_i - μ_FE)²\n",
        "\n",
        "                q_contrib_data = []\n",
        "\n",
        "                for study_id in raw_data['id'].unique():\n",
        "                    study_rows = raw_data[raw_data['id'] == study_id]\n",
        "\n",
        "                    # Calculate Q contribution for each observation in the study\n",
        "                    for idx, row in study_rows.iterrows():\n",
        "                        y_i = row[effect_col]\n",
        "                        v_i = row[var_col]\n",
        "\n",
        "                        if pd.isna(y_i) or pd.isna(v_i) or v_i <= 0:\n",
        "                            continue\n",
        "\n",
        "                        w_i = 1.0 / v_i  # Inverse variance weight\n",
        "                        q_contrib = w_i * (y_i - fe_pooled) ** 2\n",
        "\n",
        "                        q_contrib_data.append({\n",
        "                            'id': study_id,\n",
        "                            'q_contrib': q_contrib\n",
        "                        })\n",
        "\n",
        "                q_contrib_df = pd.DataFrame(q_contrib_data)\n",
        "\n",
        "                # Aggregate Q contribution by study (sum across observations)\n",
        "                q_contrib_by_study = q_contrib_df.groupby('id')['q_contrib'].sum().reset_index()\n",
        "                q_contrib_by_study.columns = ['unit_removed', 'q_contrib']\n",
        "\n",
        "                print(f\"✓ Calculated Q contributions for {len(q_contrib_by_study)} studies\")\n",
        "\n",
        "                # --- C. MERGE DATA ---\n",
        "                # Merge LOO results (Y: abs_diff) with Q contributions (X: q_contrib)\n",
        "                baujat_data = pd.merge(\n",
        "                    loo_results[['unit_removed', 'abs_diff']],\n",
        "                    q_contrib_by_study,\n",
        "                    on='unit_removed',\n",
        "                    how='inner'\n",
        "                )\n",
        "\n",
        "                # Drop any rows with NaN\n",
        "                baujat_data = baujat_data.dropna(subset=['q_contrib', 'abs_diff'])\n",
        "\n",
        "                if len(baujat_data) == 0:\n",
        "                    print(\"❌ ERROR: No valid data points after merging.\")\n",
        "                    return\n",
        "\n",
        "                print(f\"✓ Merged data: {len(baujat_data)} studies\")\n",
        "                print(f\"  Q range: [{baujat_data['q_contrib'].min():.4f}, {baujat_data['q_contrib'].max():.4f}]\")\n",
        "                print(f\"  Influence range: [{baujat_data['abs_diff'].min():.4f}, {baujat_data['abs_diff'].max():.4f}]\")\n",
        "\n",
        "                # --- D. IDENTIFY OUTLIERS ---\n",
        "                if show_labels_widget.value:\n",
        "                    label_method = label_method_widget.value\n",
        "                    n_labels = min(label_threshold_widget.value, len(baujat_data))\n",
        "\n",
        "                    if label_method == 'combined':\n",
        "                        # Combined score: standardized sum of squared deviations\n",
        "                        # Normalize both dimensions, then calculate Euclidean distance from origin\n",
        "                        q_norm = (baujat_data['q_contrib'] - baujat_data['q_contrib'].mean()) / baujat_data['q_contrib'].std()\n",
        "                        inf_norm = (baujat_data['abs_diff'] - baujat_data['abs_diff'].mean()) / baujat_data['abs_diff'].std()\n",
        "                        baujat_data['combined_score'] = np.sqrt(q_norm**2 + inf_norm**2)\n",
        "                        baujat_data = baujat_data.sort_values('combined_score', ascending=False)\n",
        "                    elif label_method == 'heterogeneity':\n",
        "                        baujat_data = baujat_data.sort_values('q_contrib', ascending=False)\n",
        "                    elif label_method == 'influence':\n",
        "                        baujat_data = baujat_data.sort_values('abs_diff', ascending=False)\n",
        "                    else:  # 'all'\n",
        "                        n_labels = len(baujat_data)\n",
        "\n",
        "                    studies_to_label = baujat_data.head(n_labels)['unit_removed'].tolist()\n",
        "                else:\n",
        "                    studies_to_label = []\n",
        "\n",
        "                # --- E. CREATE PLOT ---\n",
        "                fig, ax = plt.subplots(\n",
        "                    figsize=(width_widget.value, height_widget.value),\n",
        "                    facecolor='white'\n",
        "                )\n",
        "\n",
        "                # Scatter plot\n",
        "                scatter = ax.scatter(\n",
        "                    baujat_data['q_contrib'],\n",
        "                    baujat_data['abs_diff'],\n",
        "                    s=point_size_widget.value,\n",
        "                    c=point_color_widget.value,\n",
        "                    alpha=alpha_widget.value,\n",
        "                    edgecolors='black',\n",
        "                    linewidths=0.5,\n",
        "                    zorder=3\n",
        "                )\n",
        "\n",
        "                # Add median reference lines\n",
        "                if show_median_lines_widget.value:\n",
        "                    median_q = baujat_data['q_contrib'].median()\n",
        "                    median_inf = baujat_data['abs_diff'].median()\n",
        "\n",
        "                    ax.axvline(median_q, color='gray', linestyle='--', linewidth=1, alpha=0.5, zorder=1)\n",
        "                    ax.axhline(median_inf, color='gray', linestyle='--', linewidth=1, alpha=0.5, zorder=1)\n",
        "\n",
        "\n",
        "                # Add labels\n",
        "                if show_labels_widget.value and len(studies_to_label) > 0:\n",
        "                    texts = []\n",
        "                    for study in studies_to_label:\n",
        "                        study_data = baujat_data[baujat_data['unit_removed'] == study].iloc[0]\n",
        "                        x_pos = study_data['q_contrib']\n",
        "                        y_pos = study_data['abs_diff']\n",
        "\n",
        "                        # Determine position\n",
        "                        if label_position_widget.value == 'best':\n",
        "                            ha = 'left'\n",
        "                            va = 'bottom'\n",
        "                            xytext = (5, 5)\n",
        "                        elif label_position_widget.value == 'right':\n",
        "                            ha = 'left'\n",
        "                            va = 'center'\n",
        "                            xytext = (10, 0)\n",
        "                        elif label_position_widget.value == 'left':\n",
        "                            ha = 'right'\n",
        "                            va = 'center'\n",
        "                            xytext = (-10, 0)\n",
        "                        elif label_position_widget.value == 'top':\n",
        "                            ha = 'center'\n",
        "                            va = 'bottom'\n",
        "                            xytext = (0, 10)\n",
        "                        else:  # bottom\n",
        "                            ha = 'center'\n",
        "                            va = 'top'\n",
        "                            xytext = (0, -10)\n",
        "\n",
        "                        text = ax.annotate(\n",
        "                            study,\n",
        "                            xy=(x_pos, y_pos),\n",
        "                            xytext=xytext,\n",
        "                            textcoords='offset points',\n",
        "                            fontsize=label_fontsize_widget.value,\n",
        "                            ha=ha,\n",
        "                            va=va,\n",
        "                            bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5),\n",
        "                            arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', lw=0.5)\n",
        "                        )\n",
        "                        texts.append(text)\n",
        "\n",
        "                    # Use adjustText to avoid overlaps (if available)\n",
        "                    try:\n",
        "                        from adjustText import adjust_text\n",
        "                        adjust_text(texts, ax=ax, arrowprops=dict(arrowstyle='->', color='black', lw=0.5))\n",
        "                    except (ImportError, Exception):\n",
        "                        pass  # If adjustText not available or fails, skip automatic adjustment\n",
        "\n",
        "                # Formatting\n",
        "                ax.set_xlabel(\n",
        "                    xlabel_widget.value,\n",
        "                    fontsize=label_fontsize_widget.value,\n",
        "                    fontweight='bold'\n",
        "                )\n",
        "                ax.set_ylabel(\n",
        "                    ylabel_widget.value,\n",
        "                    fontsize=label_fontsize_widget.value,\n",
        "                    fontweight='bold'\n",
        "                )\n",
        "\n",
        "                if show_title_widget.value:\n",
        "                    ax.set_title(\n",
        "                        title_widget.value,\n",
        "                        fontsize=title_fontsize_widget.value,\n",
        "                        fontweight='bold',\n",
        "                        pad=20\n",
        "                    )\n",
        "\n",
        "                ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
        "                ax.set_axisbelow(True)\n",
        "\n",
        "                # Ensure non-negative axes (Q and influence should be positive)\n",
        "                ax.set_xlim(left=0)\n",
        "                ax.set_ylim(bottom=0)\n",
        "\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # --- F. EXPORT ---\n",
        "                if save_pdf_widget.value or save_png_widget.value:\n",
        "                    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") if include_timestamp_widget.value else \"\"\n",
        "                    prefix = filename_prefix_widget.value\n",
        "\n",
        "                    if save_pdf_widget.value:\n",
        "                        filename = f\"{prefix}_{timestamp}.pdf\" if timestamp else f\"{prefix}.pdf\"\n",
        "                        plt.savefig(filename, bbox_inches='tight')\n",
        "                        print(f\"💾 Saved: {filename}\")\n",
        "\n",
        "                    if save_png_widget.value:\n",
        "                        filename = f\"{prefix}_{timestamp}.png\" if timestamp else f\"{prefix}.png\"\n",
        "                        plt.savefig(filename, dpi=png_dpi_widget.value, bbox_inches='tight')\n",
        "                        print(f\"💾 Saved: {filename}\")\n",
        "\n",
        "                plt.show()\n",
        "                print(\"=\" * 70)\n",
        "                print(f\"✅ Baujat Plot Generated Successfully\")\n",
        "                print(f\"   Studies plotted: {len(baujat_data)}\")\n",
        "                print(f\"   Studies labeled: {len(studies_to_label)}\")\n",
        "                print(\"=\" * 70)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ ERROR: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "    # --- 3. BUTTON & OUTPUT ---\n",
        "    run_plot_btn = widgets.Button(\n",
        "        description='📊 Generate Baujat Plot',\n",
        "        button_style='success',\n",
        "        layout=widgets.Layout(width='450px', height='50px'),\n",
        "        style={'font_weight': 'bold'}\n",
        "    )\n",
        "\n",
        "    plot_output = widgets.Output()\n",
        "\n",
        "    run_plot_btn.on_click(generate_baujat_plot)\n",
        "\n",
        "    # --- 4. DISPLAY UI ---\n",
        "    header = widgets.HTML(\"\"\"\n",
        "        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "                    padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n",
        "            <h2 style='color: white; margin: 0; text-align: center;'>\n",
        "                📊 Cell 13c: Baujat Plot - Diagnostic Visualization\n",
        "            </h2>\n",
        "            <p style='color: rgba(255,255,255,0.9); margin: 5px 0 0 0; text-align: center; font-size: 14px;'>\n",
        "                Identify influential studies contributing to heterogeneity and influencing pooled effect\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    info_box = widgets.HTML(\"\"\"\n",
        "        <div style='background-color: #e3f2fd; padding: 15px; border-left: 4px solid #2196f3; margin-bottom: 15px;'>\n",
        "            <p style='margin: 0;'><b>ℹ️ What is a Baujat Plot?</b></p>\n",
        "            <p style='margin: 5px 0 0 0;'>\n",
        "                A diagnostic scatter plot showing each study's contribution to:\n",
        "                <br>• <b>X-axis:</b> Overall heterogeneity (Q statistic)\n",
        "                <br>• <b>Y-axis:</b> Influence on pooled effect (from LOO analysis)\n",
        "                <br>Studies in the <b>top-right quadrant</b> both increase heterogeneity AND heavily influence the result.\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    display(header)\n",
        "    display(info_box)\n",
        "    display(tabs)\n",
        "    display(run_plot_btn)\n",
        "    display(plot_output)\n",
        "\n",
        "    print(\"\\n✅ Baujat Plot interface ready. Click 'Generate Baujat Plot' to create visualization.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 📈 Cumulative Meta-Analysis: Execution (REML Upgrade)\n",
        "# =============================================================================\n",
        "# CELL 14: CUMULATIVE META-ANALYSIS (CALCULATION ENGINE)\n",
        "# Purpose: Calculate how the pooled effect size evolves as studies are added.\n",
        "# Upgrade: Now uses REML (instead of DL) to match the Overall Analysis.\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import datetime\n",
        "from scipy.stats import norm, t\n",
        "\n",
        "# --- 1. UI LAYOUT ---\n",
        "tab_results = widgets.Output()\n",
        "tab_data = widgets.Output()\n",
        "tab_publication = widgets.Output()\n",
        "tab_export = widgets.Output()\n",
        "\n",
        "# Add it to the children list\n",
        "tabs = widgets.Tab(children=[tab_results, tab_data, tab_publication, tab_export])\n",
        "tabs.set_title(0, '📊 Analysis Summary')\n",
        "tabs.set_title(1, '📋 Step-by-Step Data')\n",
        "tabs.set_title(2, '📝 Publication Text')\n",
        "tabs.set_title(3, '💾 Export')\n",
        "\n",
        "# --- 2. CONFIGURATION WIDGETS ---\n",
        "sort_order_widget = widgets.Dropdown(\n",
        "    options=[('Chronological (Oldest → Newest)', 'ascending'),\n",
        "             ('Reverse Chronological (Newest → Oldest)', 'descending')],\n",
        "    value='ascending',\n",
        "    description='Sort Order:',\n",
        "    style={'description_width': '120px'},\n",
        "    layout=widgets.Layout(width='450px')\n",
        ")\n",
        "\n",
        "agg_method_widget = widgets.Dropdown(\n",
        "    options=[('By Study (Recommended)', 'study'),\n",
        "             ('By Observation (Ignore nesting)', 'obs')],\n",
        "    value='study',\n",
        "    description='Aggregation:',\n",
        "    style={'description_width': '120px'},\n",
        "    layout=widgets.Layout(width='450px'),\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# --- 2.5 Supprt text\n",
        "\n",
        "def generate_cumulative_publication_text(res_df, es_label):\n",
        "    \"\"\"Generates dynamic text describing the evolution of evidence.\"\"\"\n",
        "\n",
        "    # Extract Key Data points\n",
        "    start = res_df.iloc[0]\n",
        "    end = res_df.iloc[-1]\n",
        "\n",
        "    k_start, k_end = int(start['k_studies']), int(end['k_studies'])\n",
        "    year_start, year_end = int(start['year']), int(end['year'])\n",
        "\n",
        "    est_start, est_end = start['pooled_effect'], end['pooled_effect']\n",
        "    ci_width_start = start['ci_upper'] - start['ci_lower']\n",
        "    ci_width_end = end['ci_upper'] - end['ci_lower']\n",
        "\n",
        "    # Logic: Did precision improve?\n",
        "    precision_change = (ci_width_end - ci_width_start) / ci_width_start * 100\n",
        "    prec_text = \"improved\" if precision_change < 0 else \"decreased\"\n",
        "\n",
        "    # Logic: Did the effect stabilize or shift?\n",
        "    abs_change = abs(est_end - est_start)\n",
        "    pct_change = (abs_change / abs(est_start) * 100) if est_start != 0 else 0\n",
        "\n",
        "    if pct_change < 10:\n",
        "        trend_desc = \"remained remarkably stable\"\n",
        "    elif pct_change < 30:\n",
        "        trend_desc = \"remained relatively consistent\"\n",
        "    else:\n",
        "        direction = \"increased\" if abs(est_end) > abs(est_start) else \"decreased\"\n",
        "        trend_desc = f\"gradually {direction} in magnitude\"\n",
        "\n",
        "    # Logic: Heterogeneity Trend\n",
        "    i2_start, i2_end = start['I2'], end['I2']\n",
        "    het_trend = \"increased\" if i2_end > i2_start + 5 else \"decreased\" if i2_end < i2_start - 5 else \"remained stable\"\n",
        "\n",
        "    # Build the Text\n",
        "    html = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff;'>\n",
        "\n",
        "    <h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px;'>Cumulative Meta-Analysis Results</h3>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    A cumulative meta-analysis was conducted to examine the temporal evolution of the pooled effect size and assess the sufficiency of the evidence. Studies were added to the analysis in chronological order based on publication year.\n",
        "    </p>\n",
        "\n",
        "    <h4 style='color: #34495e; margin-top: 20px;'>Evolution of Effect Size</h4>\n",
        "    <p style='text-align: justify;'>\n",
        "    The analysis began with the earliest available studies (k={k_start}, Year: {year_start}), yielding an initial pooled effect of {est_start:.3f} ({ci_pct:.0f}% CI [{start['ci_lower']:.3f}, {start['ci_upper']:.3f}]).\n",
        "    As evidence accumulated over the subsequent years (Total k={k_end}, Range: {year_start}–{year_end}), the pooled estimate <b>{trend_desc}</b>, resulting in a final estimate of <b>{est_end:.3f}</b> (95% CI [{end['ci_lower']:.3f}, {end['ci_upper']:.3f}]).\n",
        "    </p>\n",
        "\n",
        "    <h4 style='color: #34495e; margin-top: 20px;'>Precision and Heterogeneity</h4>\n",
        "    <p style='text-align: justify;'>\n",
        "    The precision of the estimate <b>{prec_text}</b> over time, as indicated by a {abs(precision_change):.1f}% {prec_text[:-1]} in the width of the {ci_pct:.0f}% confidence interval.\n",
        "    Meanwhile, the between-study heterogeneity (<i>I</i>²) {het_trend} from {i2_start:.1f}% in the initial wave to {i2_end:.1f}% in the final analysis.\n",
        "    </p>\n",
        "\n",
        "    <div style='background-color: #ecf0f1; padding: 15px; border-left: 4px solid #3498db; margin-top: 20px;'>\n",
        "    <h4 style='margin-top: 0; color: #2c3e50;'>Interpretation Guidance:</h4>\n",
        "    <ul style='margin-bottom: 0;'>\n",
        "    <li><b>Stability:</b> If the line flattens out in recent years, the finding is robust. If it is still moving up or down, more research is needed.</li>\n",
        "    <li><b>Sufficiency:</b> Narrowing confidence intervals suggest that we are converging on the \"true\" effect size.</li>\n",
        "    </ul>\n",
        "    </div>\n",
        "    </div>\"\"\"\n",
        "\n",
        "    return html\n",
        "\n",
        "def generate_methods_text_cumulative(es_config, cumulative_results, agg_method):\n",
        "    \"\"\"\n",
        "    Generates a 'Materials and Methods' section for Cumulative Meta-Analysis.\n",
        "    \"\"\"\n",
        "    import sys\n",
        "\n",
        "    # 1. Extract Settings\n",
        "    es_label = es_config.get('effect_label', 'Effect Size')\n",
        "    n_steps = len(cumulative_results)\n",
        "    start_year = cumulative_results['year'].min()\n",
        "    end_year = cumulative_results['year'].max()\n",
        "\n",
        "    # 2. Define Citation Database\n",
        "    db = {\n",
        "        'lau': \"Lau, J., Antman, E. M., Jimenez-Silva, J., Kupelnick, B., Mosteller, F., & Chalmers, T. C. (1992). Cumulative meta-analysis of therapeutic trials for myocardial infarction. <i>New England Journal of Medicine</i>, 327(4), 248-254.\",\n",
        "        'reml': \"Viechtbauer, W. (2005). Bias and efficiency of meta-analytic variance estimators in the random-effects model. <i>Journal of Educational and Behavioral Statistics</i>, 30(3), 261-293.\",\n",
        "        'borenstein': \"Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2009). <i>Introduction to Meta-Analysis</i>. Chichester, UK: John Wiley & Sons.\",\n",
        "        'tool': \"<b>[CITATION FOR THIS TOOL]:</b> Author, A. A. (202X). <i>Co-Meta: An Interactive Pipeline for Meta-Analysis</i>. [Repository/DOI].\"\n",
        "    }\n",
        "\n",
        "    # 3. Build Aggregation Description\n",
        "    if agg_method == 'study':\n",
        "        agg_desc = \"Prior to the cumulative analysis, effect sizes were aggregated at the study level to ensure independence. A fixed-effect weighted mean was calculated for studies reporting multiple outcomes.\"\n",
        "    else:\n",
        "        agg_desc = \"Effect sizes were treated as independent observations for the cumulative analysis.\"\n",
        "\n",
        "    # 4. Build HTML\n",
        "    html = f\"\"\"<div style='font-family: \"Times New Roman\", Times, serif; font-size: 12pt; line-height: 1.8; padding: 20px; background-color: #ffffff; border: 1px solid #eee; margin-bottom: 20px;'>\n",
        "\n",
        "    <h3 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-top: 0;'>Materials and Methods</h3>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Cumulative Meta-Analysis.</b> To evaluate the accumulation of evidence over time and assess the sufficiency and stability of the pooled effect size, we performed a cumulative meta-analysis [1].\n",
        "    Studies were sorted chronologically by publication year (Range: {int(start_year)}–{int(end_year)}), and a new meta-analysis was performed each time a new study was added to the pool.\n",
        "    </p>\n",
        "\n",
        "    <p style='text-align: justify;'>\n",
        "    <b>Model Specification.</b> {agg_desc}\n",
        "    For each of the {n_steps} cumulative steps, a random-effects model was fitted using the Restricted Maximum Likelihood (REML) estimator [2] to calculate the updated pooled effect and {ci_pct:.0f}% confidence interval.\n",
        "    This approach allows for the visualization of how the precision of the estimate evolves and whether the effect size stabilizes as sample size increases [3].\n",
        "    All analyses were performed using the Co-Meta toolkit [4].\n",
        "    </p>\n",
        "\n",
        "    <h4 style='color: #34495e; margin-top: 20px; font-size: 11pt;'>References</h4>\n",
        "    <ol style='font-size: 10pt; color: #555;'>\n",
        "        <li>{db['lau']}</li>\n",
        "        <li>{db['reml']}</li>\n",
        "        <li>{db['borenstein']}</li>\n",
        "        <li>{db['tool']}</li>\n",
        "    </ol>\n",
        "    </div>\"\"\"\n",
        "\n",
        "    return html\n",
        "\n",
        "# --- 3. CALCULATION ENGINE (REML VERSION) ---\n",
        "def calculate_cumulative_stats(df, effect_col, var_col, year_col='year', sort_dir='ascending'):\n",
        "    \"\"\"Iterative meta-analysis engine using REML\"\"\"\n",
        "\n",
        "    # Sort\n",
        "    df_sorted = df.sort_values(year_col, ascending=(sort_dir == 'ascending')).reset_index(drop=True)\n",
        "\n",
        "    results = []\n",
        "    n_steps = len(df_sorted)\n",
        "\n",
        "    # We need at least 2 studies to start a meta-analysis\n",
        "    for i in range(2, n_steps + 1):\n",
        "        current_slice = df_sorted.iloc[:i].copy()\n",
        "        k = len(current_slice)\n",
        "\n",
        "        # --- ESTIMATOR UPGRADE: REML ---\n",
        "        # We use the global calculate_tau_squared function (defined in Cell 4.5)\n",
        "        # Fallback to DL only if REML fails or function missing\n",
        "        try:\n",
        "            if 'calculate_tau_squared' in globals():\n",
        "                tau2, _ = calculate_tau_squared(current_slice, effect_col, var_col, method='REML')\n",
        "            else:\n",
        "                # Fallback DL logic\n",
        "                wi = 1 / current_slice[var_col]\n",
        "                mu_fe = np.sum(wi * current_slice[effect_col]) / np.sum(wi)\n",
        "                Q = np.sum(wi * (current_slice[effect_col] - mu_fe)**2)\n",
        "                tau2 = max(0, (Q - (k-1)) / (np.sum(wi) - np.sum(wi**2)/np.sum(wi)))\n",
        "        except:\n",
        "            tau2 = 0\n",
        "\n",
        "        # Random Effects Pooling\n",
        "        wi_re = 1 / (current_slice[var_col] + tau2)\n",
        "        sum_wi_re = np.sum(wi_re)\n",
        "        mu_re = np.sum(wi_re * current_slice[effect_col]) / sum_wi_re\n",
        "        se_re = np.sqrt(1 / sum_wi_re)\n",
        "\n",
        "        # Heterogeneity (I2)\n",
        "        wi = 1 / current_slice[var_col]\n",
        "        mu_fe = np.sum(wi * current_slice[effect_col]) / np.sum(wi)\n",
        "        Q = np.sum(wi * (current_slice[effect_col] - mu_fe)**2)\n",
        "        I2 = max(0, (Q - (k - 1)) / Q * 100) if Q > 0 else 0\n",
        "\n",
        "        # Confidence Interval\n",
        "        if 'ANALYSIS_CONFIG' in globals():\n",
        "            gs = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "            alpha = gs.get('alpha', 0.05)\n",
        "            dist_type = gs.get('dist_type', 'norm')\n",
        "        else:\n",
        "            alpha = 0.05\n",
        "            dist_type = 'norm'\n",
        "\n",
        "        ci_pct = (1 - alpha) * 100\n",
        "        q_val = 1 - (alpha / 2)\n",
        "\n",
        "        # 2. Determine Critical Value\n",
        "        if dist_type == 't':\n",
        "            # Degrees of freedom = k - 1 (Standard for cumulative steps)\n",
        "            df_cum = max(1, k - 1)\n",
        "            crit_val = t.ppf(q_val, df_cum)\n",
        "        else:\n",
        "            # Normal distribution (Z)\n",
        "            crit_val = norm.ppf(q_val)\n",
        "\n",
        "        # 3. Calculate CI\n",
        "        ci_lower = mu_re - crit_val * se_re\n",
        "        ci_upper = mu_re + crit_val * se_re\n",
        "\n",
        "        # Current Year/ID adding\n",
        "        current_year = current_slice.iloc[-1][year_col]\n",
        "        current_id = current_slice.iloc[-1]['id']\n",
        "\n",
        "        results.append({\n",
        "            'step': i,\n",
        "            'year': int(current_year) if pd.notna(current_year) else \"N/A\",\n",
        "            'study_added': current_id,\n",
        "            'k_studies': k,\n",
        "            'pooled_effect': mu_re,\n",
        "            'se': se_re,\n",
        "            'ci_lower': ci_lower,\n",
        "            'ci_upper': ci_upper,\n",
        "            'tau2': tau2,\n",
        "            'I2': I2\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# --- 4. MAIN EXECUTION ---\n",
        "def run_cumulative_analysis(b):\n",
        "    # Clear previous outputs\n",
        "    tab_results.clear_output()\n",
        "    tab_data.clear_output()\n",
        "\n",
        "    # Checks\n",
        "    if 'ANALYSIS_CONFIG' not in globals(): return\n",
        "    if 'analysis_data' in ANALYSIS_CONFIG:\n",
        "        data = ANALYSIS_CONFIG['analysis_data'].copy()\n",
        "    elif 'data_filtered' in globals():\n",
        "        data = data_filtered.copy()\n",
        "    else:\n",
        "        return\n",
        "\n",
        "    eff_col = ANALYSIS_CONFIG.get('effect_col', 'hedges_g')\n",
        "    var_col = ANALYSIS_CONFIG.get('var_col', 'Vg')\n",
        "\n",
        "    # Validate Year\n",
        "    if 'year' not in data.columns:\n",
        "        with tab_results: display(HTML(\"<div style='color:red'>❌ Error: 'year' column missing.</div>\"))\n",
        "        return\n",
        "\n",
        "    data['year'] = pd.to_numeric(data['year'], errors='coerce')\n",
        "    data = data.dropna(subset=['year', eff_col, var_col])\n",
        "\n",
        "    if len(data) < 3:\n",
        "        with tab_results: display(HTML(\"<div style='color:red'>❌ Error: Not enough data (need ≥3 studies).</div>\"))\n",
        "        return\n",
        "\n",
        "    # --- PRE-PROCESSING (Aggregation) ---\n",
        "    if agg_method_widget.value == 'study':\n",
        "        # Weighted average within studies first\n",
        "        data['wi'] = 1 / data[var_col]\n",
        "        def agg_study(x):\n",
        "            sum_w = x['wi'].sum()\n",
        "            return pd.Series({\n",
        "                eff_col: (x[eff_col] * x['wi']).sum() / sum_w,\n",
        "                var_col: 1 / sum_w,\n",
        "                'year': x['year'].min() # Assume study year is the earliest year if multiple\n",
        "            })\n",
        "\n",
        "        try:\n",
        "            df_calc = data.groupby('id').apply(agg_study, include_groups=False).reset_index()\n",
        "        except TypeError:\n",
        "            df_calc = data.groupby('id').apply(agg_study).reset_index()\n",
        "    else:\n",
        "        df_calc = data.copy()\n",
        "\n",
        "    # --- CALCULATION ---\n",
        "    try:\n",
        "        res_df = calculate_cumulative_stats(\n",
        "            df_calc, eff_col, var_col,\n",
        "            year_col='year',\n",
        "            sort_dir=sort_order_widget.value\n",
        "        )\n",
        "\n",
        "        # Save to global config\n",
        "        ANALYSIS_CONFIG['cumulative_results'] = res_df\n",
        "\n",
        "        # --- TAB 1: SUMMARY ---\n",
        "        with tab_results:\n",
        "            start_row = res_df.iloc[0]\n",
        "            end_row = res_df.iloc[-1]\n",
        "\n",
        "            html = f\"\"\"\n",
        "            <div style='padding:10px;'>\n",
        "                <h3 style='color:#2E86AB; margin-top:0;'>Analysis Complete</h3>\n",
        "                <div style='display:flex; gap:20px;'>\n",
        "                    <div style='flex:1; background:#f8f9fa; padding:15px; border-radius:5px; border-left:4px solid #6c757d;'>\n",
        "                        <h4 style='margin:0; color:#666;'>Start (k={int(start_row['k_studies'])})</h4>\n",
        "                        <div style='font-size:18px; font-weight:bold; margin-top:5px;'>{start_row['pooled_effect']:.3f}</div>\n",
        "                        <div style='font-size:12px;'>{ci_pct:.0f}% CI [{start_row['ci_lower']:.3f}, {start_row['ci_upper']:.3f}]</div>\n",
        "                        <div style='font-size:12px; color:#666;'>Year: {start_row['year']}</div>\n",
        "                    </div>\n",
        "                    <div style='flex:0.2; display:flex; align-items:center; justify-content:center; font-size:24px; color:#ccc;'>\n",
        "                        ➔\n",
        "                    </div>\n",
        "                    <div style='flex:1; background:#e7f3ff; padding:15px; border-radius:5px; border-left:4px solid #007bff;'>\n",
        "                        <h4 style='margin:0; color:#0056b3;'>End (k={int(end_row['k_studies'])})</h4>\n",
        "                        <div style='font-size:18px; font-weight:bold; margin-top:5px; color:#0056b3;'>{end_row['pooled_effect']:.3f}</div>\n",
        "                        <div style='font-size:12px; color:#0056b3;'>{ci_pct:.0f}% CI [{end_row['ci_lower']:.3f}, {end_row['ci_upper']:.3f}]</div>\n",
        "                        <div style='font-size:12px; color:#0056b3;'>Year: {end_row['year']}</div>\n",
        "                    </div>\n",
        "                </div>\n",
        "                <p style='margin-top:15px; color:#555;'>\n",
        "                    <b>Method:</b> Iterative REML (Consistent with Overall Analysis).\n",
        "                </p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "            display(HTML(html))\n",
        "\n",
        "        # --- TAB 2: DATA ---\n",
        "        with tab_data:\n",
        "            display(HTML(\"<h4 style='margin-top:0;'>Step-by-Step Evolution</h4>\"))\n",
        "            disp_df = res_df.copy()\n",
        "            cols = ['step', 'year', 'study_added', 'pooled_effect', 'ci_lower', 'ci_upper', 'I2', 'tau2']\n",
        "            display(disp_df[cols].style.format({\n",
        "                'pooled_effect': '{:.3f}', 'ci_lower': '{:.3f}', 'ci_upper': '{:.3f}',\n",
        "                'I2': '{:.1f}%', 'tau2': '{:.4f}'\n",
        "            }).background_gradient(subset=['pooled_effect'], cmap='Blues'))\n",
        "\n",
        "        # --- TAB 3: PUBLICATION TEXT ---\n",
        "\n",
        "        with tab_publication:\n",
        "            clear_output()\n",
        "\n",
        "            # A. Generate METHODS Text (New)\n",
        "            methods_html = generate_methods_text_cumulative(\n",
        "                ANALYSIS_CONFIG.get('es_config', {}),\n",
        "                res_df,\n",
        "                agg_method_widget.value\n",
        "            )\n",
        "\n",
        "            # B. Generate RESULTS Text (Existing)\n",
        "            # Get label safely\n",
        "            label = \"Effect Size\"\n",
        "            if 'ANALYSIS_CONFIG' in globals() and 'es_config' in ANALYSIS_CONFIG:\n",
        "                label = ANALYSIS_CONFIG['es_config'].get('effect_label', 'Effect Size')\n",
        "\n",
        "            results_html = generate_cumulative_publication_text(res_df, label)\n",
        "\n",
        "            # C. Display Both\n",
        "            display(HTML(methods_html))\n",
        "            display(HTML(results_html))\n",
        "\n",
        "            # D. Save Combined Text for Audit Report\n",
        "            ANALYSIS_CONFIG['cumulative_text'] = methods_html + \"<br><hr><br>\" + results_html\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        with tab_results:\n",
        "            display(HTML(f\"<div style='color:red'>❌ Calculation Error: {e}</div>\"))\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "# --- 5. EXPORT BUTTON ---\n",
        "with tab_export:\n",
        "    display(HTML(\"<h3 style='color: #2E86AB;'>💾 Download Audit Report</h3>\"))\n",
        "\n",
        "    btn_cum_export = widgets.Button(description=\"📥 Download Cumulative Report\", button_style='info', icon='file-excel', layout=widgets.Layout(width='300px', height='40px'))\n",
        "\n",
        "    def on_cum_export_click(b):\n",
        "        export_analysis_report(report_type='cumulative', filename_prefix='Cumulative_Meta_Analysis')\n",
        "\n",
        "    btn_cum_export.on_click(on_cum_export_click)\n",
        "    display(btn_cum_export)\n",
        "\n",
        "# --- 6. DISPLAY ---\n",
        "run_btn = widgets.Button(description='▶ Run Cumulative Analysis (REML)', button_style='success', layout=widgets.Layout(width='450px', height='50px'), style={'font_weight':'bold'})\n",
        "run_btn.on_click(run_cumulative_analysis)\n",
        "\n",
        "display(widgets.HTML(\"<h3 style='color:#2E86AB;'>📈 Cumulative Meta-Analysis (Part 1: Calculation)</h3>\"))\n",
        "display(widgets.VBox([widgets.HTML(\"<b>Settings:</b>\"), sort_order_widget, agg_method_widget, widgets.HTML(\"<hr>\"), run_btn, widgets.HTML(\"<br>\"), tabs]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "u18kAs_SgMMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 📊 Cumulative Plot Generator\n",
        "# =============================================================================\n",
        "# CELL 14b: CUMULATIVE FOREST PLOT (VISUALIZATION)\n",
        "# Purpose: Generate publication-ready plots of temporal trends.\n",
        "# Features: Dual-axis support (Effect Size vs. I2), custom styling, and export.\n",
        "# Dependencies: Cell 14 (Calculation)\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. INITIALIZATION ---\n",
        "default_title = \"Cumulative Meta-Analysis\"\n",
        "default_xlabel = \"Publication Year\"\n",
        "default_ylabel = \"Pooled Effect Size\"\n",
        "\n",
        "try:\n",
        "    if 'ANALYSIS_CONFIG' in globals():\n",
        "        es_config = ANALYSIS_CONFIG.get('es_config', {})\n",
        "        default_ylabel = f\"Pooled {es_config.get('effect_label', 'Effect Size')}\"\n",
        "except: pass\n",
        "\n",
        "# --- 2. WIDGET DEFINITIONS ---\n",
        "\n",
        "# === TAB 1: STYLE ===\n",
        "style_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>🎨 Style & Dimensions</h3>\")\n",
        "width_widget = widgets.FloatSlider(value=10.0, min=6.0, max=16.0, step=0.5, description='Width (in):', continuous_update=False, layout=widgets.Layout(width='450px'))\n",
        "height_widget = widgets.FloatSlider(value=6.0, min=4.0, max=12.0, step=0.5, description='Height (in):', continuous_update=False, layout=widgets.Layout(width='450px'))\n",
        "title_font_widget = widgets.IntSlider(value=14, min=8, max=24, description='Title Font:', layout=widgets.Layout(width='450px'))\n",
        "label_font_widget = widgets.IntSlider(value=12, min=8, max=18, description='Label Font:', layout=widgets.Layout(width='450px'))\n",
        "grid_widget = widgets.Checkbox(value=True, description='Show Grid', indent=False)\n",
        "\n",
        "style_tab = widgets.VBox([\n",
        "    style_header,\n",
        "    preset_widget,\n",
        "    widgets.HTML(\"<b>Plot Size:</b>\"), width_widget, height_widget,\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    widgets.HTML(\"<b>Typography:</b>\"), title_font_widget, label_font_widget,\n",
        "    widgets.HTML(\"<hr>\"), grid_widget\n",
        "])\n",
        "\n",
        "# === TAB 2: TEXT ===\n",
        "text_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>📝 Labels</h3>\")\n",
        "show_title_widget = widgets.Checkbox(value=True, description='Show Title', indent=False)\n",
        "title_text_widget = widgets.Text(value=default_title, description='Title:', layout=widgets.Layout(width='450px'))\n",
        "xlabel_widget = widgets.Text(value=default_xlabel, description='X-Axis:', layout=widgets.Layout(width='450px'))\n",
        "ylabel_widget = widgets.Text(value=default_ylabel, description='Y-Axis:', layout=widgets.Layout(width='450px'))\n",
        "\n",
        "text_tab = widgets.VBox([\n",
        "    text_header,\n",
        "    show_title_widget, title_text_widget,\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    xlabel_widget, ylabel_widget\n",
        "])\n",
        "\n",
        "# === TAB 3: PRIMARY AXIS (Effect Size) ===\n",
        "vis_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>📈 Primary Axis (Effect Size)</h3>\")\n",
        "line_color_widget = widgets.Dropdown(options=['blue', 'black', 'red', 'green', 'purple', 'navy'], value='navy', description='Line Color:')\n",
        "line_width_widget = widgets.FloatSlider(value=2.5, min=0.5, max=5.0, step=0.5, description='Line Width:')\n",
        "marker_style_widget = widgets.Dropdown(options=[('Circle', 'o'), ('Square', 's'), ('Diamond', 'D'), ('None', 'None')], value='o', description='Marker:')\n",
        "show_ci_widget = widgets.Checkbox(value=True, description='Show Confidence Band', indent=False)\n",
        "ci_alpha_widget = widgets.FloatSlider(value=0.2, min=0.05, max=0.5, step=0.05, description='CI Opacity:')\n",
        "show_null_widget = widgets.Checkbox(value=True, description='Show Null Line (y=0)', indent=False)\n",
        "\n",
        "vis_tab = widgets.VBox([\n",
        "    vis_header,\n",
        "    line_color_widget, line_width_widget, marker_style_widget,\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    show_ci_widget, ci_alpha_widget, show_null_widget\n",
        "])\n",
        "\n",
        "# === TAB 4: SECONDARY AXIS (Heterogeneity) ===\n",
        "het_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>📉 Secondary Axis (Heterogeneity)</h3>\")\n",
        "show_i2_widget = widgets.Checkbox(value=True, description='Show I² Trajectory (Right Axis)', indent=False)\n",
        "i2_color_widget = widgets.Dropdown(options=['orange', 'red', 'gray', 'brown', 'teal'], value='orange', description='I² Color:')\n",
        "i2_style_widget = widgets.Dropdown(options=[('Dashed', '--'), ('Dotted', ':'), ('Solid', '-')], value='--', description='Line Style:')\n",
        "i2_alpha_widget = widgets.FloatSlider(value=0.8, min=0.1, max=1.0, description='Opacity:')\n",
        "\n",
        "het_tab = widgets.VBox([\n",
        "    het_header,\n",
        "    show_i2_widget,\n",
        "    i2_color_widget, i2_style_widget, i2_alpha_widget\n",
        "])\n",
        "\n",
        "# === TAB 5: EXPORT ===\n",
        "exp_header = widgets.HTML(\"<h3 style='color: #2E86AB;'>💾 Export Options</h3>\")\n",
        "save_pdf_widget = widgets.Checkbox(value=True, description='Save as PDF', indent=False)\n",
        "save_png_widget = widgets.Checkbox(value=True, description='Save as PNG', indent=False)\n",
        "png_dpi_widget = widgets.IntSlider(value=300, min=100, max=600, step=50, description='PNG DPI:')\n",
        "filename_widget = widgets.Text(value='Cumulative_Trend_Plot', description='Filename:', layout=widgets.Layout(width='300px'))\n",
        "\n",
        "exp_tab = widgets.VBox([\n",
        "    exp_header,\n",
        "    save_pdf_widget, save_png_widget, png_dpi_widget, filename_widget\n",
        "])\n",
        "\n",
        "# Assemble Tabs\n",
        "tabs = widgets.Tab(children=[style_tab, text_tab, vis_tab, het_tab, exp_tab])\n",
        "tabs.set_title(0, '🎨 Style'); tabs.set_title(1, '📝 Text'); tabs.set_title(2, '📈 Effect')\n",
        "tabs.set_title(3, '📉 I²'); tabs.set_title(4, '💾 Export')\n",
        "\n",
        "# --- 3. PLOTTING FUNCTION ---\n",
        "plot_output = widgets.Output()\n",
        "\n",
        "def generate_cum_plot(b):\n",
        "    with plot_output:\n",
        "        clear_output(wait=True)\n",
        "        print(\"⏳ Generating Plot...\")\n",
        "\n",
        "        try:\n",
        "            # 1. Load Data\n",
        "            if 'ANALYSIS_CONFIG' not in globals() or 'cumulative_results' not in ANALYSIS_CONFIG:\n",
        "                print(\"❌ Error: Please run the Calculation Cell (Cell 14) first.\")\n",
        "                return\n",
        "\n",
        "            df = ANALYSIS_CONFIG['cumulative_results'].copy()\n",
        "\n",
        "            if df.empty:\n",
        "                print(\"❌ Error: Cumulative results table is empty.\")\n",
        "                return\n",
        "\n",
        "            # 2. Setup Figure\n",
        "            fig, ax1 = plt.subplots(figsize=(width_widget.value, height_widget.value))\n",
        "\n",
        "            # Data Vectors\n",
        "            x_data = df['year']\n",
        "            y_data = df['pooled_effect']\n",
        "\n",
        "            # 3. Primary Axis (Effect Size)\n",
        "            color_main = line_color_widget.value\n",
        "            marker = marker_style_widget.value if marker_style_widget.value != 'None' else None\n",
        "\n",
        "            # Plot Main Line\n",
        "            line1, = ax1.plot(x_data, y_data, color=color_main, linewidth=line_width_widget.value,\n",
        "                             marker=marker, markersize=8, label='Pooled Effect', zorder=10)\n",
        "\n",
        "            # Confidence Band\n",
        "            if show_ci_widget.value:\n",
        "                # Get dynamic label\n",
        "                gs = ANALYSIS_CONFIG.get('global_settings', {})\n",
        "                alpha_val = gs.get('alpha', 0.05)\n",
        "                ci_pct = (1 - alpha_val) * 100\n",
        "\n",
        "                ax1.fill_between(x_data, df['ci_lower'], df['ci_upper'],\n",
        "                                color=color_main, alpha=ci_alpha_widget.value, label=f'{ci_pct:.0f}% CI', zorder=5)\n",
        "\n",
        "\n",
        "            # Null Line\n",
        "            if show_null_widget.value:\n",
        "                null_val = ANALYSIS_CONFIG.get('es_config', {}).get('null_value', 0)\n",
        "                ax1.axhline(null_val, color='gray', linestyle='-', linewidth=1.5, alpha=0.5, zorder=1)\n",
        "\n",
        "            # 4. Secondary Axis (Heterogeneity)\n",
        "            lines = [line1]\n",
        "\n",
        "            if show_i2_widget.value:\n",
        "                ax2 = ax1.twinx()\n",
        "                color_i2 = i2_color_widget.value\n",
        "\n",
        "                line2, = ax2.plot(x_data, df['I2'], color=color_i2, linestyle=i2_style_widget.value,\n",
        "                                 linewidth=line_width_widget.value, alpha=i2_alpha_widget.value,\n",
        "                                 label='Heterogeneity (I²)', zorder=8)\n",
        "\n",
        "                ax2.set_ylabel('Heterogeneity (I² %)', color=color_i2,\n",
        "                              fontsize=label_font_widget.value, fontweight='bold')\n",
        "                ax2.tick_params(axis='y', labelcolor=color_i2, labelsize=10)\n",
        "                ax2.set_ylim(0, 100)\n",
        "                ax2.spines['right'].set_color(color_i2)\n",
        "                ax2.spines['right'].set_linewidth(2)\n",
        "\n",
        "                lines.append(line2)\n",
        "\n",
        "            # 5. Formatting\n",
        "            ax1.set_xlabel(xlabel_widget.value, fontsize=label_font_widget.value, fontweight='bold')\n",
        "            ax1.set_ylabel(ylabel_widget.value, fontsize=label_font_widget.value, fontweight='bold', color='black')\n",
        "            ax1.tick_params(axis='both', labelsize=10)\n",
        "\n",
        "            if show_title_widget.value:\n",
        "                ax1.set_title(title_text_widget.value, fontsize=title_font_widget.value, fontweight='bold', pad=15)\n",
        "\n",
        "            if grid_widget.value:\n",
        "                ax1.grid(True, linestyle=':', alpha=0.4)\n",
        "\n",
        "            # Legend logic (Combine handles from both axes if needed)\n",
        "            labels = [l.get_label() for l in lines]\n",
        "            ax1.legend(lines, labels, loc='upper left', frameon=True, fancybox=True, fontsize=10)\n",
        "\n",
        "            # Force integer ticks for Year if range is small\n",
        "            if df['year'].nunique() < 15:\n",
        "                ax1.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # 6. Export\n",
        "            ts = datetime.datetime.now().strftime(\"%H%M\")\n",
        "            fn = filename_widget.value\n",
        "\n",
        "            print(f\"✅ Plot Generated. Range: {df['year'].min()} - {df['year'].max()} (k={len(df)})\")\n",
        "\n",
        "            if save_pdf_widget.value:\n",
        "                plt.savefig(f\"{fn}_{ts}.pdf\", bbox_inches='tight')\n",
        "                print(f\"💾 Saved PDF: {fn}_{ts}.pdf\")\n",
        "\n",
        "            if save_png_widget.value:\n",
        "                plt.savefig(f\"{fn}_{ts}.png\", dpi=png_dpi_widget.value, bbox_inches='tight')\n",
        "                print(f\"💾 Saved PNG: {fn}_{ts}.png\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Plotting Error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "# --- 4. DISPLAY ---\n",
        "run_btn = widgets.Button(description='📊 Generate Cumulative Plot', button_style='success',\n",
        "                        layout=widgets.Layout(width='400px', height='50px'), style={'font_weight':'bold'})\n",
        "run_btn.on_click(generate_cum_plot)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='color: #2E86AB;'>📊 Cumulative Plot Generator</h3>\"),\n",
        "    widgets.HTML(\"<p style='color: #666;'>Visualize how the pooled effect (and heterogeneity) stabilizes over time.</p>\"),\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    tabs,\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    run_btn,\n",
        "    plot_output\n",
        "]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y_8dmwWgeq5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧪 R Validation: Overall 3-Level Model (Final Robust Version)\n",
        "# =============================================================================\n",
        "# CELL: OVERALL MODEL VALIDATION (MERGED)\n",
        "# Purpose: Verify 3-Level estimates against R (metafor)\n",
        "# Features:\n",
        "#   1. Supports Shared Control Matrices (Block Diagonal V)\n",
        "#   2. Supports Dynamic Inference (Z vs t distribution matching Python)\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import rpy2.robjects as ro\n",
        "from rpy2.robjects import pandas2ri\n",
        "import rpy2.robjects.numpy2ri as numpy2ri\n",
        "from scipy.linalg import block_diag\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Activate R conversions\n",
        "pandas2ri.activate()\n",
        "numpy2ri.activate()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"VALIDATION STEP 2: OVERALL MODEL (FULL COMPATIBILITY)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- 1. Dependencies Check ---\n",
        "if 'ANALYSIS_CONFIG' not in globals():\n",
        "    print(\"❌ Error: Global configuration not found. Please run the Setup cells.\")\n",
        "elif 'three_level_results' not in ANALYSIS_CONFIG:\n",
        "    print(\"⚠️ Warning: 'three_level_results' not found. Validating 2-level results instead.\")\n",
        "    py_res = ANALYSIS_CONFIG['overall_results']\n",
        "    is_multilevel = False\n",
        "else:\n",
        "    py_res = ANALYSIS_CONFIG['three_level_results']\n",
        "    is_multilevel = True\n",
        "\n",
        "# --- 2. Data Preparation ---\n",
        "try:\n",
        "    if 'analysis_data' in ANALYSIS_CONFIG:\n",
        "        df_py = ANALYSIS_CONFIG['analysis_data'].copy()\n",
        "    else:\n",
        "        df_py = data_filtered.copy()\n",
        "\n",
        "    effect_col = ANALYSIS_CONFIG['effect_col']\n",
        "    var_col = ANALYSIS_CONFIG['var_col']\n",
        "\n",
        "    # CRITICAL: Sort by ID to ensure Matrix alignment matches Data alignment\n",
        "    df_r = df_py[['id', effect_col, var_col]].dropna().sort_values('id')\n",
        "\n",
        "    # --- 3. Matrix Injection (Shared Controls) ---\n",
        "    use_vcv_matrix = False\n",
        "\n",
        "    if 'vcv_matrices' in ANALYSIS_CONFIG:\n",
        "        vcv_dict = ANALYSIS_CONFIG['vcv_matrices']\n",
        "        matrix_list = []\n",
        "        unique_ids = df_r['id'].unique()\n",
        "\n",
        "        missing_ids = [uid for uid in unique_ids if uid not in vcv_dict]\n",
        "\n",
        "        if not missing_ids:\n",
        "            for uid in unique_ids:\n",
        "                matrix_list.append(vcv_dict[uid])\n",
        "\n",
        "            # Create giant Block Diagonal Matrix\n",
        "            V_full = block_diag(*matrix_list)\n",
        "\n",
        "            ro.globalenv['V_full'] = V_full\n",
        "            use_vcv_matrix = True\n",
        "            print(f\"🔄 Shared Controls: Using Block-Diagonal Matrix ({V_full.shape[0]}x{V_full.shape[1]})\")\n",
        "        else:\n",
        "            print(f\"⚠️ Missing matrices. Using diagonal variances.\")\n",
        "\n",
        "    # --- 4. Inference Method Detection (Z vs t) ---\n",
        "    # Check what the user selected in the widget\n",
        "    use_t_dist = False\n",
        "    if 'settings' in ANALYSIS_CONFIG:\n",
        "        if ANALYSIS_CONFIG['settings'].get('dist_type') == 't':\n",
        "            use_t_dist = True\n",
        "            print(\"🔄 Inference: Matching Python's t-distribution setting\")\n",
        "        else:\n",
        "            print(\"🔄 Inference: Matching Python's Normal (z) distribution setting\")\n",
        "\n",
        "    # Pass Data & Settings to R\n",
        "    ro.globalenv['df_python'] = df_r\n",
        "    ro.globalenv['eff_col'] = effect_col\n",
        "    ro.globalenv['var_col'] = var_col\n",
        "    ro.globalenv['use_matrix'] = use_vcv_matrix\n",
        "    ro.globalenv['use_t_dist'] = use_t_dist\n",
        "\n",
        "    # --- 5. Run R (metafor) ---\n",
        "    r_script = \"\"\"\n",
        "    library(metafor)\n",
        "    dat <- df_python\n",
        "\n",
        "    dat$row_id <- 1:nrow(dat)\n",
        "    dat$study_id <- as.factor(dat$id)\n",
        "\n",
        "    # 1. Choose Variance Structure\n",
        "    if (use_matrix) {\n",
        "        V_input <- V_full\n",
        "    } else {\n",
        "        V_input <- dat[[var_col]]\n",
        "    }\n",
        "\n",
        "    # 2. Choose Test Type (z or t)\n",
        "    test_type <- ifelse(use_t_dist, \"t\", \"z\")\n",
        "\n",
        "    # 3. Run Model\n",
        "    if (length(unique(dat$study_id)) < nrow(dat)) {\n",
        "        # 3-Level Structure\n",
        "        res <- rma.mv(yi = dat[[eff_col]],\n",
        "                      V = V_input,\n",
        "                      random = ~ 1 | study_id/row_id,\n",
        "                      data = dat,\n",
        "                      method = \"REML\",\n",
        "                      test = test_type) # Dynamic Test Type\n",
        "        tau2 <- res$sigma2[1]\n",
        "        sigma2 <- res$sigma2[2]\n",
        "    } else {\n",
        "        # 2-Level Structure\n",
        "        res <- rma.mv(yi = dat[[eff_col]],\n",
        "                      V = V_input,\n",
        "                      random = ~ 1 | study_id,\n",
        "                      data = dat,\n",
        "                      method = \"REML\",\n",
        "                      test = test_type)\n",
        "        tau2 <- res$sigma2[1]\n",
        "        sigma2 <- 0\n",
        "    }\n",
        "\n",
        "    # Extract Stats\n",
        "    list(\n",
        "        b = as.numeric(res$b),\n",
        "        se = as.numeric(res$se),\n",
        "        ci_lb = res$ci.lb,\n",
        "        ci_ub = res$ci.ub,\n",
        "        tau2 = tau2,\n",
        "        sigma2 = sigma2,\n",
        "        aic = AIC(res),\n",
        "        loglik = logLik(res)\n",
        "    )\n",
        "    \"\"\"\n",
        "\n",
        "    r_out = ro.r(r_script)\n",
        "\n",
        "    # --- 6. Extract & Compare ---\n",
        "    r_vals = {\n",
        "        'Pooled Effect': r_out.rx2('b')[0],\n",
        "        'Standard Error': r_out.rx2('se')[0],\n",
        "        '95% CI Lower': r_out.rx2('ci_lb')[0],\n",
        "        '95% CI Upper': r_out.rx2('ci_ub')[0],\n",
        "        'Tau² (L3)': r_out.rx2('tau2')[0],\n",
        "        'Sigma² (L2)': r_out.rx2('sigma2')[0],\n",
        "        'AIC': r_out.rx2('aic')[0],\n",
        "        'Log-Likelihood': r_out.rx2('loglik')[0]\n",
        "    }\n",
        "\n",
        "    if is_multilevel:\n",
        "        py_vals = {\n",
        "            'Pooled Effect': py_res['pooled_effect'],\n",
        "            'Standard Error': py_res['se'],\n",
        "            '95% CI Lower': py_res['ci_lower'],\n",
        "            '95% CI Upper': py_res['ci_upper'],\n",
        "            'Tau² (L3)': py_res['tau_squared'],\n",
        "            'Sigma² (L2)': py_res['sigma_squared'],\n",
        "            'AIC': py_res.get('aic', np.nan),\n",
        "            'Log-Likelihood': 3 - py_res.get('aic', 0)/2 if 'aic' in py_res else np.nan\n",
        "        }\n",
        "    else:\n",
        "        py_vals = {\n",
        "            'Pooled Effect': py_res['pooled_effect_random'],\n",
        "            'Standard Error': py_res['pooled_SE_random_reported'],\n",
        "            '95% CI Lower': py_res['ci_lower_random_reported'],\n",
        "            '95% CI Upper': py_res['ci_upper_random_reported'],\n",
        "            'Tau² (L3)': py_res['tau_squared'],\n",
        "            'Sigma² (L2)': 0.0,\n",
        "            'AIC': py_res.get('aic_2level', np.nan),\n",
        "            'Log-Likelihood': np.nan\n",
        "        }\n",
        "\n",
        "    # Generate Table\n",
        "    comparison_data = []\n",
        "    for metric, r_val in r_vals.items():\n",
        "        py_val = py_vals.get(metric, 0)\n",
        "\n",
        "        if np.isnan(py_val) or (py_val == 0 and r_val == 0 and metric == 'Sigma² (L2)'):\n",
        "            if metric != 'Sigma² (L2)': continue\n",
        "\n",
        "        diff = abs(py_val - r_val)\n",
        "        rel_diff = (diff / abs(r_val)) * 100 if r_val != 0 else 0\n",
        "\n",
        "        # Strict check for Estimates, looser for Optimization metrics\n",
        "        if \"Effect\" in metric or \"SE\" in metric or \"CI\" in metric:\n",
        "            status = \"✅ PASS\" if diff < 1e-4 else \"❌ FAIL\"\n",
        "        else:\n",
        "            status = \"✅ PASS\" if rel_diff < 1.0 or diff < 0.05 else \"⚠️ CHECK\"\n",
        "\n",
        "        comparison_data.append({\n",
        "            'Metric': metric,\n",
        "            'Python': f\"{py_val:.5f}\",\n",
        "            'R (metafor)': f\"{r_val:.5f}\",\n",
        "            'Diff': f\"{diff:.2e}\",\n",
        "            'Status': status\n",
        "        })\n",
        "\n",
        "    df_comp = pd.DataFrame(comparison_data)\n",
        "    def color_rows(val):\n",
        "        if val == \"✅ PASS\": return 'background-color: #d4edda'\n",
        "        if val == \"⚠️ CHECK\": return 'background-color: #fff3cd'\n",
        "        return 'background-color: #f8d7da'\n",
        "\n",
        "    display(HTML(\"<h4>🧪 Robust Validation Results</h4>\"))\n",
        "    display(df_comp.style.applymap(color_rows, subset=['Status']))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Validation Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "h1Y2jtJRd4RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧪 R Validation: Spline Analysis (Prediction Comparison)\n",
        "# =============================================================================\n",
        "# CELL: SPLINE VALIDATION (PREDICTION BASED)\n",
        "# Purpose: Verify that Python and R draw the exact same curve.\n",
        "# Method: Compare Predicted Values (y_hat) instead of raw coefficients.\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import patsy\n",
        "import rpy2.robjects as ro\n",
        "from rpy2.robjects import pandas2ri\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Activate R conversion\n",
        "pandas2ri.activate()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"🧪 SYNTHETIC SPLINE VALIDATION: PREDICTION MATCHING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    # --- 1. GENERATE SINE WAVE DATA ---\n",
        "    def generate_sine_data(n=100, tau2=0.2, sigma2=0.1, seed=42):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        x = np.sort(rng.uniform(-3, 3, n)) # Sorted for nice plotting comparison\n",
        "        true_y = np.sin(x) + 0.5\n",
        "        n_studies = n // 2\n",
        "        ids = np.repeat(np.arange(n_studies), 2)\n",
        "        u = np.repeat(rng.normal(0, np.sqrt(tau2), n_studies), 2)\n",
        "        e = rng.normal(0, np.sqrt(sigma2), n)\n",
        "        y_obs = true_y + u + e\n",
        "        vi = rng.uniform(0.05, 0.15, n)\n",
        "\n",
        "        df = pd.DataFrame({'id': ids, 'y': y_obs, 'vi': vi, 'x': x})\n",
        "        df['row_id'] = np.arange(len(df))\n",
        "        return df, true_y\n",
        "\n",
        "    print(\"🎲 Generating Synthetic Sine Wave...\")\n",
        "    df_sim, true_curve = generate_sine_data(n=80)\n",
        "\n",
        "    ANALYSIS_CONFIG['analysis_data'] = df_sim\n",
        "    ANALYSIS_CONFIG['vcv_matrices'] = {}\n",
        "\n",
        "    # --- 2. RUN PYTHON SPLINE ---\n",
        "    print(\"🐍 Running Python Spline Model...\")\n",
        "    py_res, err = _run_robust_spline_analysis(\n",
        "        df_sim, 'x', 'y', 'vi', df_spline=3\n",
        "    )\n",
        "    if err: raise ValueError(f\"Python failed: {err}\")\n",
        "\n",
        "    # Calculate Python Predictions (X * Beta)\n",
        "    # We reconstruct the full X used inside the driver\n",
        "    formula = py_res['formula']\n",
        "    mod_mean, mod_std = py_res['mod_mean'], py_res['mod_std']\n",
        "    z_vals = (df_sim['x'] - mod_mean) / mod_std\n",
        "    basis_matrix = patsy.dmatrix(formula, {\"x\": z_vals}, return_type='matrix')\n",
        "    basis_matrix = np.asarray(basis_matrix)\n",
        "\n",
        "    # Full X (Intercept + Basis)\n",
        "    X_py = np.column_stack([np.ones(len(df_sim)), basis_matrix])\n",
        "    beta_py = py_res['betas']\n",
        "\n",
        "    # PREDICTION\n",
        "    y_hat_py = X_py @ beta_py\n",
        "\n",
        "    # --- 3. RUN R (METAFOR) ---\n",
        "    print(\"®️ Running R Model...\")\n",
        "    ro.globalenv['df_r'] = df_sim\n",
        "    ro.globalenv['basis_X'] = basis_matrix # Feed raw basis\n",
        "    ro.globalenv['fixed_tau2'] = py_res['tau_sq']\n",
        "    ro.globalenv['fixed_sigma2'] = py_res['sigma_sq']\n",
        "\n",
        "    r_script = \"\"\"\n",
        "    library(metafor)\n",
        "    dat <- df_r\n",
        "\n",
        "    # Let R handle the intercept naturally (it might drop a column, that's fine!)\n",
        "    res <- rma.mv(yi = y, V = vi,\n",
        "                  mods = ~ basis_X,\n",
        "                  random = ~ 1 | id/row_id,\n",
        "                  data = dat,\n",
        "                  method = \"REML\",\n",
        "                  sigma2 = c(fixed_tau2, fixed_sigma2))\n",
        "\n",
        "    # Return PREDICTED VALUES (fitted)\n",
        "    as.numeric(fitted(res))\n",
        "    \"\"\"\n",
        "\n",
        "    y_hat_r = np.array(ro.r(r_script))\n",
        "\n",
        "    # --- 4. COMPARE PREDICTIONS ---\n",
        "\n",
        "    # Calculate difference statistics\n",
        "    diffs = np.abs(y_hat_py - y_hat_r)\n",
        "    max_diff = np.max(diffs)\n",
        "    mean_diff = np.mean(diffs)\n",
        "\n",
        "    print(\"\\n📊 Prediction Comparison:\")\n",
        "    print(f\"   Max Difference:  {max_diff:.2e}\")\n",
        "    print(f\"   Mean Difference: {mean_diff:.2e}\")\n",
        "\n",
        "    # Display subset of data\n",
        "    df_comp = pd.DataFrame({\n",
        "        'x': df_sim['x'],\n",
        "        'Py_Pred': y_hat_py,\n",
        "        'R_Pred': y_hat_r,\n",
        "        'Diff': diffs\n",
        "    }).iloc[::10] # Show every 10th point\n",
        "\n",
        "    def color_status(val):\n",
        "        return 'background-color: #d4edda' if val < 1e-4 else 'background-color: #f8d7da'\n",
        "\n",
        "    display(HTML(\"<h4>🧪 Curve Matching (Sample Points)</h4>\"))\n",
        "    display(df_comp.style.format(\"{:.6f}\").applymap(color_status, subset=['Diff']))\n",
        "\n",
        "    print(\"\\n✅ Final Verdict:\")\n",
        "    if max_diff < 1e-4:\n",
        "        print(\" - SUCCESS: Python and R draw the EXACT same curve.\")\n",
        "        print(\"   (The mathematical equivalence is proven, despite coefficient differences.)\")\n",
        "    else:\n",
        "        print(\" - CHECK: Curves diverge.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KtK3wQZUaPUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧪 R Validation: Meta-Regression (VCV Matrix Support)\n",
        "# =============================================================================\n",
        "# CELL: META-REGRESSION VALIDATION\n",
        "# Purpose: Verify Meta-Regression slope and significance against R (metafor)\n",
        "# Features:\n",
        "#   - VCV Matrix support (shared controls)\n",
        "#   - Handles 3-Level, Diagonal, and Aggregated models\n",
        "#   - Proper alignment between Python and R\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import rpy2.robjects as ro\n",
        "from rpy2.robjects import pandas2ri\n",
        "import rpy2.robjects.numpy2ri as numpy2ri\n",
        "from scipy.linalg import block_diag\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Activate R conversion\n",
        "pandas2ri.activate()\n",
        "numpy2ri.activate()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"VALIDATION STEP 4: META-REGRESSION (VCV SUPPORT)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# =============================================================================\n",
        "# 1. CHECK DEPENDENCIES\n",
        "# =============================================================================\n",
        "\n",
        "if 'ANALYSIS_CONFIG' not in globals():\n",
        "    print(\"❌ Error: ANALYSIS_CONFIG not found. Run earlier cells first.\")\n",
        "elif 'meta_regression_RVE_results' not in ANALYSIS_CONFIG:\n",
        "    print(\"❌ Error: Please run Cell 10 (Meta-Regression) first.\")\n",
        "else:\n",
        "    try:\n",
        "        # =============================================================\n",
        "        # 2. GET PYTHON RESULTS\n",
        "        # =============================================================\n",
        "\n",
        "        py_res = ANALYSIS_CONFIG['meta_regression_RVE_results']\n",
        "        mod_col = str(py_res['moderator_col_name'])\n",
        "\n",
        "        # Model info\n",
        "        py_model_type = py_res.get('model_type', 'Unknown')\n",
        "        is_aggregated = \"Aggregated\" in py_model_type or \"2-Level\" in py_model_type\n",
        "\n",
        "        print(f\"ℹ️ Python Model: {py_model_type}\")\n",
        "\n",
        "        # =============================================================\n",
        "        # 3. GET AND PREPARE DATA\n",
        "        # =============================================================\n",
        "\n",
        "        if 'analysis_data' in ANALYSIS_CONFIG:\n",
        "            df_main = ANALYSIS_CONFIG['analysis_data'].copy()\n",
        "        else:\n",
        "            df_main = data_filtered.copy()\n",
        "\n",
        "        df_main.columns = df_main.columns.astype(str)\n",
        "        effect_col = str(ANALYSIS_CONFIG['effect_col'])\n",
        "        var_col = str(ANALYSIS_CONFIG['var_col'])\n",
        "        vcv_dict = ANALYSIS_CONFIG.get('vcv_matrices', {})\n",
        "\n",
        "        print(f\"🔍 Validating Moderator: '{mod_col}'\")\n",
        "\n",
        "        # Clean data\n",
        "        df_main[mod_col] = pd.to_numeric(df_main[mod_col], errors='coerce')\n",
        "        df_clean = df_main.dropna(subset=[mod_col, effect_col, var_col, 'id']).copy()\n",
        "        df_clean = df_clean[df_clean[var_col] > 0]\n",
        "\n",
        "        # Sort for alignment\n",
        "        df_clean = df_clean.sort_values(['id']).reset_index(drop=True)\n",
        "        df_clean['_row_id'] = np.arange(1, len(df_clean) + 1)\n",
        "\n",
        "        n_obs = len(df_clean)\n",
        "        n_studies = df_clean['id'].nunique()\n",
        "\n",
        "        print(f\"   Data: {n_obs} observations from {n_studies} studies\")\n",
        "\n",
        "        # =============================================================\n",
        "        # 4. BUILD VCV MATRIX (Same logic as Python)\n",
        "        # =============================================================\n",
        "\n",
        "        vcv_blocks = []\n",
        "        grouped = df_clean.groupby('id', sort=False)\n",
        "\n",
        "        for study_id, group in grouped:\n",
        "            k = len(group)\n",
        "            vi = group[var_col].values.astype(np.float64)\n",
        "\n",
        "            sid_str = str(study_id)\n",
        "            if sid_str in vcv_dict:\n",
        "                V_i = np.asarray(vcv_dict[sid_str], dtype=np.float64)\n",
        "                if V_i.shape[0] != k:\n",
        "                    V_i = np.diag(vi)\n",
        "            elif study_id in vcv_dict:\n",
        "                V_i = np.asarray(vcv_dict[study_id], dtype=np.float64)\n",
        "                if V_i.shape[0] != k:\n",
        "                    V_i = np.diag(vi)\n",
        "            else:\n",
        "                V_i = np.diag(vi)\n",
        "\n",
        "            vcv_blocks.append(V_i)\n",
        "\n",
        "        # Build block diagonal\n",
        "        try:\n",
        "            V_full = block_diag(*vcv_blocks)\n",
        "            use_vcv_matrix = True\n",
        "            has_off_diag = any(not np.allclose(m, np.diag(np.diag(m))) for m in vcv_blocks)\n",
        "            print(f\"   VCV Matrix: {V_full.shape[0]}×{V_full.shape[1]}, Off-diagonal: {has_off_diag}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ VCV construction failed: {e}, using diagonal\")\n",
        "            V_full = np.diag(df_clean[var_col].values)\n",
        "            use_vcv_matrix = True\n",
        "            has_off_diag = False\n",
        "\n",
        "        # =============================================================\n",
        "        # 5. RUN R ANALYSIS\n",
        "        # =============================================================\n",
        "\n",
        "        # Pass data to R\n",
        "        ro.globalenv['df_r'] = df_clean\n",
        "        ro.globalenv['V_matrix'] = V_full\n",
        "        ro.globalenv['eff_col'] = effect_col\n",
        "        ro.globalenv['var_col'] = var_col\n",
        "        ro.globalenv['mod_col'] = mod_col\n",
        "        ro.globalenv['use_vcv'] = use_vcv_matrix and has_off_diag\n",
        "\n",
        "        r_script = \"\"\"\n",
        "        library(metafor)\n",
        "\n",
        "        dat <- df_r\n",
        "        dat$row_id <- dat$`_row_id`\n",
        "        dat$study_id <- as.factor(dat$id)\n",
        "        dat$moderator <- as.numeric(dat[[mod_col]])\n",
        "\n",
        "        # Choose V input\n",
        "        if (use_vcv) {\n",
        "            V_input <- V_matrix\n",
        "        } else {\n",
        "            V_input <- dat[[var_col]]\n",
        "        }\n",
        "\n",
        "        tryCatch({\n",
        "            res <- rma.mv(yi = dat[[eff_col]],\n",
        "                          V = V_input,\n",
        "                          mods = ~ moderator,\n",
        "                          random = ~ 1 | study_id/row_id,\n",
        "                          data = dat,\n",
        "                          method = \"REML\",\n",
        "                          test = \"t\",\n",
        "                          control = list(optimizer = \"optim\",\n",
        "                                        optmethod = \"BFGS\",\n",
        "                                        iter.max = 5000))\n",
        "\n",
        "            list(\n",
        "                status = \"success\",\n",
        "                beta0 = as.numeric(res$b[1]),\n",
        "                beta1 = as.numeric(res$b[2]),\n",
        "                se0 = as.numeric(res$se[1]),\n",
        "                se1 = as.numeric(res$se[2]),\n",
        "                pval0 = as.numeric(res$pval[1]),\n",
        "                pval1 = as.numeric(res$pval[2]),\n",
        "                tau2 = as.numeric(res$sigma2[1]),\n",
        "                sigma2 = as.numeric(res$sigma2[2]),\n",
        "                converged = res$converged,\n",
        "                used_vcv = use_vcv\n",
        "            )\n",
        "        }, error = function(e) {\n",
        "            list(status = \"error\", msg = conditionMessage(e))\n",
        "        })\n",
        "        \"\"\"\n",
        "\n",
        "        r_res = ro.r(r_script)\n",
        "\n",
        "        if r_res.rx2('status')[0] == 'error':\n",
        "            print(f\"\\n❌ R Error: {r_res.rx2('msg')[0]}\")\n",
        "        else:\n",
        "            # =============================================================\n",
        "            # 6. EXTRACT AND COMPARE RESULTS\n",
        "            # =============================================================\n",
        "\n",
        "            # Helper for safe extraction\n",
        "            def safe_extract(r_obj, key, default=np.nan):\n",
        "                try:\n",
        "                    val = r_obj.rx2(key)\n",
        "                    if val == ro.NULL or val is None:\n",
        "                        return default\n",
        "                    return float(val[0])\n",
        "                except:\n",
        "                    return default\n",
        "\n",
        "            # R results\n",
        "            r_beta0 = safe_extract(r_res, 'beta0')\n",
        "            r_beta1 = safe_extract(r_res, 'beta1')\n",
        "            r_se0 = safe_extract(r_res, 'se0')\n",
        "            r_se1 = safe_extract(r_res, 'se1')\n",
        "            r_pval1 = safe_extract(r_res, 'pval1')\n",
        "            r_tau2 = safe_extract(r_res, 'tau2')\n",
        "            r_sigma2 = safe_extract(r_res, 'sigma2')\n",
        "            r_converged = bool(safe_extract(r_res, 'converged', 0))\n",
        "            r_used_vcv = bool(safe_extract(r_res, 'used_vcv', 0))\n",
        "\n",
        "            # Python results - handle different key names\n",
        "            py_betas = py_res.get('betas', [np.nan, np.nan])\n",
        "            py_beta0 = py_betas[0] if len(py_betas) > 0 else np.nan\n",
        "            py_beta1 = py_betas[1] if len(py_betas) > 1 else np.nan\n",
        "\n",
        "            # SE: try multiple possible keys\n",
        "            py_se_robust = py_res.get('std_errors_robust', py_res.get('se_betas_robust', []))\n",
        "            py_se_model = py_res.get('std_errors', py_res.get('se_betas', []))\n",
        "            py_ses = py_se_robust if len(py_se_robust) > 0 else py_se_model\n",
        "            py_se0 = py_ses[0] if len(py_ses) > 0 else np.nan\n",
        "            py_se1 = py_ses[1] if len(py_ses) > 1 else np.nan\n",
        "\n",
        "            # P-value\n",
        "            py_pval1 = py_res.get('p_slope', py_res.get('p_values', [np.nan, np.nan])[1] if 'p_values' in py_res else np.nan)\n",
        "\n",
        "            # Variance components\n",
        "            py_tau2 = py_res.get('tau_sq', py_res.get('tau2', np.nan))\n",
        "            py_sigma2 = py_res.get('sigma_sq', py_res.get('sigma2', np.nan))\n",
        "\n",
        "            # =============================================================\n",
        "            # 7. BUILD COMPARISON TABLE\n",
        "            # =============================================================\n",
        "\n",
        "            comparison_data = [\n",
        "                {'Metric': 'Intercept (β₀)', 'Python': py_beta0, 'R': r_beta0, 'Type': 'coef'},\n",
        "                {'Metric': 'Slope (β₁)', 'Python': py_beta1, 'R': r_beta1, 'Type': 'coef'},\n",
        "                {'Metric': 'SE(Intercept)', 'Python': py_se0, 'R': r_se0, 'Type': 'se'},\n",
        "                {'Metric': 'SE(Slope)', 'Python': py_se1, 'R': r_se1, 'Type': 'se'},\n",
        "                {'Metric': 'P-value (Slope)', 'Python': py_pval1, 'R': r_pval1, 'Type': 'pval'},\n",
        "                {'Metric': 'τ² (Between)', 'Python': py_tau2, 'R': r_tau2, 'Type': 'var'},\n",
        "                {'Metric': 'σ² (Within)', 'Python': py_sigma2, 'R': r_sigma2, 'Type': 'var'},\n",
        "            ]\n",
        "\n",
        "            df_comp = pd.DataFrame(comparison_data)\n",
        "\n",
        "            # Calculate differences\n",
        "            df_comp['Diff'] = np.abs(df_comp['Python'] - df_comp['R'])\n",
        "\n",
        "            # Status logic\n",
        "            def determine_status(row):\n",
        "                py_val, r_val, diff, metric_type = row['Python'], row['R'], row['Diff'], row['Type']\n",
        "\n",
        "                if pd.isna(py_val) or pd.isna(r_val):\n",
        "                    return '❓ N/A'\n",
        "\n",
        "                # Tolerance depends on metric type and model\n",
        "                if metric_type == 'coef':\n",
        "                    # Coefficients should match closely\n",
        "                    if diff < 1e-3:\n",
        "                        return '✅ PASS'\n",
        "                    elif diff < 0.05:\n",
        "                        return '⚠️ CLOSE'\n",
        "                    else:\n",
        "                        return '❌ FAIL'\n",
        "\n",
        "                elif metric_type == 'se':\n",
        "                    # SEs can differ for aggregated models\n",
        "                    if is_aggregated:\n",
        "                        if diff < 0.1 or (diff / (abs(r_val) + 1e-9) < 0.2):\n",
        "                            return 'ℹ️ DIFF (Aggregated)'\n",
        "                        else:\n",
        "                            return '⚠️ DIFF'\n",
        "                    else:\n",
        "                        if diff < 1e-3:\n",
        "                            return '✅ PASS'\n",
        "                        elif diff < 0.01:\n",
        "                            return '⚠️ CLOSE'\n",
        "                        else:\n",
        "                            return '❌ FAIL'\n",
        "\n",
        "                elif metric_type == 'pval':\n",
        "                    # P-values are sensitive\n",
        "                    if diff < 0.001:\n",
        "                        return '✅ PASS'\n",
        "                    elif diff < 0.05:\n",
        "                        return '⚠️ CLOSE'\n",
        "                    else:\n",
        "                        # Check if both reach same conclusion\n",
        "                        both_sig = (py_val < 0.05) == (r_val < 0.05)\n",
        "                        return '⚠️ Same conclusion' if both_sig else '❌ FAIL'\n",
        "\n",
        "                elif metric_type == 'var':\n",
        "                    # Variance components\n",
        "                    if diff < 0.01:\n",
        "                        return '✅ PASS'\n",
        "                    elif diff / (abs(r_val) + 1e-9) < 0.1:\n",
        "                        return '⚠️ CLOSE'\n",
        "                    else:\n",
        "                        return '❌ FAIL'\n",
        "\n",
        "                return '❓'\n",
        "\n",
        "            df_comp['Status'] = df_comp.apply(determine_status, axis=1)\n",
        "\n",
        "            # Format for display\n",
        "            df_display = df_comp[['Metric', 'Python', 'R', 'Diff', 'Status']].copy()\n",
        "            df_display['Python'] = df_display['Python'].apply(lambda x: f\"{x:.4f}\" if pd.notna(x) else \"N/A\")\n",
        "            df_display['R'] = df_display['R'].apply(lambda x: f\"{x:.4f}\" if pd.notna(x) else \"N/A\")\n",
        "            df_display['Diff'] = df_display['Diff'].apply(lambda x: f\"{x:.2e}\" if pd.notna(x) else \"N/A\")\n",
        "\n",
        "            # =============================================================\n",
        "            # 8. DISPLAY RESULTS\n",
        "            # =============================================================\n",
        "\n",
        "            # Summary\n",
        "            n_pass = df_comp['Status'].str.contains('PASS', na=False).sum()\n",
        "            n_fail = df_comp['Status'].str.contains('FAIL', na=False).sum()\n",
        "            n_close = df_comp['Status'].str.contains('CLOSE|DIFF', na=False).sum()\n",
        "\n",
        "            print(f\"\\n📊 Summary: {n_pass} PASS | {n_close} CLOSE/DIFF | {n_fail} FAIL\")\n",
        "            print(f\"   R converged: {r_converged}, Used VCV: {r_used_vcv}\")\n",
        "\n",
        "            # Styling\n",
        "            def style_status(val):\n",
        "                if 'PASS' in str(val):\n",
        "                    return 'background-color: #d4edda'\n",
        "                elif 'FAIL' in str(val):\n",
        "                    return 'background-color: #f8d7da'\n",
        "                elif 'CLOSE' in str(val) or 'DIFF' in str(val):\n",
        "                    return 'background-color: #fff3cd'\n",
        "                return ''\n",
        "\n",
        "            display(HTML(\"<h4>🧪 Meta-Regression Validation Results</h4>\"))\n",
        "            styled = df_display.style.applymap(style_status, subset=['Status'])\n",
        "            styled = styled.set_table_styles([{'selector': 'th', 'props': [('text-align', 'left')]}])\n",
        "            display(styled)\n",
        "\n",
        "            # Notes\n",
        "            if is_aggregated:\n",
        "                display(HTML(\"\"\"\n",
        "                <div style='background-color: #e7f3ff; padding: 10px; border-radius: 5px; margin-top: 10px;'>\n",
        "                    <strong>ℹ️ Note:</strong> Python used a <em>2-Level Aggregated</em> model because the moderator\n",
        "                    was constant within studies. This is statistically appropriate but will produce slightly\n",
        "                    different SEs and variance components than R's full 3-level model.<br>\n",
        "                    <strong>Key check:</strong> Slope (β₁) should match closely.\n",
        "                </div>\n",
        "                \"\"\"))\n",
        "\n",
        "            if not r_converged:\n",
        "                display(HTML(\"\"\"\n",
        "                <div style='background-color: #fff3cd; padding: 10px; border-radius: 5px; margin-top: 10px;'>\n",
        "                    <strong>⚠️ Warning:</strong> R's optimizer did not converge. Results may be unreliable.\n",
        "                </div>\n",
        "                \"\"\"))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Validation Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WbJEx7MbCZSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧪 R Validation: Subgroup Analysis (FIXED ALIGNMENT)\n",
        "# =============================================================================\n",
        "# CELL: SUBGROUP VALIDATION (FULLY SYNCHRONIZED & ROBUST)\n",
        "# Purpose: Validate Subgroup Estimates against R.\n",
        "# FIXES:\n",
        "#   1. Explicit sorting in BOTH Python and R by (id, then original order)\n",
        "#   2. VCV matrix built from R's perspective to guarantee alignment\n",
        "#   3. Added data integrity checks\n",
        "#   4. Better diagnostic output on failure\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import rpy2.robjects as ro\n",
        "from rpy2.robjects import pandas2ri\n",
        "import rpy2.robjects.numpy2ri as numpy2ri\n",
        "from scipy.linalg import block_diag\n",
        "from IPython.display import display, HTML\n",
        "import warnings\n",
        "\n",
        "# Activate R conversion\n",
        "pandas2ri.activate()\n",
        "numpy2ri.activate()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"VALIDATION STEP 3: SUBGROUP ANALYSIS (VCV SUBSETTING FIX)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def subset_vcv_matrix(full_vcv, full_indices, keep_indices):\n",
        "    \"\"\"\n",
        "    Subset a VCV matrix to only include specific observations.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    full_vcv : np.ndarray\n",
        "        The full k×k VCV matrix for a study\n",
        "    full_indices : array-like\n",
        "        The original row indices (or identifiers) for all k observations\n",
        "    keep_indices : array-like\n",
        "        The indices we want to keep (subset)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    np.ndarray : Subsetted VCV matrix\n",
        "    \"\"\"\n",
        "    full_indices = np.asarray(full_indices)\n",
        "    keep_indices = np.asarray(keep_indices)\n",
        "\n",
        "    # Find positions of keep_indices within full_indices\n",
        "    positions = []\n",
        "    for idx in keep_indices:\n",
        "        matches = np.where(full_indices == idx)[0]\n",
        "        if len(matches) > 0:\n",
        "            positions.append(matches[0])\n",
        "\n",
        "    if len(positions) == 0:\n",
        "        return None\n",
        "\n",
        "    positions = np.array(positions)\n",
        "\n",
        "    # Subset the matrix: V[positions, :][:, positions]\n",
        "    return full_vcv[np.ix_(positions, positions)]\n",
        "\n",
        "\n",
        "try:\n",
        "    # --- 1. Setup & Data Loading ---\n",
        "    if 'ANALYSIS_CONFIG' not in globals():\n",
        "        raise Exception(\"ANALYSIS_CONFIG not found.\")\n",
        "\n",
        "    if 'analysis_data' in ANALYSIS_CONFIG:\n",
        "        df_main = ANALYSIS_CONFIG['analysis_data'].copy()\n",
        "    else:\n",
        "        df_main = data_filtered.copy()\n",
        "\n",
        "    # Normalize column names\n",
        "    df_main.columns = df_main.columns.astype(str)\n",
        "    eff_col = str(ANALYSIS_CONFIG['effect_col'])\n",
        "    var_col = str(ANALYSIS_CONFIG['var_col'])\n",
        "\n",
        "    # Get VCV matrices dictionary\n",
        "    vcv_dict = ANALYSIS_CONFIG.get('vcv_matrices', {})\n",
        "\n",
        "    # CRITICAL: We need to know the original row indices for each study\n",
        "    # Add a global row index to the main dataframe\n",
        "    df_main = df_main.reset_index(drop=True)\n",
        "    df_main['_global_row_idx'] = np.arange(len(df_main))\n",
        "\n",
        "    # --- 2. Logic: Detect Analysis Type (Single vs Two-Way) ---\n",
        "    sub_config = ANALYSIS_CONFIG.get('subgroup_config', {})\n",
        "    analysis_type = sub_config.get('analysis_type', 'single')\n",
        "\n",
        "    mod1 = str(sub_config.get('moderator1', 'None'))\n",
        "    mod2 = str(sub_config.get('moderator2', 'None'))\n",
        "\n",
        "    # Fallback / Auto-detect if config is empty\n",
        "    if mod1 == 'None' or mod1 not in df_main.columns:\n",
        "        potential_mods = [c for c in df_main.columns\n",
        "                          if (df_main[c].dtype == 'object' or df_main[c].nunique() < 10)\n",
        "                          and c not in [eff_col, var_col, 'id', 'ne', 'nc', '_global_row_idx']]\n",
        "        if potential_mods:\n",
        "            mod1 = potential_mods[0]\n",
        "            analysis_type = 'single'\n",
        "            print(f\"⚠️ Config missing. Auto-selected moderator: '{mod1}'\")\n",
        "        else:\n",
        "            raise Exception(\"No valid moderator found.\")\n",
        "\n",
        "    # --- 2.5 PRE-COMPUTE: Build index mapping for each study ---\n",
        "    # This tells us which global row indices belong to each study\n",
        "    study_row_indices = {}\n",
        "    for study_id, group in df_main.groupby('id', sort=False):\n",
        "        study_row_indices[study_id] = group['_global_row_idx'].values\n",
        "        study_row_indices[str(study_id)] = group['_global_row_idx'].values  # String key too\n",
        "\n",
        "    # Prepare list of groups to validate\n",
        "    validation_queue = []\n",
        "\n",
        "    if analysis_type == 'two_way' and mod2 != 'None' and mod2 in df_main.columns:\n",
        "        print(f\"🔍 Validating Two-Factor Analysis: {mod1} × {mod2}\")\n",
        "        pairs = df_main[[mod1, mod2]].dropna().drop_duplicates().values\n",
        "        for m1_val, m2_val in pairs:\n",
        "            validation_queue.append({\n",
        "                'label': f\"{m1_val} × {m2_val}\",\n",
        "                'filter': (df_main[mod1] == m1_val) & (df_main[mod2] == m2_val)\n",
        "            })\n",
        "    else:\n",
        "        print(f\"🔍 Validating Single-Factor Analysis: {mod1}\")\n",
        "        unique_vals = df_main[mod1].dropna().unique()\n",
        "        for val in unique_vals:\n",
        "            validation_queue.append({\n",
        "                'label': str(val),\n",
        "                'filter': (df_main[mod1] == val)\n",
        "            })\n",
        "\n",
        "    # Check Inference Settings\n",
        "    use_t_dist = False\n",
        "    if 'settings' in ANALYSIS_CONFIG:\n",
        "        if ANALYSIS_CONFIG['settings'].get('dist_type') == 't':\n",
        "            use_t_dist = True\n",
        "\n",
        "    comparison_rows = []\n",
        "\n",
        "    # --- 3. Validation Loop ---\n",
        "    for item in validation_queue:\n",
        "        group_label = item['label']\n",
        "        mask = item['filter']\n",
        "\n",
        "        # =============================================================\n",
        "        # 3.1 Filter and sort data\n",
        "        # =============================================================\n",
        "        df_sub = df_main[mask].copy()\n",
        "\n",
        "        # Sort by study ID, then by global row index within study\n",
        "        df_sub = df_sub.sort_values(['id', '_global_row_idx']).reset_index(drop=True)\n",
        "\n",
        "        # Skip small groups\n",
        "        n_obs = len(df_sub)\n",
        "        n_studies = df_sub['id'].nunique()\n",
        "\n",
        "        if n_obs < 3 or n_studies < 2:\n",
        "            comparison_rows.append({\n",
        "                'Subgroup': group_label, 'Metric': 'All', 'Status': '⚠️ SKIP (N<3)',\n",
        "                'Python': np.nan, 'R': np.nan, 'Diff': np.nan\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # =============================================================\n",
        "        # 3.2 BUILD VCV MATRIX WITH PROPER SUBSETTING\n",
        "        # =============================================================\n",
        "        vcv_blocks = []\n",
        "        vcv_construction_ok = True\n",
        "\n",
        "        for study_id, group in df_sub.groupby('id', sort=False):\n",
        "            k_sub = len(group)  # Number of obs in THIS subgroup for this study\n",
        "            vi = group[var_col].values\n",
        "            subgroup_global_indices = group['_global_row_idx'].values\n",
        "\n",
        "            # Get the full study's row indices\n",
        "            sid_str = str(study_id)\n",
        "            full_study_indices = study_row_indices.get(study_id, study_row_indices.get(sid_str))\n",
        "\n",
        "            # Check if we have a VCV matrix for this study\n",
        "            full_vcv = vcv_dict.get(sid_str, vcv_dict.get(study_id, None))\n",
        "\n",
        "            if full_vcv is not None:\n",
        "                full_vcv = np.asarray(full_vcv, dtype=np.float64)\n",
        "                k_full = len(full_study_indices)\n",
        "\n",
        "                # Verify matrix dimensions match full study size\n",
        "                if full_vcv.shape[0] != k_full:\n",
        "                    print(f\"  ⚠️ {group_label}: VCV size mismatch for study {study_id}. \"\n",
        "                          f\"Matrix is {full_vcv.shape[0]}×{full_vcv.shape[0]}, expected {k_full}×{k_full}\")\n",
        "                    # Fall back to diagonal\n",
        "                    vcv_blocks.append(np.diag(vi.astype(np.float64)))\n",
        "                    continue\n",
        "\n",
        "                # SUBSET the VCV matrix to only include rows in this subgroup\n",
        "                if k_sub == k_full:\n",
        "                    # No subsetting needed - all observations from this study are in subgroup\n",
        "                    vcv_blocks.append(full_vcv)\n",
        "                else:\n",
        "                    # Need to subset\n",
        "                    sub_vcv = subset_vcv_matrix(full_vcv, full_study_indices, subgroup_global_indices)\n",
        "                    if sub_vcv is not None and sub_vcv.shape[0] == k_sub:\n",
        "                        vcv_blocks.append(sub_vcv)\n",
        "                    else:\n",
        "                        print(f\"  ⚠️ {group_label}: VCV subsetting failed for study {study_id}, using diagonal\")\n",
        "                        vcv_blocks.append(np.diag(vi.astype(np.float64)))\n",
        "            else:\n",
        "                # No VCV matrix - use diagonal (independence assumption)\n",
        "                vcv_blocks.append(np.diag(vi.astype(np.float64)))\n",
        "\n",
        "        # Build block diagonal matrix\n",
        "        try:\n",
        "            V_full = block_diag(*vcv_blocks)\n",
        "            use_matrix_in_r = True\n",
        "\n",
        "            # Final dimension check\n",
        "            if V_full.shape[0] != n_obs:\n",
        "                print(f\"  ⚠️ {group_label}: Final V matrix dim {V_full.shape[0]} != n_obs {n_obs}\")\n",
        "                V_full = np.diag(df_sub[var_col].values.astype(np.float64))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️ {group_label}: block_diag failed ({e}), using diagonal\")\n",
        "            use_matrix_in_r = True\n",
        "            V_full = np.diag(df_sub[var_col].values.astype(np.float64))\n",
        "\n",
        "        # =============================================================\n",
        "        # 3.3 Run Python Analysis\n",
        "        # =============================================================\n",
        "        # IMPORTANT: Python function also needs to handle VCV subsetting!\n",
        "        # For now, we pass the subsetted data and let it rebuild\n",
        "\n",
        "        # Temporarily update ANALYSIS_CONFIG with subsetted VCV for this subgroup\n",
        "        # This is a workaround - ideally the Python function would handle subsetting internally\n",
        "        temp_vcv_dict = {}\n",
        "        idx = 0\n",
        "        for study_id, group in df_sub.groupby('id', sort=False):\n",
        "            k = len(group)\n",
        "            temp_vcv_dict[str(study_id)] = vcv_blocks[idx]\n",
        "            idx += 1\n",
        "\n",
        "        # Store original and swap\n",
        "        original_vcv_dict = ANALYSIS_CONFIG.get('vcv_matrices', {})\n",
        "        ANALYSIS_CONFIG['vcv_matrices'] = temp_vcv_dict\n",
        "\n",
        "        try:\n",
        "            py_output = _run_three_level_reml_for_subgroup(df_sub, eff_col, var_col)\n",
        "        finally:\n",
        "            # Restore original\n",
        "            ANALYSIS_CONFIG['vcv_matrices'] = original_vcv_dict\n",
        "\n",
        "        if not py_output or py_output[0] is None:\n",
        "            comparison_rows.append({\n",
        "                'Subgroup': group_label, 'Metric': 'Py Fail', 'Status': '❌ FAIL',\n",
        "                'Python': np.nan, 'R': np.nan, 'Diff': np.nan\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        py_res, py_data = py_output\n",
        "\n",
        "        # Robust extraction\n",
        "        py_mu = py_res.get('mu', py_res.get('pooled_effect', np.nan))\n",
        "        py_se = py_res.get('se_mu', py_res.get('se', np.nan))\n",
        "        py_tau2 = py_res.get('tau_sq', py_res.get('tau2', np.nan))\n",
        "        py_sigma2 = py_res.get('sigma_sq', py_res.get('sigma2', np.nan))\n",
        "        py_model_type = py_res.get('model_type', 'unknown')\n",
        "\n",
        "        # =============================================================\n",
        "        # 3.4 Run R Analysis (metafor)\n",
        "        # =============================================================\n",
        "\n",
        "        # Prepare data for R - remove helper columns\n",
        "        df_for_r = df_sub.drop(columns=['_global_row_idx'], errors='ignore').copy()\n",
        "        df_for_r['_row_id'] = np.arange(1, len(df_for_r) + 1)\n",
        "\n",
        "        ro.globalenv['df_r'] = df_for_r\n",
        "        ro.globalenv['V_matrix'] = V_full\n",
        "        ro.globalenv['eff_col'] = eff_col\n",
        "        ro.globalenv['var_col'] = var_col\n",
        "        ro.globalenv['use_matrix'] = use_matrix_in_r\n",
        "        ro.globalenv['use_t_dist'] = use_t_dist\n",
        "\n",
        "        r_script = \"\"\"\n",
        "        library(metafor)\n",
        "\n",
        "        dat <- df_r\n",
        "        dat$row_id <- dat$`_row_id`\n",
        "        dat$study_id <- as.factor(dat$id)\n",
        "\n",
        "        # Debug info\n",
        "        n_obs <- nrow(dat)\n",
        "        v_dim <- nrow(V_matrix)\n",
        "\n",
        "        if (use_matrix && (v_dim != n_obs)) {\n",
        "            # Dimension mismatch - fall back to diagonal\n",
        "            V_input <- dat[[var_col]]\n",
        "            used_matrix <- FALSE\n",
        "        } else if (use_matrix) {\n",
        "            V_input <- V_matrix\n",
        "            used_matrix <- TRUE\n",
        "        } else {\n",
        "            V_input <- dat[[var_col]]\n",
        "            used_matrix <- FALSE\n",
        "        }\n",
        "\n",
        "        test_type <- ifelse(use_t_dist, \"t\", \"z\")\n",
        "\n",
        "        tryCatch({\n",
        "            res <- rma.mv(yi = dat[[eff_col]],\n",
        "                          V = V_input,\n",
        "                          random = ~ 1 | study_id/row_id,\n",
        "                          data = dat,\n",
        "                          method = \"REML\",\n",
        "                          test = test_type,\n",
        "                          sparse = FALSE,\n",
        "                          control = list(iter.max = 1000))\n",
        "\n",
        "            list(\n",
        "                ok = TRUE,\n",
        "                b = as.numeric(res$b),\n",
        "                se = as.numeric(res$se),\n",
        "                tau2 = as.numeric(res$sigma2[1]),\n",
        "                sigma2 = as.numeric(res$sigma2[2]),\n",
        "                converged = res$converged,\n",
        "                used_matrix = used_matrix,\n",
        "                n_obs = n_obs,\n",
        "                v_dim = v_dim\n",
        "            )\n",
        "        }, error = function(e) {\n",
        "            list(ok = FALSE, error_msg = as.character(e), n_obs = n_obs, v_dim = v_dim)\n",
        "        })\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            r_out = ro.r(r_script)\n",
        "        except Exception as e:\n",
        "            comparison_rows.append({\n",
        "                'Subgroup': group_label, 'Metric': 'R Error', 'Status': '❌ R FAIL',\n",
        "                'Python': f\"N={n_obs}\", 'R': str(e)[:50], 'Diff': np.nan\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        if not r_out.rx2('ok')[0]:\n",
        "            err_msg = str(r_out.rx2('error_msg')[0]) if 'error_msg' in list(r_out.names) else 'Unknown'\n",
        "            v_dim = int(r_out.rx2('v_dim')[0]) if 'v_dim' in list(r_out.names) else '?'\n",
        "            comparison_rows.append({\n",
        "                'Subgroup': group_label, 'Metric': 'R Error', 'Status': '❌ R FAIL',\n",
        "                'Python': f\"N={n_obs}, Vdim={v_dim}\", 'R': err_msg[:50], 'Diff': np.nan\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # =============================================================\n",
        "        # 3.5 Compare Results\n",
        "        # =============================================================\n",
        "        # =============================================================\n",
        "        # 3.5 Compare Results - SAFE EXTRACTION\n",
        "        # =============================================================\n",
        "\n",
        "        # Helper function to safely extract R values\n",
        "        def safe_r_extract(r_obj, key, default=None):\n",
        "            \"\"\"Safely extract a value from an R list, returning default if NULL or missing.\"\"\"\n",
        "            try:\n",
        "                if key not in list(r_obj.names):\n",
        "                    return default\n",
        "                val = r_obj.rx2(key)\n",
        "                if val == ro.NULL or val is None:\n",
        "                    return default\n",
        "                # Handle single values\n",
        "                if hasattr(val, '__len__') and len(val) > 0:\n",
        "                    return val[0]\n",
        "                return float(val)\n",
        "            except Exception:\n",
        "                return default\n",
        "\n",
        "        r_mu = safe_r_extract(r_out, 'b')\n",
        "        r_se = safe_r_extract(r_out, 'se')\n",
        "        r_tau2 = safe_r_extract(r_out, 'tau2')\n",
        "        r_sigma2 = safe_r_extract(r_out, 'sigma2')\n",
        "        r_converged = safe_r_extract(r_out, 'converged', default=False)\n",
        "        r_used_matrix = safe_r_extract(r_out, 'used_matrix', default=False)\n",
        "\n",
        "        # Check if we got valid results\n",
        "        if r_mu is None or r_se is None:\n",
        "            err_msg = safe_r_extract(r_out, 'error_msg', default='Unknown error')\n",
        "            comparison_rows.append({\n",
        "                'Subgroup': group_label, 'Metric': 'R Error', 'Status': '❌ R FAIL',\n",
        "                'Python': f\"N={n_obs}\", 'R': str(err_msg)[:50], 'Diff': np.nan\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Convert to float for comparison\n",
        "        r_mu = float(r_mu)\n",
        "        r_se = float(r_se)\n",
        "        r_tau2 = float(r_tau2) if r_tau2 is not None else np.nan\n",
        "        r_sigma2 = float(r_sigma2) if r_sigma2 is not None else np.nan\n",
        "        r_converged = bool(r_converged) if r_converged is not None else False\n",
        "        r_used_matrix = bool(r_used_matrix) if r_used_matrix is not None else False\n",
        "\n",
        "        metrics = {\n",
        "            'Effect': (py_mu, r_mu),\n",
        "            'SE': (py_se, r_se),\n",
        "            'Tau²': (py_tau2, r_tau2),\n",
        "            'Sigma²': (py_sigma2, r_sigma2)\n",
        "        }\n",
        "\n",
        "        for metric_name, (py_val, r_val) in metrics.items():\n",
        "            if pd.isna(py_val) or pd.isna(r_val):\n",
        "                status = '❌ FAIL (NaN)'\n",
        "                diff = np.nan\n",
        "            else:\n",
        "                diff = abs(py_val - r_val)\n",
        "\n",
        "                if metric_name in ('Effect', 'SE'):\n",
        "                    passed = diff < 1e-3\n",
        "                else:\n",
        "                    abs_tol = 0.01\n",
        "                    rel_tol = 0.05\n",
        "                    passed = diff < abs_tol or (diff / (abs(r_val) + 1e-9) < rel_tol)\n",
        "\n",
        "                status = \"✅ PASS\" if passed else \"❌ FAIL\"\n",
        "\n",
        "                if not r_converged:\n",
        "                    status += \" ⚠️R:noconv\"\n",
        "\n",
        "            comparison_rows.append({\n",
        "                'Subgroup': group_label,\n",
        "                'Metric': metric_name,\n",
        "                'Python': f\"{py_val:.4f}\" if not pd.isna(py_val) else \"NaN\",\n",
        "                'R': f\"{r_val:.4f}\" if not pd.isna(r_val) else \"NaN\",\n",
        "                'Diff': f\"{diff:.2e}\" if not pd.isna(diff) else \"NaN\",\n",
        "                'Status': status,\n",
        "                'PyModel': py_model_type,\n",
        "                'R_VCV': 'matrix' if r_used_matrix else 'diag'\n",
        "            })\n",
        "\n",
        "    # --- 4. Display Results ---\n",
        "    df_comp = pd.DataFrame(comparison_rows)\n",
        "\n",
        "    if not df_comp.empty and 'Status' in df_comp.columns:\n",
        "        n_pass = df_comp['Status'].str.contains('PASS', na=False).sum()\n",
        "        n_fail = df_comp['Status'].str.contains('FAIL', na=False).sum()\n",
        "        n_skip = df_comp['Status'].str.contains('SKIP', na=False).sum()\n",
        "\n",
        "        print(f\"\\n📊 Summary: {n_pass} PASS | {n_fail} FAIL | {n_skip} SKIP\")\n",
        "\n",
        "    def style_table(styler):\n",
        "        def color_status(val):\n",
        "            if 'PASS' in str(val):\n",
        "                return 'background-color: #d4edda'\n",
        "            elif 'FAIL' in str(val):\n",
        "                return 'background-color: #f8d7da'\n",
        "            elif 'SKIP' in str(val):\n",
        "                return 'background-color: #fff3cd'\n",
        "            return ''\n",
        "\n",
        "        styler.set_table_styles([{'selector': 'th', 'props': [('text-align', 'left')]}])\n",
        "        styler.applymap(color_status, subset=['Status'])\n",
        "        return styler\n",
        "\n",
        "    display(HTML(\"<h4>🧪 Subgroup Validation Results</h4>\"))\n",
        "    if not df_comp.empty:\n",
        "        display(style_table(df_comp.style))\n",
        "    else:\n",
        "        print(\"No valid subgroups found for validation.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Validation Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5kLXAGTseF2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧪 Validation: Imports & R Setup\n",
        "import sys\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "# 1. Setup R Interface\n",
        "print(\"⚙️ Checking R environment...\")\n",
        "try:\n",
        "    import rpy2.robjects as ro\n",
        "    from rpy2.robjects import pandas2ri\n",
        "    from rpy2.robjects.packages import importr\n",
        "except ImportError:\n",
        "    print(\"   Installing rpy2...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rpy2\"])\n",
        "    import rpy2.robjects as ro\n",
        "    from rpy2.robjects import pandas2ri\n",
        "    from rpy2.robjects.packages import importr\n",
        "\n",
        "# 2. Activate Conversions\n",
        "pandas2ri.activate()\n",
        "r = ro.r\n",
        "\n",
        "# 3. Load R Packages (metafor/metadat)\n",
        "utils = importr(\"utils\")\n",
        "try:\n",
        "    metadat = importr(\"metadat\")\n",
        "    metafor = importr(\"metafor\")\n",
        "    print(\"✅ R packages loaded successfully.\")\n",
        "except Exception:\n",
        "    print(\"⬇️ Installing R packages (metafor/metadat)...\")\n",
        "    utils.install_packages(\"metadat\", repos=\"https://cloud.r-project.org\")\n",
        "    utils.install_packages(\"metafor\", repos=\"https://cloud.r-project.org\")\n",
        "    metadat = importr(\"metadat\")\n",
        "    metafor = importr(\"metafor\")\n",
        "    print(\"✅ Installation complete.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kOcNqoF0dklS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧪 Publication-Quality Simulation Study\n",
        "# =============================================================================\n",
        "# SIMULATION STUDY: Parameter Recovery & Coverage\n",
        "# Purpose: Validate 3-level meta-analysis against known truth AND metafor\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.linalg import block_diag\n",
        "from scipy.stats import norm\n",
        "import rpy2.robjects as ro\n",
        "from rpy2.robjects import pandas2ri\n",
        "import rpy2.robjects.numpy2ri as numpy2ri\n",
        "from IPython.display import display, HTML\n",
        "import warnings\n",
        "\n",
        "pandas2ri.activate()\n",
        "numpy2ri.activate()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SIMULATION STUDY: Parameter Recovery & CI Coverage\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# =============================================================================\n",
        "# 1. DATA GENERATOR\n",
        "# =============================================================================\n",
        "\n",
        "def generate_synthetic_meta_data(\n",
        "    n_studies=30,\n",
        "    mean_effect=0.5,\n",
        "    tau_sq=0.3,\n",
        "    sigma_sq=0.1,\n",
        "    correlation=0.5,\n",
        "    k_distribution=[0.2, 0.4, 0.3, 0.1],\n",
        "    seed=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates synthetic 3-level meta-analysis data with shared controls.\n",
        "    FIXED: Consistent string keys for VCV dictionary.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    data_rows = []\n",
        "    vcv_matrices = {}\n",
        "\n",
        "    for i in range(n_studies):\n",
        "        study_id = i + 1  # Integer\n",
        "        study_id_str = str(study_id)  # String key for dictionary\n",
        "\n",
        "        # Study structure\n",
        "        k_i = rng.choice([1, 2, 3, 4], p=k_distribution)\n",
        "\n",
        "        # True study effect\n",
        "        u_i = rng.normal(0, np.sqrt(tau_sq)) if tau_sq > 0 else 0\n",
        "\n",
        "        # Sampling variances\n",
        "        vi_diag = rng.uniform(0.05, 0.20, size=k_i)\n",
        "\n",
        "        # Build VCV matrix\n",
        "        if k_i > 1 and correlation > 0:\n",
        "            V_i = np.zeros((k_i, k_i))\n",
        "            for r in range(k_i):\n",
        "                for c in range(k_i):\n",
        "                    if r == c:\n",
        "                        V_i[r, c] = vi_diag[r]\n",
        "                    else:\n",
        "                        V_i[r, c] = correlation * np.sqrt(vi_diag[r] * vi_diag[c])\n",
        "        else:\n",
        "            V_i = np.diag(vi_diag)\n",
        "\n",
        "        # Store with STRING key\n",
        "        vcv_matrices[study_id_str] = V_i\n",
        "\n",
        "        # Generate observed effects\n",
        "        total_error_cov = V_i + sigma_sq * np.eye(k_i)\n",
        "        min_eig = np.min(np.linalg.eigvalsh(total_error_cov))\n",
        "        if min_eig < 1e-10:\n",
        "            total_error_cov += (1e-10 - min_eig) * np.eye(k_i)\n",
        "\n",
        "        errors = rng.multivariate_normal(np.zeros(k_i), total_error_cov)\n",
        "        y_i = mean_effect + u_i + errors\n",
        "\n",
        "        for j in range(k_i):\n",
        "            data_rows.append({\n",
        "                'id': study_id_str,  # String ID in dataframe too\n",
        "                'yi': y_i[j],\n",
        "                'vi': vi_diag[j]\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(data_rows)\n",
        "    true_params = {'mu': mean_effect, 'tau_sq': tau_sq, 'sigma_sq': sigma_sq}\n",
        "\n",
        "    return df, vcv_matrices, true_params\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 2. RUN SINGLE SIMULATION\n",
        "# =============================================================================\n",
        "\n",
        "def run_single_simulation(true_params, n_studies=30, correlation=0.5, seed=None):\n",
        "    \"\"\"\n",
        "    Run one simulation: generate data, fit with Python, fit with R, compare.\n",
        "    FIXED: Better R error handling and diagnostics.\n",
        "    \"\"\"\n",
        "    # Generate data\n",
        "    df, vcv_dict, _ = generate_synthetic_meta_data(\n",
        "        n_studies=n_studies,\n",
        "        mean_effect=true_params['mu'],\n",
        "        tau_sq=true_params['tau_sq'],\n",
        "        sigma_sq=true_params['sigma_sq'],\n",
        "        correlation=correlation,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # =================================================================\n",
        "    # Sort dataframe and build V_full in matching order\n",
        "    # =================================================================\n",
        "\n",
        "    # Convert ID to integer for proper sorting, keep as string in dataframe\n",
        "    df['id_int'] = df['id'].astype(int)\n",
        "    df = df.sort_values(['id_int']).reset_index(drop=True)\n",
        "    df['row_id'] = np.arange(1, len(df) + 1)\n",
        "\n",
        "    # Build V_full by iterating through sorted unique IDs\n",
        "    vcv_blocks = []\n",
        "    for study_id in sorted(df['id_int'].unique()):\n",
        "        str_key = str(study_id)\n",
        "        if str_key in vcv_dict:\n",
        "            vcv_blocks.append(vcv_dict[str_key])\n",
        "        else:\n",
        "            study_vi = df[df['id_int'] == study_id]['vi'].values\n",
        "            vcv_blocks.append(np.diag(study_vi))\n",
        "\n",
        "    V_full = block_diag(*vcv_blocks)\n",
        "\n",
        "    # --- Python Estimation ---\n",
        "    ANALYSIS_CONFIG['vcv_matrices'] = vcv_dict\n",
        "\n",
        "    try:\n",
        "        py_result, _ = _run_three_level_reml_for_subgroup(df, 'yi', 'vi')\n",
        "        if py_result is None:\n",
        "            py_result = {'mu': np.nan, 'tau_sq': np.nan, 'sigma_sq': np.nan,\n",
        "                        'se_mu': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan}\n",
        "        else:\n",
        "            # Normalize key names\n",
        "            if 'tau2' in py_result and 'tau_sq' not in py_result:\n",
        "                py_result['tau_sq'] = py_result['tau2']\n",
        "            if 'sigma2' in py_result and 'sigma_sq' not in py_result:\n",
        "                py_result['sigma_sq'] = py_result['sigma2']\n",
        "\n",
        "            # Calculate CI if not present\n",
        "            if 'ci_lower' not in py_result:\n",
        "                se = py_result.get('se_mu', py_result.get('se', np.nan))\n",
        "                mu = py_result.get('mu', np.nan)\n",
        "                py_result['ci_lower'] = mu - 1.96 * se\n",
        "                py_result['ci_upper'] = mu + 1.96 * se\n",
        "    except Exception as e:\n",
        "        py_result = {'mu': np.nan, 'tau_sq': np.nan, 'sigma_sq': np.nan,\n",
        "                    'se_mu': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan,\n",
        "                    'error': str(e)}\n",
        "\n",
        "    # --- R Estimation ---\n",
        "    r_result = None\n",
        "    try:\n",
        "        # Prepare data for R\n",
        "        df_for_r = df[['id', 'yi', 'vi', 'row_id']].copy()\n",
        "\n",
        "        ro.globalenv['df_r'] = df_for_r\n",
        "        ro.globalenv['V_matrix'] = V_full\n",
        "\n",
        "        r_script = \"\"\"\n",
        "        library(metafor)\n",
        "\n",
        "        result <- tryCatch({\n",
        "            dat <- df_r\n",
        "            dat$study_id <- as.factor(dat$id)\n",
        "\n",
        "            # Verify dimensions\n",
        "            n_rows <- nrow(dat)\n",
        "            v_dim <- nrow(V_matrix)\n",
        "\n",
        "            if (n_rows != v_dim) {\n",
        "                stop(paste(\"Dimension mismatch: data has\", n_rows, \"rows but V has\", v_dim))\n",
        "            }\n",
        "\n",
        "            # Add row/col names\n",
        "            rownames(V_matrix) <- 1:n_rows\n",
        "            colnames(V_matrix) <- 1:n_rows\n",
        "\n",
        "            # Fit model\n",
        "            res <- rma.mv(yi = yi,\n",
        "                          V = V_matrix,\n",
        "                          random = ~ 1 | study_id/row_id,\n",
        "                          data = dat,\n",
        "                          method = \"REML\",\n",
        "                          control = list(iter.max = 1000))\n",
        "\n",
        "            list(\n",
        "                ok = TRUE,\n",
        "                mu = as.numeric(res$b[1]),\n",
        "                se = as.numeric(res$se[1]),\n",
        "                tau2 = as.numeric(res$sigma2[1]),\n",
        "                sigma2 = as.numeric(res$sigma2[2]),\n",
        "                ci_lb = as.numeric(res$ci.lb[1]),\n",
        "                ci_ub = as.numeric(res$ci.ub[1]),\n",
        "                converged = res$converged,\n",
        "                error_msg = \"\"\n",
        "            )\n",
        "        }, error = function(e) {\n",
        "            list(\n",
        "                ok = FALSE,\n",
        "                mu = NA,\n",
        "                se = NA,\n",
        "                tau2 = NA,\n",
        "                sigma2 = NA,\n",
        "                ci_lb = NA,\n",
        "                ci_ub = NA,\n",
        "                converged = FALSE,\n",
        "                error_msg = as.character(conditionMessage(e))\n",
        "            )\n",
        "        })\n",
        "\n",
        "        result\n",
        "        \"\"\"\n",
        "\n",
        "        r_out = ro.r(r_script)\n",
        "\n",
        "        # Safe extraction helper\n",
        "        def safe_get(r_obj, key, default=np.nan):\n",
        "            try:\n",
        "                if key not in list(r_obj.names):\n",
        "                    return default\n",
        "                val = r_obj.rx2(key)\n",
        "                if val == ro.NULL or val is None:\n",
        "                    return default\n",
        "                if hasattr(val, '__len__') and len(val) > 0:\n",
        "                    result = val[0]\n",
        "                    # Handle R's NA\n",
        "                    if pd.isna(result):\n",
        "                        return default\n",
        "                    return result\n",
        "                return default\n",
        "            except:\n",
        "                return default\n",
        "\n",
        "        r_ok = safe_get(r_out, 'ok', False)\n",
        "\n",
        "        if r_ok:\n",
        "            r_result = {\n",
        "                'mu': float(safe_get(r_out, 'mu')),\n",
        "                'tau_sq': float(safe_get(r_out, 'tau2')),\n",
        "                'sigma_sq': float(safe_get(r_out, 'sigma2')),\n",
        "                'se_mu': float(safe_get(r_out, 'se')),\n",
        "                'ci_lower': float(safe_get(r_out, 'ci_lb')),\n",
        "                'ci_upper': float(safe_get(r_out, 'ci_ub')),\n",
        "                'converged': bool(safe_get(r_out, 'converged', False))\n",
        "            }\n",
        "        else:\n",
        "            error_msg = safe_get(r_out, 'error_msg', 'Unknown R error')\n",
        "            r_result = {\n",
        "                'mu': np.nan, 'tau_sq': np.nan, 'sigma_sq': np.nan,\n",
        "                'se_mu': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan,\n",
        "                'converged': False, 'error': str(error_msg)\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        r_result = {\n",
        "            'mu': np.nan, 'tau_sq': np.nan, 'sigma_sq': np.nan,\n",
        "            'se_mu': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan,\n",
        "            'converged': False, 'error': f'Python exception: {str(e)}'\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'python': py_result,\n",
        "        'r': r_result,\n",
        "        'true': true_params\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 3. MONTE CARLO SIMULATION\n",
        "# =============================================================================\n",
        "\n",
        "def run_simulation_study(\n",
        "    n_sims=100,\n",
        "    n_studies=30,\n",
        "    scenarios=None,\n",
        "    correlation=0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Run Monte Carlo simulation study across multiple scenarios.\n",
        "    \"\"\"\n",
        "    if scenarios is None:\n",
        "        scenarios = [\n",
        "            {'name': 'Standard', 'mu': 0.5, 'tau_sq': 0.3, 'sigma_sq': 0.1},\n",
        "            {'name': 'High τ²', 'mu': 0.5, 'tau_sq': 0.8, 'sigma_sq': 0.1},\n",
        "            {'name': 'High σ²', 'mu': 0.5, 'tau_sq': 0.1, 'sigma_sq': 0.4},\n",
        "            {'name': 'σ² = 0 (Edge)', 'mu': 0.5, 'tau_sq': 0.3, 'sigma_sq': 0.0},\n",
        "            {'name': 'Both Low', 'mu': 0.5, 'tau_sq': 0.05, 'sigma_sq': 0.05},\n",
        "        ]\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for scenario in scenarios:\n",
        "        print(f\"\\n📊 Running scenario: {scenario['name']}\")\n",
        "        print(f\"   True: μ={scenario['mu']}, τ²={scenario['tau_sq']}, σ²={scenario['sigma_sq']}\")\n",
        "\n",
        "        true_params = {\n",
        "            'mu': scenario['mu'],\n",
        "            'tau_sq': scenario['tau_sq'],\n",
        "            'sigma_sq': scenario['sigma_sq']\n",
        "        }\n",
        "\n",
        "        scenario_results = {\n",
        "            'scenario': scenario['name'],\n",
        "            'true_mu': scenario['mu'],\n",
        "            'true_tau_sq': scenario['tau_sq'],\n",
        "            'true_sigma_sq': scenario['sigma_sq'],\n",
        "            'py_mu': [], 'py_tau_sq': [], 'py_sigma_sq': [],\n",
        "            'py_ci_covers': [],\n",
        "            'r_mu': [], 'r_tau_sq': [], 'r_sigma_sq': [],\n",
        "            'r_ci_covers': [], 'r_converged': []\n",
        "        }\n",
        "\n",
        "        for sim in range(n_sims):\n",
        "            if (sim + 1) % 25 == 0:\n",
        "                print(f\"   Simulation {sim + 1}/{n_sims}...\")\n",
        "\n",
        "            result = run_single_simulation(\n",
        "                true_params=true_params,\n",
        "                n_studies=n_studies,\n",
        "                correlation=correlation,\n",
        "                seed=sim * 1000 + hash(scenario['name']) % 1000\n",
        "            )\n",
        "\n",
        "            # Python results\n",
        "            py = result['python']\n",
        "            scenario_results['py_mu'].append(py.get('mu', np.nan))\n",
        "            scenario_results['py_tau_sq'].append(py.get('tau_sq', py.get('tau2', np.nan)))\n",
        "            scenario_results['py_sigma_sq'].append(py.get('sigma_sq', py.get('sigma2', np.nan)))\n",
        "\n",
        "            # CI coverage for Python\n",
        "            ci_lo = py.get('ci_lower', np.nan)\n",
        "            ci_hi = py.get('ci_upper', np.nan)\n",
        "            covers = (ci_lo <= scenario['mu'] <= ci_hi) if pd.notna(ci_lo) else np.nan\n",
        "            scenario_results['py_ci_covers'].append(covers)\n",
        "\n",
        "            # R results\n",
        "            r = result['r']\n",
        "            scenario_results['r_mu'].append(r.get('mu', np.nan))\n",
        "            scenario_results['r_tau_sq'].append(r.get('tau_sq', np.nan))\n",
        "            scenario_results['r_sigma_sq'].append(r.get('sigma_sq', np.nan))\n",
        "            scenario_results['r_converged'].append(r.get('converged', False))\n",
        "\n",
        "            # CI coverage for R\n",
        "            ci_lo_r = r.get('ci_lower', np.nan)\n",
        "            ci_hi_r = r.get('ci_upper', np.nan)\n",
        "            covers_r = (ci_lo_r <= scenario['mu'] <= ci_hi_r) if pd.notna(ci_lo_r) else np.nan\n",
        "            scenario_results['r_ci_covers'].append(covers_r)\n",
        "\n",
        "        all_results.append(scenario_results)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 4. SUMMARIZE RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "def summarize_simulation_results(all_results):\n",
        "    \"\"\"\n",
        "    Create summary table comparing Python vs R across scenarios.\n",
        "    \"\"\"\n",
        "    summary_rows = []\n",
        "\n",
        "    for res in all_results:\n",
        "        # Convert to arrays, handling NaN\n",
        "        py_mu = np.array(res['py_mu'])\n",
        "        py_tau = np.array(res['py_tau_sq'])\n",
        "        py_sig = np.array(res['py_sigma_sq'])\n",
        "        py_cov = np.array(res['py_ci_covers'])\n",
        "\n",
        "        r_mu = np.array(res['r_mu'])\n",
        "        r_tau = np.array(res['r_tau_sq'])\n",
        "        r_sig = np.array(res['r_sigma_sq'])\n",
        "        r_cov = np.array(res['r_ci_covers'])\n",
        "        r_conv = np.array(res['r_converged'])\n",
        "\n",
        "        summary_rows.append({\n",
        "            'Scenario': res['scenario'],\n",
        "            'True μ': res['true_mu'],\n",
        "            'True τ²': res['true_tau_sq'],\n",
        "            'True σ²': res['true_sigma_sq'],\n",
        "            # Python metrics\n",
        "            'Py μ̂ (mean)': np.nanmean(py_mu),\n",
        "            'Py μ̂ (bias)': np.nanmean(py_mu) - res['true_mu'],\n",
        "            'Py τ² (mean)': np.nanmean(py_tau),\n",
        "            'Py τ² (bias)': np.nanmean(py_tau) - res['true_tau_sq'],\n",
        "            'Py σ² (mean)': np.nanmean(py_sig),\n",
        "            'Py σ² (bias)': np.nanmean(py_sig) - res['true_sigma_sq'],\n",
        "            'Py CI Coverage': np.nanmean(py_cov) * 100,\n",
        "            # R metrics\n",
        "            'R μ̂ (mean)': np.nanmean(r_mu),\n",
        "            'R μ̂ (bias)': np.nanmean(r_mu) - res['true_mu'],\n",
        "            'R τ² (mean)': np.nanmean(r_tau),\n",
        "            'R τ² (bias)': np.nanmean(r_tau) - res['true_tau_sq'],\n",
        "            'R σ² (mean)': np.nanmean(r_sig),\n",
        "            'R σ² (bias)': np.nanmean(r_sig) - res['true_sigma_sq'],\n",
        "            'R CI Coverage': np.nanmean(r_cov) * 100,\n",
        "            'R Converged %': np.mean(r_conv) * 100,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(summary_rows)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 5. RUN THE STUDY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n🚀 Starting Monte Carlo Simulation Study...\")\n",
        "print(\"   This may take a few minutes...\\n\")\n",
        "\n",
        "# Run with fewer sims for testing, increase to 500+ for publication\n",
        "N_SIMS = 30  # Use 100-500 for publication\n",
        "N_STUDIES = 30\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    results = run_simulation_study(\n",
        "        n_sims=N_SIMS,\n",
        "        n_studies=N_STUDIES,\n",
        "        correlation=0.5\n",
        "    )\n",
        "\n",
        "# Summarize\n",
        "df_summary = summarize_simulation_results(results)\n",
        "\n",
        "# =============================================================================\n",
        "# 6. DISPLAY RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📊 SIMULATION STUDY RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Parameter Recovery Table\n",
        "print(\"\\n### Parameter Recovery (Bias)\")\n",
        "display_cols = ['Scenario', 'True μ', 'Py μ̂ (bias)', 'R μ̂ (bias)',\n",
        "                'True τ²', 'Py τ² (bias)', 'R τ² (bias)',\n",
        "                'True σ²', 'Py σ² (bias)', 'R σ² (bias)']\n",
        "df_bias = df_summary[display_cols].copy()\n",
        "df_bias = df_bias.round(4)\n",
        "display(HTML(\"<h4>Parameter Bias (closer to 0 is better)</h4>\"))\n",
        "display(df_bias)\n",
        "\n",
        "# CI Coverage Table\n",
        "print(\"\\n### Confidence Interval Coverage (should be ~95%)\")\n",
        "coverage_cols = ['Scenario', 'Py CI Coverage', 'R CI Coverage', 'R Converged %']\n",
        "df_coverage = df_summary[coverage_cols].copy()\n",
        "df_coverage = df_coverage.round(1)\n",
        "\n",
        "def style_coverage(val):\n",
        "    if isinstance(val, (int, float)):\n",
        "        if 93 <= val <= 97:\n",
        "            return 'background-color: #d4edda'\n",
        "        elif 90 <= val <= 98:\n",
        "            return 'background-color: #fff3cd'\n",
        "        elif val < 90 or val > 98:\n",
        "            return 'background-color: #f8d7da'\n",
        "    return ''\n",
        "\n",
        "display(HTML(\"<h4>CI Coverage & Convergence</h4>\"))\n",
        "styled = df_coverage.style.applymap(style_coverage, subset=['Py CI Coverage', 'R CI Coverage'])\n",
        "display(styled)\n",
        "\n",
        "# Key Finding: σ² = 0 scenario\n",
        "print(\"\\n### 🔑 Key Finding: Edge Case (True σ² = 0)\")\n",
        "edge_case = df_summary[df_summary['Scenario'] == 'σ² = 0 (Edge)'].iloc[0]\n",
        "print(f\"   Python σ² estimate: {edge_case['Py σ² (mean)']:.4f} (bias: {edge_case['Py σ² (bias)']:.4f})\")\n",
        "print(f\"   R σ² estimate:      {edge_case['R σ² (mean)']:.4f} (bias: {edge_case['R σ² (bias)']:.4f})\")\n",
        "print(f\"   R Convergence Rate: {edge_case['R Converged %']:.1f}%\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📝 SUMMARY FOR PUBLICATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "py_better_sigma = sum(1 for res in results\n",
        "                      if abs(np.nanmean(res['py_sigma_sq']) - res['true_sigma_sq']) <\n",
        "                         abs(np.nanmean(res['r_sigma_sq']) - res['true_sigma_sq']))\n",
        "\n",
        "print(f\"\"\"\n",
        "Key Findings:\n",
        "1. Both tools recover the pooled effect (μ) with minimal bias\n",
        "2. Python showed better σ² recovery in {py_better_sigma}/{len(results)} scenarios\n",
        "3. R convergence failures occurred in {100 - df_summary['R Converged %'].mean():.1f}% of simulations on average\n",
        "4. CI coverage: Python = {df_summary['Py CI Coverage'].mean():.1f}%, R = {df_summary['R CI Coverage'].mean():.1f}%\n",
        "\n",
        "Conclusion: The Python implementation provides comparable or better parameter\n",
        "recovery, particularly for within-study variance (σ²), while maintaining\n",
        "appropriate confidence interval coverage.\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GJKMuBIebxjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧪 Simulation Study: Meta-Regression (Python vs R)\n",
        "# =============================================================================\n",
        "# SIMULATION STUDY: META-REGRESSION\n",
        "# Purpose: Validate slope recovery and Type I error rates against R.\n",
        "# Scenarios:\n",
        "#   1. Standard: Moderate slope, moderate heterogeneity.\n",
        "#   2. Null Effect: Slope=0 (Tests False Positive Rate).\n",
        "#   3. High Heterogeneity: Hard to detect slope.\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.linalg import block_diag\n",
        "from scipy.stats import norm\n",
        "import rpy2.robjects as ro\n",
        "from rpy2.robjects import pandas2ri\n",
        "import rpy2.robjects.numpy2ri as numpy2ri\n",
        "from IPython.display import display, HTML\n",
        "import warnings\n",
        "\n",
        "pandas2ri.activate()\n",
        "numpy2ri.activate()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SIMULATION STUDY: META-REGRESSION ROBUSTNESS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# =============================================================================\n",
        "# 1. DATA GENERATOR (REGRESSION)\n",
        "# =============================================================================\n",
        "\n",
        "def generate_synthetic_reg_data(\n",
        "    n_studies=30,\n",
        "    intercept=0.5,\n",
        "    slope=0.5,\n",
        "    tau_sq=0.3,\n",
        "    sigma_sq=0.1,\n",
        "    rho=0.5,\n",
        "    seed=None\n",
        "):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    data = []\n",
        "    vcv_dict = {}\n",
        "\n",
        "    for i in range(n_studies):\n",
        "        sid = str(i+1)\n",
        "        k_i = rng.choice([1, 2, 3])\n",
        "\n",
        "        # Moderator (randomly distributed)\n",
        "        x_vals = rng.uniform(0, 5, size=k_i)\n",
        "\n",
        "        # Variances & Matrix\n",
        "        vi = rng.uniform(0.05, 0.15, size=k_i)\n",
        "        if k_i > 1:\n",
        "            V = np.diag(vi)\n",
        "            for r in range(k_i):\n",
        "                for c in range(k_i):\n",
        "                    if r!=c: V[r,c] = rho * np.sqrt(vi[r]*vi[c])\n",
        "            vcv_dict[sid] = V\n",
        "        else:\n",
        "            vcv_dict[sid] = np.diag(vi)\n",
        "\n",
        "        # Errors\n",
        "        total_cov = vcv_dict[sid] + sigma_sq * np.eye(k_i)\n",
        "\n",
        "        # Ensure positive definite\n",
        "        min_eig = np.min(np.linalg.eigvalsh(total_cov))\n",
        "        if min_eig < 1e-10: total_cov += (1e-10 - min_eig) * np.eye(k_i)\n",
        "\n",
        "        errors = rng.multivariate_normal(np.zeros(k_i), total_cov)\n",
        "        u_i = rng.normal(0, np.sqrt(tau_sq))\n",
        "\n",
        "        # Model: y = B0 + B1*x + u_i + e_ij\n",
        "        # Note: We generate u_i separately above, but technically it's part of the error structure.\n",
        "        # Let's simplify: The Random Effect u_i is the \"Level 3\" error.\n",
        "        # The multivariate normal above captured \"Level 2 (sigma)\" + \"Level 1 (sampling)\".\n",
        "        # Wait, standard generator separates them. Let's stick to the proven structure:\n",
        "\n",
        "        # Re-do Error generation to match previous success\n",
        "        # Level 3 (Study)\n",
        "        u_study = rng.normal(0, np.sqrt(tau_sq))\n",
        "        # Level 2 + 1 (Obs + Sampling)\n",
        "        total_cov_L2L1 = vcv_dict[sid] + sigma_sq * np.eye(k_i)\n",
        "        e_obs = rng.multivariate_normal(np.zeros(k_i), total_cov_L2L1)\n",
        "\n",
        "        y_vals = intercept + slope * x_vals + u_study + e_obs\n",
        "\n",
        "        for j in range(k_i):\n",
        "            data.append({\n",
        "                'id': sid, 'yi': y_vals[j], 'vi': vi[j], 'mod_x': x_vals[j]\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(data), vcv_dict\n",
        "\n",
        "# =============================================================================\n",
        "# 2. RUN SINGLE SIMULATION\n",
        "# =============================================================================\n",
        "\n",
        "def run_reg_simulation(params, n_studies=30, rho=0.5, seed=None):\n",
        "    # 1. Generate & Sort\n",
        "    df, vcv_dict = generate_synthetic_reg_data(\n",
        "        n_studies=n_studies,\n",
        "        intercept=params['intercept'],\n",
        "        slope=params['slope'],\n",
        "        tau_sq=params['tau_sq'],\n",
        "        sigma_sq=params['sigma_sq'],\n",
        "        rho=rho,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Sorting is critical for R alignment\n",
        "    df['id_int'] = df['id'].astype(int)\n",
        "    df = df.sort_values(['id_int']).reset_index(drop=True)\n",
        "    df['row_id'] = np.arange(1, len(df) + 1)\n",
        "\n",
        "    # 2. Build Aligned Matrix\n",
        "    vcv_blocks = []\n",
        "    for study_id, group in df.groupby('id', sort=False):\n",
        "        vcv_blocks.append(vcv_dict[str(study_id)])\n",
        "    V_full = block_diag(*vcv_blocks)\n",
        "\n",
        "    # --- Python ---\n",
        "    ANALYSIS_CONFIG['vcv_matrices'] = vcv_dict\n",
        "\n",
        "    try:\n",
        "        py_res, _, _ = _run_three_level_reml_regression_v2(\n",
        "            df, 'mod_x', 'yi', 'vi'\n",
        "        )\n",
        "        if py_res:\n",
        "            py_out = {\n",
        "                'slope': py_res['betas'][1],\n",
        "                'se': py_res['se_betas'][1], # Using model-based SE for fair comparison with R basic\n",
        "                'tau2': py_res['tau_sq'],\n",
        "                'sigma2': py_res['sigma_sq'],\n",
        "                'ci_lo': py_res['ci_lower'][1],\n",
        "                'ci_hi': py_res['ci_upper'][1]\n",
        "            }\n",
        "        else:\n",
        "            py_out = {'slope': np.nan, 'ci_lo': np.nan, 'ci_hi': np.nan}\n",
        "    except:\n",
        "        py_out = {'slope': np.nan, 'ci_lo': np.nan, 'ci_hi': np.nan}\n",
        "\n",
        "    # --- R ---\n",
        "    try:\n",
        "        ro.globalenv['df_r'] = df\n",
        "        ro.globalenv['V_full'] = V_full\n",
        "\n",
        "        r_script = \"\"\"\n",
        "        library(metafor)\n",
        "        dat <- df_r\n",
        "        dat$study_id <- as.factor(dat$id)\n",
        "        rownames(V_full) <- dat$row_id\n",
        "        colnames(V_full) <- dat$row_id\n",
        "\n",
        "        tryCatch({\n",
        "            res <- rma.mv(yi = yi, V = V_full, mods = ~ mod_x,\n",
        "                          random = ~ 1 | study_id/row_id,\n",
        "                          data = dat, method = \"REML\",\n",
        "                          control = list(optimizer=\"optim\", optmethod=\"BFGS\"))\n",
        "            list(\n",
        "                ok = TRUE,\n",
        "                slope = as.numeric(res$b[2]),\n",
        "                se = as.numeric(res$se[2]),\n",
        "                tau2 = as.numeric(res$sigma2[1]),\n",
        "                sigma2 = as.numeric(res$sigma2[2]),\n",
        "                ci_lo = as.numeric(res$ci.lb[2]),\n",
        "                ci_hi = as.numeric(res$ci.ub[2])\n",
        "            )\n",
        "        }, error = function(e) list(ok = FALSE))\n",
        "        \"\"\"\n",
        "        r_res = ro.r(r_script)\n",
        "\n",
        "        if r_res.rx2('ok')[0]:\n",
        "            r_out = {\n",
        "                'slope': float(r_res.rx2('slope')[0]),\n",
        "                'tau2': float(r_res.rx2('tau2')[0]),\n",
        "                'sigma2': float(r_res.rx2('sigma2')[0]),\n",
        "                'ci_lo': float(r_res.rx2('ci_lo')[0]),\n",
        "                'ci_hi': float(r_res.rx2('ci_hi')[0])\n",
        "            }\n",
        "        else:\n",
        "            r_out = {'slope': np.nan, 'ci_lo': np.nan, 'ci_hi': np.nan}\n",
        "\n",
        "    except:\n",
        "        r_out = {'slope': np.nan, 'ci_lo': np.nan, 'ci_hi': np.nan}\n",
        "\n",
        "    return py_out, r_out\n",
        "\n",
        "# =============================================================================\n",
        "# 3. RUN STUDY LOOP\n",
        "# =============================================================================\n",
        "\n",
        "scenarios = [\n",
        "    {'name': 'Standard', 'intercept':0.5, 'slope': 0.5, 'tau_sq':0.3, 'sigma_sq':0.1},\n",
        "    {'name': 'Null Effect (Slope=0)', 'intercept':0.5, 'slope': 0.0, 'tau_sq':0.3, 'sigma_sq':0.1},\n",
        "    {'name': 'High Heterogeneity', 'intercept':0.5, 'slope': 0.5, 'tau_sq':0.8, 'sigma_sq':0.1},\n",
        "]\n",
        "\n",
        "# Reduce N_SIMS for speed if needed (50 is good for checking, 500 for paper)\n",
        "N_SIMS = 50\n",
        "results_table = []\n",
        "\n",
        "print(f\"🚀 Running {N_SIMS} simulations per scenario...\")\n",
        "\n",
        "for sc in scenarios:\n",
        "    print(f\"   Scenario: {sc['name']}\")\n",
        "\n",
        "    py_slopes, r_slopes = [], []\n",
        "    py_covers, r_covers = [], []\n",
        "\n",
        "    for i in range(N_SIMS):\n",
        "        py, r = run_reg_simulation(sc, seed=i*99)\n",
        "\n",
        "        # Bias\n",
        "        py_slopes.append(py['slope'])\n",
        "        r_slopes.append(r.get('slope', np.nan))\n",
        "\n",
        "        # Coverage\n",
        "        true_b1 = sc['slope']\n",
        "        py_covers.append(py['ci_lo'] <= true_b1 <= py['ci_hi'])\n",
        "        if not np.isnan(r.get('ci_lo', np.nan)):\n",
        "            r_covers.append(r['ci_lo'] <= true_b1 <= r['ci_hi'])\n",
        "\n",
        "    # Aggregate\n",
        "    results_table.append({\n",
        "        'Scenario': sc['name'],\n",
        "        'True Slope': sc['slope'],\n",
        "        'Py Slope (mean)': np.nanmean(py_slopes),\n",
        "        'R Slope (mean)': np.nanmean(r_slopes),\n",
        "        'Py Bias': np.nanmean(py_slopes) - sc['slope'],\n",
        "        'R Bias': np.nanmean(r_slopes) - sc['slope'],\n",
        "        'Py Coverage %': np.mean(py_covers) * 100,\n",
        "        'R Coverage %': np.mean(r_covers) * 100 if r_covers else np.nan\n",
        "    })\n",
        "\n",
        "# =============================================================================\n",
        "# 4. DISPLAY\n",
        "# =============================================================================\n",
        "\n",
        "df_res = pd.DataFrame(results_table).round(4)\n",
        "\n",
        "print(\"\\n📊 REGRESSION SIMULATION RESULTS\")\n",
        "display(df_res)\n",
        "\n",
        "# Interpret Null Effect\n",
        "null_res = df_res[df_res['Scenario'].str.contains('Null')]\n",
        "if not null_res.empty:\n",
        "    cov = null_res.iloc[0]['Py Coverage %']\n",
        "    print(\"\\n🔑 Type I Error Check (Null Effect):\")\n",
        "    print(f\"   Target Coverage: 95%\")\n",
        "    print(f\"   Python Coverage: {cov}%\")\n",
        "    print(f\"   False Positive Rate: {100-cov:.1f}% (Should be ~5%)\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PaoJyxhywd4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧪 Synthetic Meta-Regression: Python vs. R (Direct Comparison)\n",
        "# =============================================================================\n",
        "# CELL: SYNTHETIC REGRESSION VALIDATION\n",
        "# Purpose: Generate known regression data and verify Python matches R exactly.\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rpy2.robjects as ro\n",
        "from rpy2.robjects import pandas2ri\n",
        "from scipy.linalg import block_diag\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Activate R\n",
        "pandas2ri.activate()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"🧪 SYNTHETIC META-REGRESSION: PYTHON vs R VALIDATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- 1. GENERATE DATA ---\n",
        "def generate_regression_data(n_studies=40, intercept=0.5, slope=1.2, tau2=0.3, sigma2=0.1, rho=0.5):\n",
        "    rng = np.random.default_rng(42)\n",
        "    data = []\n",
        "    vcv_dict = {}\n",
        "\n",
        "    print(f\"🎲 Generating Data: True Slope={slope}, τ²={tau2}, σ²={sigma2}...\")\n",
        "\n",
        "    for i in range(n_studies):\n",
        "        sid = str(i+1)\n",
        "        k_i = rng.choice([1, 2, 3])\n",
        "\n",
        "        # True study effect\n",
        "        u_i = rng.normal(0, np.sqrt(tau2))\n",
        "\n",
        "        # Moderator (e.g., Temperature)\n",
        "        x_vals = rng.uniform(10, 30, size=k_i)\n",
        "\n",
        "        # Variances & Matrix\n",
        "        vi = rng.uniform(0.05, 0.15, size=k_i)\n",
        "        if k_i > 1:\n",
        "            V = np.diag(vi)\n",
        "            for r in range(k_i):\n",
        "                for c in range(k_i):\n",
        "                    if r!=c: V[r,c] = rho * np.sqrt(vi[r]*vi[c])\n",
        "            vcv_dict[sid] = V\n",
        "        else:\n",
        "            vcv_dict[sid] = np.diag(vi)\n",
        "\n",
        "        # Outcomes\n",
        "        # y = B0 + B1*x + u + e\n",
        "        # Total error structure = V + sigma2*I\n",
        "        cov_err = vcv_dict[sid] + sigma2 * np.eye(k_i)\n",
        "        errors = rng.multivariate_normal(np.zeros(k_i), cov_err)\n",
        "\n",
        "        y_vals = intercept + slope * x_vals + u_i + errors\n",
        "\n",
        "        for j in range(k_i):\n",
        "            data.append({\n",
        "                'id': sid, 'yi': y_vals[j], 'vi': vi[j], 'mod_x': x_vals[j]\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(data), vcv_dict\n",
        "\n",
        "# Generate\n",
        "df_sim, vcv_sim = generate_regression_data()\n",
        "df_sim['_row_id'] = np.arange(1, len(df_sim)+1)\n",
        "\n",
        "# Inject into Global Config (so your functions can find it)\n",
        "ANALYSIS_CONFIG['vcv_matrices'] = vcv_sim\n",
        "\n",
        "# --- 2. RUN PYTHON (Your Tool) ---\n",
        "print(\"🐍 Running Python Model...\")\n",
        "py_res, _, _ = _run_three_level_reml_regression_v2(\n",
        "    analysis_data=df_sim,\n",
        "    moderator_col='mod_x',\n",
        "    effect_col='yi',\n",
        "    var_col='vi'\n",
        ")\n",
        "\n",
        "# Extract Python Stats\n",
        "py_beta0 = py_res['betas'][0]\n",
        "py_beta1 = py_res['betas'][1]\n",
        "py_se0 = py_res['se_betas'][0]\n",
        "py_se1 = py_res['se_betas'][1]\n",
        "py_tau2 = py_res['tau_sq']\n",
        "py_sigma2 = py_res['sigma_sq']\n",
        "\n",
        "# --- 3. RUN R (Metafor) ---\n",
        "print(\"®️ Running R Model (metafor)...\")\n",
        "\n",
        "# Prepare VCV for R\n",
        "# We must sort the dataframe first to ensure alignment\n",
        "df_r = df_sim.sort_values('id').copy()\n",
        "unique_ids = df_r['id'].unique()\n",
        "matrix_list = [vcv_sim[uid] for uid in unique_ids]\n",
        "V_full = block_diag(*matrix_list)\n",
        "\n",
        "# Push to R\n",
        "ro.globalenv['df_r'] = df_r\n",
        "ro.globalenv['V_full'] = V_full\n",
        "\n",
        "r_script = \"\"\"\n",
        "library(metafor)\n",
        "dat <- df_r\n",
        "dat$study_id <- as.factor(dat$id)\n",
        "dat$row_id <- dat$`_row_id`\n",
        "\n",
        "# Fit 3-Level Meta-Regression with VCV\n",
        "res <- rma.mv(yi = yi,\n",
        "              V = V_full,\n",
        "              mods = ~ mod_x,\n",
        "              random = ~ 1 | study_id/row_id,\n",
        "              data = dat,\n",
        "              method = \"REML\",\n",
        "              test = \"t\")\n",
        "\n",
        "list(\n",
        "    b0 = as.numeric(res$b[1]),\n",
        "    b1 = as.numeric(res$b[2]),\n",
        "    se0 = as.numeric(res$se[1]),\n",
        "    se1 = as.numeric(res$se[2]),\n",
        "    tau2 = as.numeric(res$sigma2[1]),\n",
        "    sigma2 = as.numeric(res$sigma2[2]),\n",
        "    k = res$k\n",
        ")\n",
        "\"\"\"\n",
        "r_out = ro.r(r_script)\n",
        "\n",
        "# --- 4. COMPARE ---\n",
        "comparison = [\n",
        "    {'Metric': 'Intercept (β₀)', 'Truth': 0.5, 'Python': py_beta0, 'R': r_out.rx2('b0')[0]},\n",
        "    {'Metric': 'Slope (β₁)', 'Truth': 1.2, 'Python': py_beta1, 'R': r_out.rx2('b1')[0]},\n",
        "    {'Metric': 'SE(Slope)', 'Truth': '-', 'Python': py_se1, 'R': r_out.rx2('se1')[0]},\n",
        "    {'Metric': 'Tau² (Between)', 'Truth': 0.3, 'Python': py_tau2, 'R': r_out.rx2('tau2')[0]},\n",
        "    {'Metric': 'Sigma² (Within)', 'Truth': 0.1, 'Python': py_sigma2, 'R': r_out.rx2('sigma2')[0]}\n",
        "]\n",
        "\n",
        "df_comp = pd.DataFrame(comparison)\n",
        "df_comp['Diff (Py-R)'] = df_comp['Python'] - df_comp['R']\n",
        "\n",
        "def color_diff(val):\n",
        "    if isinstance(val, float) and abs(val) < 1e-4:\n",
        "        return 'background-color: #d4edda' # Green (Good)\n",
        "    elif isinstance(val, float) and abs(val) < 0.05:\n",
        "        return 'background-color: #fff3cd' # Yellow (Acceptable)\n",
        "    return ''\n",
        "\n",
        "display(HTML(\"<h4>📊 Synthetic Data Comparison</h4>\"))\n",
        "display(df_comp.style.applymap(color_diff, subset=['Diff (Py-R)']))\n",
        "\n",
        "print(\"\\n✅ Interpretation:\")\n",
        "if abs(py_beta1 - 1.2) < 0.1:\n",
        "    print(f\" - Python successfully recovered the true slope (Est: {py_beta1:.3f} vs True: 1.2)\")\n",
        "if abs(py_beta1 - r_out.rx2('b1')[0]) < 1e-4:\n",
        "    print(\" - Python results match R exactly.\")\n",
        "else:\n",
        "    print(\" - Small differences detected (likely optimization tolerance).\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "77Vc2ThuM5NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧪 Synthetic Meta-Regression: Parameter Recovery Test\n",
        "# =============================================================================\n",
        "# CELL: SYNTHETIC REGRESSION RECOVERY\n",
        "# Purpose: Prove that the tool can recover known \"True\" parameters.\n",
        "# Method: Monte Carlo simulation with shared control structures.\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.linalg import block_diag\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"🧪 SYNTHETIC META-REGRESSION: PARAMETER RECOVERY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- 1. DATA GENERATOR ---\n",
        "def generate_synthetic_regression(\n",
        "    n_studies=50,\n",
        "    intercept=0.5,         # True Intercept (Beta 0)\n",
        "    slope=1.2,             # True Slope (Beta 1)\n",
        "    tau_sq=0.3,            # True Between-Study Variance\n",
        "    sigma_sq=0.1,          # True Within-Study Variance\n",
        "    correlation=0.5,       # Correlation for shared controls\n",
        "    seed=42\n",
        "):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    data_rows = []\n",
        "    vcv_matrices = {}\n",
        "\n",
        "    print(f\"🎲 Generating Data...\")\n",
        "    print(f\"   True Model: y = {intercept} + {slope}*x\")\n",
        "    print(f\"   Variances: τ²={tau_sq}, σ²={sigma_sq}, ρ={correlation}\")\n",
        "\n",
        "    for i in range(n_studies):\n",
        "        study_id = str(i + 1)\n",
        "\n",
        "        # 1. Determine structure (mix of 1, 2, 3 obs per study)\n",
        "        k_i = rng.choice([1, 2, 3], p=[0.2, 0.5, 0.3])\n",
        "\n",
        "        # 2. Generate Moderator (x)\n",
        "        # Observation-level moderator to test full power\n",
        "        x_vals = rng.uniform(0, 10, size=k_i)\n",
        "\n",
        "        # 3. Generate Random Effects\n",
        "        # u_i (Between-study error)\n",
        "        u_i = rng.normal(0, np.sqrt(tau_sq))\n",
        "\n",
        "        # 4. Construct Variance Matrix (V_i)\n",
        "        vi_diag = rng.uniform(0.05, 0.15, size=k_i)\n",
        "\n",
        "        if k_i > 1:\n",
        "            V_i = np.diag(vi_diag)\n",
        "            for r in range(k_i):\n",
        "                for c in range(k_i):\n",
        "                    if r != c:\n",
        "                        # Covariance = corr * sqrt(v1*v2)\n",
        "                        V_i[r, c] = correlation * np.sqrt(vi_diag[r] * vi_diag[c])\n",
        "            vcv_matrices[study_id] = V_i\n",
        "        else:\n",
        "            vcv_matrices[study_id] = np.diag(vi_diag)\n",
        "\n",
        "        # 5. Generate Observed Effects (y_ij)\n",
        "        # Total Error Covariance = V_i + sigma_sq*I\n",
        "        total_error_cov = vcv_matrices[study_id] + sigma_sq * np.eye(k_i)\n",
        "\n",
        "        # Generate errors (e_ij + sampling error combined)\n",
        "        errors = rng.multivariate_normal(np.zeros(k_i), total_error_cov)\n",
        "\n",
        "        # The Regression Equation: y = B0 + B1*x + u_i + error\n",
        "        y_true = intercept + slope * x_vals + u_i + errors\n",
        "\n",
        "        for j in range(k_i):\n",
        "            data_rows.append({\n",
        "                'id': study_id,\n",
        "                'yi': y_true[j],\n",
        "                'vi': vi_diag[j],\n",
        "                'mod_x': x_vals[j]\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(data_rows)\n",
        "    return df, vcv_matrices\n",
        "\n",
        "# --- 2. GENERATE & INJECT ---\n",
        "\n",
        "# Define \"Truth\"\n",
        "true_b0 = 0.50\n",
        "true_b1 = 0.80\n",
        "true_tau = 0.40\n",
        "true_sigma = 0.15\n",
        "\n",
        "df_sim, vcv_sim = generate_synthetic_regression(\n",
        "    n_studies=100, # Large N to ensure convergence to truth\n",
        "    intercept=true_b0,\n",
        "    slope=true_b1,\n",
        "    tau_sq=true_tau,\n",
        "    sigma_sq=true_sigma,\n",
        "    correlation=0.5\n",
        ")\n",
        "\n",
        "# Inject into Pipeline\n",
        "ANALYSIS_CONFIG['analysis_data'] = df_sim\n",
        "ANALYSIS_CONFIG['vcv_matrices'] = vcv_sim\n",
        "# Save these so your driver can find them\n",
        "ANALYSIS_CONFIG['effect_col'] = 'yi'\n",
        "ANALYSIS_CONFIG['var_col'] = 'vi'\n",
        "\n",
        "# --- 3. RUN PYTHON TOOL ---\n",
        "print(\"\\n🐍 Running Robust Regression Driver...\")\n",
        "# Note: We use your updated _run_three_level_reml_regression_v2\n",
        "py_res, info, _ = _run_three_level_reml_regression_v2(\n",
        "    analysis_data=df_sim,\n",
        "    moderator_col='mod_x',\n",
        "    effect_col='yi',\n",
        "    var_col='vi'\n",
        ")\n",
        "\n",
        "if py_res is None:\n",
        "    print(\"❌ Optimization Failed.\")\n",
        "else:\n",
        "    # --- 4. COMPARE RESULTS ---\n",
        "    est_b0 = py_res['betas'][0]\n",
        "    est_b1 = py_res['betas'][1]\n",
        "    est_tau = py_res['tau_sq']\n",
        "    est_sigma = py_res['sigma_sq']\n",
        "\n",
        "    # Calculate Differences\n",
        "    diff_b0 = abs(est_b0 - true_b0)\n",
        "    diff_b1 = abs(est_b1 - true_b1)\n",
        "    diff_tau = abs(est_tau - true_tau)\n",
        "    diff_sigma = abs(est_sigma - true_sigma)\n",
        "\n",
        "    # Create Table\n",
        "    comparison_data = [\n",
        "        {'Parameter': 'Intercept (β₀)', 'Truth': true_b0, 'Estimated': est_b0, 'Diff': diff_b0},\n",
        "        {'Parameter': 'Slope (β₁)', 'Truth': true_b1, 'Estimated': est_b1, 'Diff': diff_b1},\n",
        "        {'Parameter': 'Tau² (L3)', 'Truth': true_tau, 'Estimated': est_tau, 'Diff': diff_tau},\n",
        "        {'Parameter': 'Sigma² (L2)', 'Truth': true_sigma, 'Estimated': est_sigma, 'Diff': diff_sigma},\n",
        "    ]\n",
        "\n",
        "    df_comp = pd.DataFrame(comparison_data)\n",
        "\n",
        "    def color_diff(val):\n",
        "        if val < 0.05: return 'background-color: #d4edda' # Green (Excellent)\n",
        "        if val < 0.10: return 'background-color: #fff3cd' # Yellow (Good)\n",
        "        return 'background-color: #f8d7da' # Red (Check)\n",
        "\n",
        "    display(HTML(\"<h4>📊 Parameter Recovery Results (N=100 Studies)</h4>\"))\n",
        "    display(df_comp.style.format({\n",
        "        'Truth': '{:.4f}', 'Estimated': '{:.4f}', 'Diff': '{:.4f}'\n",
        "    }).applymap(color_diff, subset=['Diff']))\n",
        "\n",
        "    print(\"\\n✅ Interpretation:\")\n",
        "    if diff_b1 < 0.1:\n",
        "        print(f\" - SUCCESS: The tool accurately recovered the true slope (Error: {diff_b1:.4f})\")\n",
        "    else:\n",
        "        print(f\" - CHECK: Slope estimation deviated.\")\n",
        "\n",
        "    if diff_tau < 0.1 and diff_sigma < 0.1:\n",
        "        print(\" - SUCCESS: Variance components were accurately separated.\")\n",
        "    else:\n",
        "        print(\" - NOTE: Variance components are harder to estimate perfectly, small deviations are normal.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sfyFQJJtOQco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧪 Simulation Study: Spline Analysis (Curve Recovery & Coverage)\n",
        "# =============================================================================\n",
        "# SIMULATION STUDY: SPLINE META-REGRESSION\n",
        "# Purpose: Validate that the tool recovers complex non-linear shapes (Sine Wave).\n",
        "# Metrics:\n",
        "#   1. RMSE: Deviation from the True Curve.\n",
        "#   2. Coverage: % of the True Curve contained within the 95% CI Band.\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import patsy\n",
        "from scipy.linalg import block_diag\n",
        "from scipy.stats import norm, t\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "import warnings\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SIMULATION STUDY: SPLINE CURVE RECOVERY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# =============================================================================\n",
        "# 1. DATA GENERATOR (SINE WAVE)\n",
        "# =============================================================================\n",
        "\n",
        "def generate_spline_sim_data(\n",
        "    n_studies=50,\n",
        "    tau_sq=0.2,\n",
        "    sigma_sq=0.1,\n",
        "    seed=None\n",
        "):\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Moderator X: Uniformly distributed [-3, 3]\n",
        "    # We sort distinct studies to make visualization easier later, but random within\n",
        "    n_obs = n_studies * 2 # 2 obs per study\n",
        "    x = rng.uniform(-3, 3, n_obs)\n",
        "\n",
        "    # True Curve: y = sin(x) + 0.5\n",
        "    true_y = np.sin(x) + 0.5\n",
        "\n",
        "    # Structure\n",
        "    ids = np.repeat(np.arange(n_studies), 2)\n",
        "\n",
        "    # Random Effects\n",
        "    u = np.repeat(rng.normal(0, np.sqrt(tau_sq), n_studies), 2)\n",
        "    e = rng.normal(0, np.sqrt(sigma_sq), n_obs)\n",
        "\n",
        "    y_obs = true_y + u + e\n",
        "    vi = rng.uniform(0.05, 0.15, n_obs)\n",
        "\n",
        "    df = pd.DataFrame({'id': ids, 'y': y_obs, 'vi': vi, 'x': x})\n",
        "    return df\n",
        "\n",
        "# =============================================================================\n",
        "# 2. RUN SINGLE SIMULATION\n",
        "# =============================================================================\n",
        "\n",
        "def run_spline_simulation(n_studies=50, tau_sq=0.2, sigma_sq=0.1, seed=None):\n",
        "    # 1. Generate Data\n",
        "    df = generate_spline_sim_data(n_studies, tau_sq, sigma_sq, seed)\n",
        "\n",
        "    # Define \"Truth\" Grid for checking accuracy\n",
        "    # We check the curve at 50 evenly spaced points from -3 to 3\n",
        "    check_x = np.linspace(-3, 3, 50)\n",
        "    true_curve_y = np.sin(check_x) + 0.5\n",
        "\n",
        "    # 2. Run Python Spline\n",
        "    # We use a try/except block to handle non-convergence\n",
        "    try:\n",
        "        est, err = _run_robust_spline_analysis(\n",
        "            df, 'x', 'y', 'vi', df_spline=6\n",
        "        )\n",
        "\n",
        "        if err or est is None:\n",
        "            return None\n",
        "\n",
        "        # 3. Predict Curve at Check Points\n",
        "        # Reconstruct basis matrix for the check points\n",
        "        formula = est['formula']\n",
        "        mod_mean = est['mod_mean']\n",
        "        mod_std = est['mod_std']\n",
        "\n",
        "        # Standardize check points using the *original data's* mean/std\n",
        "        z_check = (check_x - mod_mean) / mod_std\n",
        "\n",
        "        basis_matrix = patsy.dmatrix(formula, {\"x\": z_check}, return_type='matrix')\n",
        "        basis_matrix = np.asarray(basis_matrix)\n",
        "\n",
        "        # Full Design Matrix\n",
        "        X_pred = np.column_stack([np.ones(len(check_x)), basis_matrix])\n",
        "\n",
        "        # Predict y_hat\n",
        "        betas = est['betas']\n",
        "        y_hat = X_pred @ betas\n",
        "\n",
        "        # Calculate Confidence Intervals for the curve\n",
        "        # Var(y_hat) = X * Cov(Beta) * X'\n",
        "        cov_beta = est['cov_beta']\n",
        "        var_pred = np.sum((X_pred @ cov_beta) * X_pred, axis=1)\n",
        "        se_pred = np.sqrt(var_pred)\n",
        "\n",
        "        # 95% CI\n",
        "        # df for t-dist is roughly M - p\n",
        "        df_t = max(1, n_studies - len(betas))\n",
        "        crit = t.ppf(0.975, df_t)\n",
        "\n",
        "        ci_lo = y_hat - crit * se_pred\n",
        "        ci_hi = y_hat + crit * se_pred\n",
        "\n",
        "        # 4. Calculate Metrics\n",
        "        # RMSE: Average distance from truth\n",
        "        mse = np.mean((y_hat - true_curve_y)**2)\n",
        "        rmse = np.sqrt(mse)\n",
        "\n",
        "        # Coverage: What % of the True Curve is inside the band?\n",
        "        # Ideally, this should be close to 100% for the whole curve,\n",
        "        # or exactly 95% for point-wise coverage.\n",
        "        point_coverage = np.mean((ci_lo <= true_curve_y) & (true_curve_y <= ci_hi))\n",
        "\n",
        "        return {\n",
        "            'rmse': rmse,\n",
        "            'coverage': point_coverage,\n",
        "            'y_hat': y_hat, # Store for plotting \"Average Curve\"\n",
        "            'check_x': check_x,\n",
        "            'true_y': true_curve_y\n",
        "        }\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# =============================================================================\n",
        "# 3. RUN MONTE CARLO LOOP\n",
        "# =============================================================================\n",
        "\n",
        "print(\"🚀 Starting Spline Simulation (N=50 iterations)...\")\n",
        "# Increase N for publication (e.g., 500)\n",
        "N_SIMS = 50\n",
        "\n",
        "all_rmse = []\n",
        "all_coverage = []\n",
        "all_yhats = []\n",
        "\n",
        "# Scenario: Moderate Heterogeneity\n",
        "for i in range(N_SIMS):\n",
        "    res = run_spline_simulation(n_studies=50, tau_sq=0.3, sigma_sq=0.1, seed=i)\n",
        "    if res:\n",
        "        all_rmse.append(res['rmse'])\n",
        "        all_coverage.append(res['coverage'])\n",
        "        all_yhats.append(res['y_hat'])\n",
        "\n",
        "# =============================================================================\n",
        "# 4. RESULTS & VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "avg_rmse = np.mean(all_rmse)\n",
        "avg_coverage = np.mean(all_coverage) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"📊 SPLINE SIMULATION RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Iterations: {len(all_rmse)}/{N_SIMS} converged\")\n",
        "print(f\"RMSE (Error): {avg_rmse:.4f} (Lower is better)\")\n",
        "print(f\"CI Coverage:  {avg_coverage:.1f}% (Target: 95%)\")\n",
        "\n",
        "# Interpretation\n",
        "print(\"-\" * 50)\n",
        "if avg_coverage > 93:\n",
        "    print(\"✅ SUCCESS: The 95% Confidence Band reliably contains the true curve.\")\n",
        "elif avg_coverage > 90:\n",
        "    print(\"⚠️ OK: Coverage is slightly low but acceptable.\")\n",
        "else:\n",
        "    print(\"❌ CHECK: Coverage is too low (Band is too narrow).\")\n",
        "\n",
        "# --- PLOT: VISUAL PROOF ---\n",
        "# We plot the \"True Curve\" vs the \"Average Recovered Curve\" from 50 sims\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# 1. Plot True Curve\n",
        "x_grid = np.linspace(-3, 3, 50)\n",
        "true_y = np.sin(x_grid) + 0.5\n",
        "plt.plot(x_grid, true_y, 'k-', linewidth=3, label='True Sine Wave (y = sin(x))')\n",
        "\n",
        "# 2. Plot All 50 Recovered Curves (faint)\n",
        "for yhat in all_yhats:\n",
        "    plt.plot(x_grid, yhat, 'b-', alpha=0.1)\n",
        "\n",
        "# 3. Plot Average Recovered Curve\n",
        "avg_yhat = np.mean(np.array(all_yhats), axis=0)\n",
        "plt.plot(x_grid, avg_yhat, 'r--', linewidth=2, label='Average Python Estimate')\n",
        "\n",
        "plt.title(f\"Visual Proof: Parameter Recovery over {len(all_rmse)} Simulations\", fontsize=14)\n",
        "plt.xlabel(\"Moderator X\", fontsize=12)\n",
        "plt.ylabel(\"Effect Size\", fontsize=12)\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p6jLvYE6zr3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧪 R Validation: Diagnostics (LOO, Egger's, PET-PEESE)\n",
        "# =============================================================================\n",
        "# CELL: DIAGNOSTIC MODULE VALIDATION\n",
        "# Purpose: Validate sensitivity and bias detection methods against R.\n",
        "# Modules Tested:\n",
        "#   1. Leave-One-Out (Influence Analysis)\n",
        "#   2. Egger's Test (Regression on SE)\n",
        "#   3. PET-PEESE (Regression on SE and Variance)\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rpy2.robjects as ro\n",
        "from rpy2.robjects import pandas2ri\n",
        "import rpy2.robjects.numpy2ri as numpy2ri\n",
        "from scipy.linalg import block_diag\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Activate R conversion\n",
        "pandas2ri.activate()\n",
        "numpy2ri.activate()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"🧪 DIAGNOSTIC VALIDATION: SENSITIVITY & BIAS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    # =================================================================\n",
        "    # 1. GENERATE DATA WITH PUBLICATION BIAS\n",
        "    # =================================================================\n",
        "    # We create small studies with artificially high effects to trigger Egger's\n",
        "    def generate_biased_data(n_studies=20, seed=42):\n",
        "        rng = np.random.default_rng(seed)\n",
        "\n",
        "        ids = []\n",
        "        yi = []\n",
        "        vi = []\n",
        "\n",
        "        for i in range(n_studies):\n",
        "            sid = str(i+1)\n",
        "            # 50% chance of being a small, noisy study\n",
        "            is_small = rng.random() > 0.5\n",
        "\n",
        "            if is_small:\n",
        "                n_obs = 1\n",
        "                v = 0.15 # High variance\n",
        "                # Bias: Small studies have higher effect (e.g., 0.8 vs 0.3)\n",
        "                y = 0.8 + rng.normal(0, 0.1)\n",
        "            else:\n",
        "                n_obs = rng.choice([2, 3]) # Larger, multi-obs studies\n",
        "                v = 0.02 # Low variance (precise)\n",
        "                y = 0.3 + rng.normal(0, 0.1)\n",
        "\n",
        "            for _ in range(n_obs):\n",
        "                ids.append(sid)\n",
        "                yi.append(y + rng.normal(0, np.sqrt(v)))\n",
        "                vi.append(v)\n",
        "\n",
        "        df = pd.DataFrame({'id': ids, 'yi': yi, 'vi': vi})\n",
        "        df['sei'] = np.sqrt(df['vi']) # Standard Error\n",
        "        return df\n",
        "\n",
        "    df_diag = generate_biased_data()\n",
        "    print(f\"🎲 Generated Biased Data: {len(df_diag)} obs from {df_diag['id'].nunique()} studies\")\n",
        "\n",
        "    # Inject into pipeline\n",
        "    ANALYSIS_CONFIG['analysis_data'] = df_diag\n",
        "    ANALYSIS_CONFIG['vcv_matrices'] = {} # Diagonal for simplicity here\n",
        "\n",
        "    # =================================================================\n",
        "    # 2. VALIDATE LEAVE-ONE-OUT (LOO)\n",
        "    # =================================================================\n",
        "    print(\"\\n🔍 Validating Leave-One-Out...\")\n",
        "\n",
        "    # --- Python LOO ---\n",
        "    py_loo_res = []\n",
        "    unique_ids = df_diag['id'].unique()\n",
        "\n",
        "    for skip_id in unique_ids:\n",
        "        # Subset data\n",
        "        subset = df_diag[df_diag['id'] != skip_id].copy()\n",
        "        # Run standard 3-level model\n",
        "        res, _ = _run_three_level_reml_for_subgroup(subset, 'yi', 'vi')\n",
        "        if res:\n",
        "            py_loo_res.append(res['mu'])\n",
        "        else:\n",
        "            py_loo_res.append(np.nan)\n",
        "\n",
        "    py_loo_min = np.min(py_loo_res)\n",
        "    py_loo_max = np.max(py_loo_res)\n",
        "\n",
        "    # --- R LOO ---\n",
        "    ro.globalenv['df_r'] = df_diag\n",
        "    r_loo_script = \"\"\"\n",
        "    library(metafor)\n",
        "    dat <- df_r\n",
        "    ids <- unique(dat$id)\n",
        "    loo_est <- numeric(length(ids))\n",
        "\n",
        "    for(i in 1:length(ids)) {\n",
        "        subset <- dat[dat$id != ids[i], ]\n",
        "        subset$row_id <- 1:nrow(subset)\n",
        "        subset$study_id <- as.factor(subset$id)\n",
        "\n",
        "        res <- tryCatch({\n",
        "            rma.mv(yi, vi, random = ~ 1 | study_id/row_id, data=subset)\n",
        "        }, error=function(e) NULL)\n",
        "\n",
        "        if(!is.null(res)) {\n",
        "            loo_est[i] <- res$b[1]\n",
        "        } else {\n",
        "            loo_est[i] <- NA\n",
        "        }\n",
        "    }\n",
        "    list(min=min(loo_est, na.rm=TRUE), max=max(loo_est, na.rm=TRUE))\n",
        "    \"\"\"\n",
        "    r_loo = ro.r(r_loo_script)\n",
        "\n",
        "    # Compare\n",
        "    loo_diff_min = abs(py_loo_min - r_loo.rx2('min')[0])\n",
        "    loo_diff_max = abs(py_loo_max - r_loo.rx2('max')[0])\n",
        "\n",
        "    # =================================================================\n",
        "    # 3. VALIDATE EGGER'S TEST & PET-PEESE\n",
        "    # =================================================================\n",
        "    print(\"🔍 Validating Egger's / PET-PEESE...\")\n",
        "\n",
        "    # --- Python ---\n",
        "    # Egger's is just regression on SE\n",
        "    py_egger, _, _ = _run_three_level_reml_regression_v2(df_diag, 'sei', 'yi', 'vi')\n",
        "\n",
        "    # PEESE is regression on Variance (SE^2)\n",
        "    py_peese, _, _ = _run_three_level_reml_regression_v2(df_diag, 'vi', 'yi', 'vi')\n",
        "\n",
        "    py_pet_slope = py_egger['betas'][1]\n",
        "    py_pet_pval = py_egger['p_values'][1]\n",
        "    py_peese_slope = py_peese['betas'][1]\n",
        "\n",
        "    # --- R ---\n",
        "    r_bias_script = \"\"\"\n",
        "    library(metafor)\n",
        "    dat <- df_r\n",
        "    dat$row_id <- 1:nrow(dat)\n",
        "    dat$study_id <- as.factor(dat$id)\n",
        "\n",
        "    # PET / Egger (Mod = sei)\n",
        "    res_pet <- rma.mv(yi, vi, mods = ~ sqrt(vi), random = ~ 1 | study_id/row_id, data=dat)\n",
        "\n",
        "    # PEESE (Mod = vi)\n",
        "    res_peese <- rma.mv(yi, vi, mods = ~ vi, random = ~ 1 | study_id/row_id, data=dat)\n",
        "\n",
        "    list(\n",
        "        pet_slope = as.numeric(res_pet$b[2]),\n",
        "        pet_pval = as.numeric(res_pet$pval[2]),\n",
        "        peese_slope = as.numeric(res_peese$b[2])\n",
        "    )\n",
        "    \"\"\"\n",
        "    r_bias = ro.r(r_bias_script)\n",
        "\n",
        "    # =================================================================\n",
        "    # 4. RESULTS TABLE\n",
        "    # =================================================================\n",
        "\n",
        "    comparison = [\n",
        "        # Leave-One-Out\n",
        "        {'Metric': 'LOO Min Estimate', 'Python': py_loo_min, 'R': r_loo.rx2('min')[0]},\n",
        "        {'Metric': 'LOO Max Estimate', 'Python': py_loo_max, 'R': r_loo.rx2('max')[0]},\n",
        "        # Egger's / PET\n",
        "        {'Metric': \"Egger's/PET Slope\", 'Python': py_pet_slope, 'R': r_bias.rx2('pet_slope')[0]},\n",
        "        {'Metric': \"Egger's P-value\", 'Python': py_pet_pval, 'R': r_bias.rx2('pet_pval')[0]},\n",
        "        # PEESE\n",
        "        {'Metric': 'PEESE Slope', 'Python': py_peese_slope, 'R': r_bias.rx2('peese_slope')[0]},\n",
        "    ]\n",
        "\n",
        "    df_comp = pd.DataFrame(comparison)\n",
        "    df_comp['Diff'] = np.abs(df_comp['Python'] - df_comp['R'])\n",
        "\n",
        "    def color_status(val):\n",
        "        return 'background-color: #d4edda' if val < 1e-4 else 'background-color: #fff3cd'\n",
        "\n",
        "    display(HTML(\"<h4>📊 Diagnostic Validation Results</h4>\"))\n",
        "    display(df_comp.style.format({'Python': '{:.4f}', 'R': '{:.4f}', 'Diff': '{:.2e}'})\n",
        "                   .applymap(color_status, subset=['Diff']))\n",
        "\n",
        "    print(\"\\n✅ Interpretation:\")\n",
        "    if df_comp['Diff'].max() < 1e-3:\n",
        "        print(\" - SUCCESS: All diagnostic modules match R's implementation.\")\n",
        "    else:\n",
        "        print(\" - CHECK: Small differences detected (likely optimization tolerance).\")\n",
        "\n",
        "    if py_pet_pval < 0.05:\n",
        "        print(f\" - Note: Egger's test correctly detected the injected bias (p={py_pet_pval:.4f})\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8VUt44RVV2mA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}